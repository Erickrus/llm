{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CcmDWBEO5Lf0",
        "JFHMhH-v5I7o",
        "RZT3wYaD5fQZ",
        "hnXsC9bbHBGZ"
      ],
      "authorship_tag": "ABX9TyNvtS2fT4GWhky/FVEPFuB8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erickrus/llm/blob/main/SoftmaxWithTemperature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM decode process - Temperature, Top_P and Top_K"
      ],
      "metadata": {
        "id": "TuX510B46QJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=lH9YPeSq6IA"
      ],
      "metadata": {
        "id": "3EHqD6V46gBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM decode - Softmax with Temperature"
      ],
      "metadata": {
        "id": "CcmDWBEO5Lf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title softmax(z)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    #print(\"softmax\")\n",
        "    #@markdown $\\sigma(z_i)=\\frac{e^{z_i}}{\\sum^N_{j=0}e^{z_j}}$\n",
        "    logits = np.asarray(z)\n",
        "    exponential_logits = np.exp(logits)\n",
        "    probs = exponential_logits / np.sum(exponential_logits)\n",
        "    return probs"
      ],
      "metadata": {
        "id": "cpA4Vwe5vPS8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title softmax_with_temperature(z, temperature)\n",
        "def softmax_with_temperature(z, temperature=0.0):\n",
        "    #@markdown The only difference is divided by $\\theta$\n",
        "    #@markdown\n",
        "    #@markdown $\\sigma(z_i)=\\frac{\\frac{e^{z_i}}{\\theta}}{\\sum^N_{j=0}\\frac{e^{z_j}}{\\theta}}$\n",
        "    #@markdown\n",
        "    #@markdown Notice, when temperature = 0.0 , it is the same as temperature = 1.0\n",
        "    if temperature == 0.0:\n",
        "        return softmax(z)\n",
        "    #print(\"softmax_with_temperature\")\n",
        "    theta = temperature\n",
        "    logits = np.asarray(z) / theta\n",
        "    exponential_logits = np.exp(logits)\n",
        "    probs = exponential_logits / np.sum(exponential_logits)\n",
        "    return probs"
      ],
      "metadata": {
        "id": "v5s3AsYgyLGP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title define a toy dataset: words and logits\n",
        "#@markdown | logits | words |\n",
        "#@markdown |:---:|:---:|\n",
        "#@markdown | 1.0 | I |\n",
        "#@markdown | 3.0 | boy |\n",
        "#@markdown | 2.0 | hello |\n",
        "#@markdown | 9.0 | LLM |\n",
        "#@markdown | 8.0 | NLP |\n",
        "#@markdown | 4.0 | N-gram |\n",
        "logits = [1.0, 3.0, 2.0, 9.0, 8.0, 4.0]\n",
        "words = ['I', 'boy', 'hello', 'LLM', 'NLP', 'N-gram']"
      ],
      "metadata": {
        "id": "tlvc6bd7z7IO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title temperature_random_sampling(logits, words, temperature)\n",
        "def temperature_random_sampling(logits, words, temperature=0.0, print_words=False):\n",
        "  #@markdown step 1: get logits, words\n",
        "  #@markdown\n",
        "  #@markdown step 2: soft-max with temperature-based probability\n",
        "  probs =softmax_with_temperature(logits, temperature)\n",
        "  if print_words:\n",
        "      print('Logit to Prob:', list(map(list, zip(words, probs))))\n",
        "  #sample using np.random.choice()\n",
        "  #https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
        "  #@markdown step 3: select one token randomly\n",
        "  random_samp_index = np.random.choice(range(len(logits)), p=probs)\n",
        "  print('Sampled word:', words[random_samp_index])"
      ],
      "metadata": {
        "id": "Avnbl3lk0IS9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title run temperature_random_sampling 10 times, with temperature = 1.0\n",
        "temperature = 1.0\n",
        "for i in range(10):\n",
        "  temperature_random_sampling(logits, words, temperature)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PcWQ9S10svf",
        "outputId": "2337e358-446e-4122-c06c-7a749575762e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled word: LLM\n",
            "Sampled word: LLM\n",
            "Sampled word: LLM\n",
            "Sampled word: LLM\n",
            "Sampled word: LLM\n",
            "Sampled word: LLM\n",
            "Sampled word: NLP\n",
            "Sampled word: LLM\n",
            "Sampled word: NLP\n",
            "Sampled word: LLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM decode - Top_P"
      ],
      "metadata": {
        "id": "JFHMhH-v5I7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title define top_index(probs, top_p)\n",
        "#@markdown sort based on probs, pick the first N with sum(prob) <= top_p\n",
        "def top_p_indexes(probs, top_p):\n",
        "  sorted_index = np.argsort(probs)[::-1]\n",
        "  cum_prob = 0.0\n",
        "  selected_index = []\n",
        "  for i in sorted_index:\n",
        "    if cum_prob <= top_p:\n",
        "      selected_index.append(i)\n",
        "      cum_prob += probs[i]\n",
        "    else:\n",
        "      break\n",
        "  return selected_index"
      ],
      "metadata": {
        "id": "SOWL6mYf3fWh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title define top_p_sampling(logits, words, top_p)\n",
        "def top_p_sampling(logits, words, top_p, print_words=False):\n",
        "  #@markdown step 1, get logits\n",
        "  probs = softmax(logits)\n",
        "  #@markdown step 2, sort based on probability\n",
        "  #@markdown\n",
        "  #@markdown step 3, filter token where cummulative probability exceeds TOP_P\n",
        "  selected_index = top_p_indexes(probs, top_p)\n",
        "  #@markdown step 4, Recalculate probability using softmax filter token\n",
        "  top_p_prob = softmax(probs[selected_index])\n",
        "  if print_words:\n",
        "    selected_words = []\n",
        "    for i in selected_index:\n",
        "      selected_words.append(words[i])\n",
        "    print('Top P words and Normalized Prob:', list(map(list, zip(selected_words, top_p_prob))))\n",
        "    #@markdown step 5, Random Sampling using final probability\n",
        "    random_samp_index = np.random.choice(selected_index, p=top_p_prob)\n",
        "    print('Sampled word:', words[random_samp_index])\n"
      ],
      "metadata": {
        "id": "zUztpeFq3z6O",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_p = 0.9\n",
        "top_p_sampling(logits, words, top_p, print_words=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEUNo2Db48O7",
        "outputId": "37fef969-1be0-4637-9a35-fccb9084b70f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top P words and Normalized Prob: [['LLM', 0.6126841114435158], ['NLP', 0.3873158885564842]]\n",
            "Sampled word: NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM decode - Top_K"
      ],
      "metadata": {
        "id": "RZT3wYaD5fQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title top_k_sampling(logits, words, top_k)\n",
        "#select the top_k's index, then sample from them\n",
        "\n",
        "def top_k_sampling(logits, words, top_k, print_words=False):\n",
        "  #@markdown step 1, Get logits and words\n",
        "\n",
        "  #@markdown step 2, Calculate probability using soft-max\n",
        "  probs = softmax(logits)\n",
        "\n",
        "  #@markdown step 3, Sort based on Probability\n",
        "\n",
        "  #@markdown step 4, Filter TOP_K tokens\n",
        "  selected_index = np.argsort(probs)[::-1][:top_k]\n",
        "\n",
        "  #@markdown step 5, Recalculate probability using soft-max on filtered token\n",
        "  top_k_prob = softmax(probs[selected_index])\n",
        "  if print_words:\n",
        "    selected_words = []\n",
        "    for i in selected_index:\n",
        "      selected_words.append(words[i])\n",
        "    print('Top K words and Normalized Prob:', list(map(list, zip(selected_words, top_k_prob))))\n",
        "    #@markdown step 6, Random Sampling using final probability\n",
        "    random_samp_index = np.random.choice(selected_index, p=top_k_prob)\n",
        "    print('Sampled word:', words[random_samp_index])"
      ],
      "metadata": {
        "id": "QiFcgsBw5iIn",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 3\n",
        "top_k_sampling(logits, words, top_k, print_words=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy83E5zX50XH",
        "outputId": "46f3201f-dcdb-44a3-eede-0d8d321f0923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top K words and Normalized Prob: [['LLM', 0.47200693217709], ['NLP', 0.29838505834640666], ['N-gram', 0.22960800947650348]]\n",
            "Sampled word: NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Greedy decode"
      ],
      "metadata": {
        "id": "hnXsC9bbHBGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function essentially performs a greedy selection of the token candidate with the highest logit value. It's commonly used in machine learning models for tasks like text generation, where the model needs to choose the most probable token to continue generating text.\n",
        "\n",
        "https://github.com/yui0/slibs/blob/4ecfe3824fd0f848f57216b800578090a3df353d/ggml/llama/llama.cpp#L2350\n",
        "\n",
        "```python\n",
        "def llama_sample_token_greedy(ctx: LlamaContext, candidates: List[LlamaTokenData]) -> int:\n",
        "    t_start_sample_us = ggml_time_us()  # Assuming ggml_time_us() is defined elsewhere\n",
        "\n",
        "    # Find max element\n",
        "    max_data = max(candidates, key=lambda x: x.logit)\n",
        "    result = max_data.id\n",
        "\n",
        "    if ctx:\n",
        "        ctx.t_sample_us += ggml_time_us() - t_start_sample_us\n",
        "        ctx.n_sample += 1\n",
        "\n",
        "    return result\n",
        "```"
      ],
      "metadata": {
        "id": "UX7PDw_6Gd4g"
      }
    }
  ]
}
