{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMExxY9wTNgT8UeJmufqdk6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erickrus/llm/blob/main/llama_3_2_vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvl_gdVsXp0J",
        "outputId": "064ea750-be8b-4201-82f8-c7869874f774"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov  9 14:40:54 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   43C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXHMD_V2XefD",
        "outputId": "5057c7b0-3d9d-41e8-b75a-d78af98e15de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnxn_ROeYTER",
        "outputId": "1d5cfc21-5e74-4eb9-f63a-9bfd8ac746c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2-vision:11b"
      ],
      "metadata": {
        "id": "bLm7OEb1YGgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICOvWdXZYfwQ",
        "outputId": "39913b90-e59c-45a6-d9c6-375138d2f667"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                   ID              SIZE      MODIFIED     \n",
            "llama3.2-vision:11b    38107a0cd119    7.9 GB    1 second ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import json\n",
        "import requests\n",
        "\n",
        "def query_image(\n",
        "    prompt,\n",
        "    image_filename,\n",
        "    url='localhost:11434',\n",
        "    model_name= \"llama3.2-vision:11b\",\n",
        "):\n",
        "    api_url = f\"http://{url}/api/generate\"\n",
        "    image_base64 = base64.b64encode(open(image_filename, \"rb\").read()).decode('utf-8')\n",
        "\n",
        "    data = {\n",
        "      \"model\": model_name,\n",
        "      \"prompt\": prompt,\n",
        "      \"stream\": False,\n",
        "      \"images\": [image_base64]\n",
        "    }\n",
        "\n",
        "    data_json = json.dumps(data)\n",
        "    try:\n",
        "        response = requests.post(api_url, json=data)\n",
        "        response.raise_for_status()\n",
        "        print(response.json()['response'])\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(\"Request failed:\", e)\n",
        "\n",
        "query_image(\"What is in this picture?\", \"/content/image.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4KoruFlYmsF",
        "outputId": "759af06f-a348-499f-b133-8a79bb291998"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image depicts a cat dressed in traditional Chinese attire, complete with a red hat and robe. The cat's facial expression is one of calmness and serenity, as if it is reflecting on the wisdom of Confucius or contemplating the meaning of life. The overall atmosphere of the image is one of tranquility and cultural richness, inviting the viewer to appreciate the beauty of traditional Chinese culture through the eyes of a feline observer.\n"
          ]
        }
      ]
    }
  ]
}