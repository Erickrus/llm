{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAl5CGiPwJ9vZJOaH1xTl2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erickrus/llm/blob/main/einsum_is_all_you_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EINSUM IS ALL YOU NEED**\n",
        "\n",
        "EINSTEIN SUMMATION IN DEEP LEARNING\n",
        "\n",
        "pytorch: https://pytorch.org/docs/stable/generated/torch.einsum.html\n",
        "\n",
        "tutorial url: https://rockt.github.io/2018/04/30/einsum"
      ],
      "metadata": {
        "id": "7LCz0-Bt8kAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "– Tim Rocktäschel, 30/04/2018 – *updated 02/05/2018*\n",
        "\n",
        "When talking to colleagues I realized that not everyone knows about *einsum*, my favorite function for developing deep learning models.\n",
        "This post is trying to change that once and for all! :)\n",
        "Einstein summation (einsum) is implemented in [numpy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html), as well as deep learning libraries such as [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/einsum) and, thanks to [Thomas Viehmann](https://github.com/pytorch/pytorch/pull/6307), recently also [PyTorch](http://pytorch.org/docs/master/torch.html?highlight=torch%20max#torch.einsum).\n",
        "For background reading on einsum, I recommend the excellent blog posts by [Olexa Bilaniuk](https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/) and [Alex Riley](http://ajcr.net/Basic-guide-to-einsum/).\n",
        "While their posts discuss einsum in the context of numpy, I am going to illustrate how einsum is extremely useful for writing elegant PyTorch/TensorFlow models.[^1]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nxWEB4hxz5Q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Einsum Notation[^2]\n",
        "\n",
        "If you are anything like me, you find it difficult to remember the names and signatures of all the different functions in PyTorch/TensorFlow for calculating dot products, outer products, transposes and matrix-vector or matrix-matrix multiplications.\n",
        "Einsum notation is an elegant way to express all of these, as well as complex operations on tensors, using essentially a domain-specific language.  \n",
        "This has benefits beyond not having to memorize or regularly looking up specific library functions.\n",
        "Once you understand and make use of einsum, you will be able to write more concise and efficient code more quickly.\n",
        "When not using einsum it is easy to introduce unnecessary reshaping and transposing of tensors, as well as intermediate tensors that could be omitted.\n",
        "Furthermore, domain-specific languages like einsum can sometimes be compiled to high-performing code, and an einsum-like domain-specific language is in fact the basis for the recently introduced [Tensor Comprehensions](http://pytorch.org/2018/03/05/tensor-comprehensions.html)[^3] in PyTorch which automatically generate GPU code and auto-tune that code for specific input sizes.\n",
        "In addition, projects like [opt einsum](https://github.com/dgasmith/opt_einsum) and [tf einsum opt](https://github.com/Bihaqo/tf_einsum_opt) can be used to optimize tensor contraction order of einsum expressions.[^4]"
      ],
      "metadata": {
        "id": "8Hyy1xU90IuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we want to multiply two matrices **A** in $ℝ^{I × K}$ and **B** in $ℝ^{K × J}$ followed by calculating the sum of each column resulting in a vector **c** in $ℝ^J$. Using Einstein summation notation, we can write this as\n",
        "\n",
        "$ c_j = \\sum_i\\sum_k A_{ik}B_{kj} = A_{ik}B_{kj} $\n",
        "\n",
        "which specifies how all individual elements $c_i$ in **c** are calculated from multiplying values in the column vectors **$A_{i:}$** and row vectors **$B_{:j}$** and summing them up. Note that for Einstein notation, the summation Sigmas can be dropped as we implicitly sum over repeated indices (k in this example) and indices not mentioned in the output specification (i in this example).\n",
        "\n",
        "So far so good, but we can also express more basic operations using einsum. For instance, calculating the dot product of two vectors **a**, **b** in $ℝ^I$ can be written as\n",
        "\n",
        "$ c = \\sum_i a_i b_i = a_i b_i. $\n",
        "\n",
        "A problem that I encounter often in deep learning is applying a transformation to vectors in a higher-order tensor. For example, I might have a tensor that contains T-long sequences of K-dimensional word vectors for N training examples in a batch and I want to project the word vectors to a different dimension Q. Let **T** in $ℝ^{N × T × K}$ be an order-3 tensor where the first dimension corresponds to the batch, the second dimension to the sequence length, and the last dimension to the word vectors. In addition, let **W** in $ℝ^{K × Q}$ be a projection matrix. The desired computation can be expressed using einsum\n",
        "\n",
        "$ C_{ntq} = \\sum_k T_{ntk}W_{kq} = T_{ntk}W_{kq}. $\n",
        "\n",
        "As a final example, say you are given an order-4 tensor **T** in $ℝ^{N × T × K × M}$ and you are supposed to project vectors in the 3rd dimension to Q using the projection matrix from before. However, let's say I also ask you to sum over the 2nd dimension and transpose the first and last dimension in the result, yielding a tensor **C** in $ℝ^{M × Q × N}$.[^5] Einsum to the rescue!\n",
        "\n",
        "$ C_{mqn} = \\sum_t\\sum_k T_{ntkm}W_{kq} = T_{ntkm}W_{kq}. $\n",
        "\n",
        "Note that transposing the result of the tensor contraction is achieved by swapping n with m $C_{mqn}$ instead of $C_{nqm}$.\n"
      ],
      "metadata": {
        "id": "1ei52--s0x_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 All you Need: Einsum in numpy, PyTorch, and TensorFlow\n",
        "\n",
        "Einsum is implemented in [numpy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) via `np.einsum`, in [PyTorch](http://pytorch.org/docs/master/torch.html?highlight=torch%20max#torch.einsum) via `torch.einsum`, and in [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/einsum) via `tf.einsum`.[^6] All three einsum functions share the same signature `einsum(equation, operands)` where `equation` is a string representing the Einstein summation and `operands` is a sequence of tensors.[^7] The examples above can all be written using an equation string. For instance, our first example $c_j = \\sum_i\\sum_k A_{ik}B_{kj}$ can be written as the equation string \"`ik,kj -> j`\". Note that the naming of the indices (i, j, k) is arbitrary but it needs to be used consistently.\n"
      ],
      "metadata": {
        "id": "sh7QtCBIprLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's great about having einsum not only in numpy but also in PyTorch and TensorFlow is that it can be used in arbitrary computation graphs for neural network architectures and that we can backpropagate through it.\n",
        "A typical call to einsum has the following form\n",
        "$\n",
        "result = einsum(\"{\\color{red}\\square\\square},{\\color{purple}\\square\\square\\square},{\\color{blue}\\square\\square}\\,\\text{->}\\,{\\color{green}\\square\\square}\", {\\color{red}\\text{arg1}}, {\\color{purple}\\text{arg2}}, {\\color{blue}\\text{arg3}})\n",
        "$\n",
        "where $ \\square $ is a placeholder for a character identifying a tensor dimension.\n",
        "\n",
        "From this equation string, we can infer that $ \\color{red}\\text{arg1} $ and {\\color{blue}\\text{arg3}} are matrices, {\\color{purple}\\text{arg2}} is an order-3 tensor, and that the $\\color{green}\\textbf{result}$ of this einsum operation is a matrix.\n",
        "\n",
        "Note that einsum works with a variable number of inputs.\n",
        "In the example above, einsum specifies an operation on three arguments, but it can also be used for operations involving one, two, or more than three arguments.\n",
        "\n",
        "Einsum is best learned by studying examples, so let's go through some examples for einsum in PyTorch that correspond to library functions which are used in many deep learning models.\n"
      ],
      "metadata": {
        "id": "tqGWclerp9KS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOOTNOTES:\n",
        "\n",
        "1\n",
        "My examples use PyTorch, but translating them to TensorFlow is trivial.\n",
        "\n",
        "2\n",
        "The first version of this post was incorrectly using a summation Sigma which is not Einstein notation but classical notation. Thanks to Christian Wolf and reddit/ML for pointing this out.\n",
        "\n",
        "3\n",
        "Vasilache, Zinenko, Theodoridis, Goyal, DeVito, Moses, Verdoolaege, Adams and Cohen. Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions. arXiv preprint arXiv:1802.04730. 2018\n",
        "4\n",
        "Thanks to Stephan Hoyer and Alexander Novikov for the pointers.\n",
        "\n",
        "5\n",
        "Thanks to Blammar for pointing out a previous error.\n",
        "\n",
        "6\n",
        "Thanks to Martin Trapp for pointing out that there is also a Julia implementation.\n",
        "\n",
        "7\n",
        "In numpy and TensorFlow, operands can be a variable-length argument list whereas in PyTorch it needs to be a list.\n",
        "\n",
        "8\n",
        "Farquhar, Rocktäschel, Igl and Whiteson. TreeQN and ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning. in: International Conference on Learning Representations (ICLR). 2018\n",
        "\n",
        "9\n",
        "He, Zhang, Ren and Sun. Deep Residual Learning for Image Recognition. in: 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR. 2016\n",
        "\n",
        "10\n",
        "Rocktäschel, Grefenstette, Hermann, Kocisky and Blunsom. Reasoning about Entailment with Neural Attention. in: International Conference on Learning Representations (ICLR). 2016"
      ],
      "metadata": {
        "id": "VDV6mk-w817S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Matrix Scalar Multiplication:**\n",
        "\n",
        "   If you have a matrix $A$ and you want to multiply it by a scalar $k$, the result $B$ is obtained by multiplying each element of $A$ by $k$:\n",
        "\n",
        "   $\n",
        "   B_{ij} = k \\cdot A_{ij}\n",
        "   $\n"
      ],
      "metadata": {
        "id": "Shui58_lyrLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title numpy example\n",
        "import numpy as np\n",
        "\n",
        "A = np.round(np.random.uniform(size=[4, 3]) * 5, 0)\n",
        "k = np.round(np.random.uniform(size=1) * 3, 0)\n",
        "B = np.einsum('ij,k->ij', A, k)\n",
        "\n",
        "print(\"Matrix Scalar Multiplication\")\n",
        "print(\"A{0}: \\n{1}\".format(A.shape, A))\n",
        "print(\"k{0}: {1}\".format(k.shape, k))\n",
        "print((\"einsum: %s,%s->%s\" % (str(A.shape), str(k.shape), str(B.shape))).replace(\" \",\"\"))\n",
        "print(\"B{0}: \\n{1}\".format(B.shape, B))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "dFBv8fmqy0J-",
        "outputId": "92de59ff-db39-4f88-ef32-736856fc744d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Scalar Multiplication\n",
            "A(4, 3): \n",
            "[[4. 2. 4.]\n",
            " [1. 1. 2.]\n",
            " [1. 4. 4.]\n",
            " [3. 0. 4.]]\n",
            "k(1,): [1.]\n",
            "einsum:(4,3),(1,)->(4,3)\n",
            "B(4, 3): \n",
            "[[4. 2. 4.]\n",
            " [1. 1. 2.]\n",
            " [1. 4. 4.]\n",
            " [3. 0. 4.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Matrix Multiplication (Dot Product):**\n",
        "\n",
        "   Matrix multiplication involves taking the dot product of rows and columns. If you have matrices $A$ and $B$ with dimensions $m \\times n$ and $n \\times p$, respectively, the resulting matrix $C$ will have dimensions $m \\times p$. The element at position $(i, j)$ in $C$ is obtained by summing the products of corresponding elements in the $i$-th row of $A$ and the $j$-th column of $B$:\n",
        "\n",
        "   $\n",
        "   C_{ij} = \\sum_{k=1}^{n} A_{ik} \\cdot B_{kj}\n",
        "   $\n"
      ],
      "metadata": {
        "id": "Z8_sPl2R4Haw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title numpy example\n",
        "import numpy as np\n",
        "\n",
        "A = np.round(np.random.uniform(size=[4, 3]) * 5, 0)\n",
        "B = np.round(np.random.uniform(size=[3, 4]) * 5, 0)\n",
        "\n",
        "C = np.einsum('ik,kj->ij', A, B)\n",
        "\n",
        "print(\"Matrix Multiplication (Dot Product)\")\n",
        "print(\"A{0}: \\n{1}\".format(A.shape, A))\n",
        "print(\"B{0}: \\n{1}\".format(B.shape, B))\n",
        "print((\"einsum: %s,%s->%s\" % (str(A.shape), str(B.shape), str(C.shape))).replace(\" \",\"\"))\n",
        "print(\"C{0}: \\n{1}\".format(C.shape, C))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "R8hEF_FJfzeF",
        "outputId": "3452ef69-af0c-42ee-f157-1b421120faff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Multiplication (Dot Product)\n",
            "A(4, 3): \n",
            "[[3. 2. 2.]\n",
            " [0. 3. 5.]\n",
            " [1. 2. 3.]\n",
            " [3. 2. 4.]]\n",
            "B(3, 4): \n",
            "[[2. 1. 5. 4.]\n",
            " [1. 2. 5. 4.]\n",
            " [2. 4. 2. 3.]]\n",
            "einsum:(4,3),(3,4)->(4,4)\n",
            "C(4, 4): \n",
            "[[12. 15. 29. 26.]\n",
            " [13. 26. 25. 27.]\n",
            " [10. 17. 21. 21.]\n",
            " [16. 23. 33. 32.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Hadamard Product (Element-wise Multiplication):**\n",
        "\n",
        "   If you have two matrices $A$ and $B$ of the same dimensions, the Hadamard product $C$ is obtained by multiplying corresponding elements:\n",
        "\n",
        "   $\n",
        "   C_{ij} = A_{ij} \\cdot B_{ij}\n",
        "   $\n"
      ],
      "metadata": {
        "id": "nIklX5iPf0XJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title numpy example\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "A = np.round(np.random.uniform(size=[4, 3]) * 5, 0)\n",
        "B = np.round(np.random.uniform(size=[4, 3]) * 5, 0)\n",
        "\n",
        "C = np.einsum('ij,ij->ij', A, B)\n",
        "\n",
        "print(\"Hadamard Product (Element-wise Multiplication)\")\n",
        "print(\"A{0}: \\n{1}\".format(A.shape, A))\n",
        "print(\"B{0}: \\n{1}\".format(B.shape, B))\n",
        "print((\"einsum: %s,%s->%s\" % (str(A.shape), str(B.shape), str(C.shape))).replace(\" \",\"\"))\n",
        "print(\"C{0}: \\n{1}\".format(C.shape, C))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "bcP42YzdgRcD",
        "outputId": "24d35478-2f7a-4a6a-c800-c173da185fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadamard Product (Element-wise Multiplication)\n",
            "A(4, 3): \n",
            "[[4. 2. 4.]\n",
            " [0. 2. 5.]\n",
            " [5. 5. 4.]\n",
            " [3. 2. 1.]]\n",
            "B(4, 3): \n",
            "[[3. 4. 2.]\n",
            " [2. 1. 3.]\n",
            " [4. 1. 4.]\n",
            " [3. 2. 4.]]\n",
            "einsum:(4,3),(4,3)->(4,3)\n",
            "C(4, 3): \n",
            "[[12.  8.  8.]\n",
            " [ 0.  2. 15.]\n",
            " [20.  5. 16.]\n",
            " [ 9.  4.  4.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Transpose:**\n",
        "\n",
        "   If you have a matrix $A$ with dimensions $m \\times n$, the transpose $B$ is obtained by swapping its rows and columns. The element at position $(i, j)$ in $B$ is the element at position $(j, i)$ in $A$:\n",
        "\n",
        "   $\n",
        "   B_{ij} = A_{ji}\n",
        "   $\n"
      ],
      "metadata": {
        "id": "3g8Wr1DEgRpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title numpy example\n",
        "import numpy as np\n",
        "\n",
        "A = np.round(np.random.uniform(size=[4, 3]) * 5, 0)\n",
        "B = np.einsum('ji->ij', A)\n",
        "\n",
        "print(\"Transpose\")\n",
        "print(\"A{0}: \\n{1}\".format(A.shape, A))\n",
        "\n",
        "print((\"einsum: %s->%s\" % (str(A.shape), str(B.shape))).replace(\" \",\"\"))\n",
        "print(\"B{0}: \\n{1}\".format(B.shape, B))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "ZPAhCOYCgn_l",
        "outputId": "76c62a7d-602a-4fae-be06-e0812023e6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transpose\n",
            "A(4, 3): \n",
            "[[4. 4. 4.]\n",
            " [4. 3. 1.]\n",
            " [4. 2. 1.]\n",
            " [2. 4. 1.]]\n",
            "einsum:(4,3)->(3,4)\n",
            "B(3, 4): \n",
            "[[4. 4. 4. 2.]\n",
            " [4. 3. 2. 4.]\n",
            " [4. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Trace:**\n",
        "\n",
        "   The trace of a square matrix $A$ is the sum of its diagonal elements:\n",
        "\n",
        "   $\n",
        "   \\text{tr}(A) = \\sum_{i=1}^{n} A_{ii}\n",
        "   $\n"
      ],
      "metadata": {
        "id": "cXAw1PUGgoG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title numpy example\n",
        "import numpy as np\n",
        "\n",
        "A = np.round(np.random.uniform(size=[3, 3]) * 5, 0)\n",
        "trace_A = np.einsum('ii', A)\n",
        "\n",
        "print(\"Trace\")\n",
        "print(\"A{0}: \\n{1}\".format(A.shape, A))\n",
        "\n",
        "print((\"einsum: %s->%s\" % (str(A.shape), str(\"(1)\"))).replace(\" \",\"\"))\n",
        "print(\"trace_A{0}: \\n{1}\".format(\"(1)\", trace_A))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "yTavHB3AhFeG",
        "outputId": "7baa3067-2bcf-4fa4-812e-8f371f2d0dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trace\n",
            "A(3, 3): \n",
            "[[2. 1. 1.]\n",
            " [1. 1. 3.]\n",
            " [4. 1. 4.]]\n",
            "einsum:(3,3)->(1)\n",
            "trace_A(1): \n",
            "7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Vector-Matrix Multiplication:**\n",
        "\n",
        "**Row Vector by Matrix:**\n",
        "\n",
        "a row vector $ \\mathbf{v} $ and a matrix $ \\mathbf{M} $, the result $ \\mathbf{r} $ is obtained by multiplying the row vector by each column of the matrix:\n",
        "\n",
        "$ \\mathbf{r}_i = \\sum_j \\mathbf{v}_j \\cdot \\mathbf{M}_{ji} $\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M014Rekg0LKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title numpy example\n",
        "import numpy as np\n",
        "\n",
        "# Row vector by matrix\n",
        "v = np.round(np.random.uniform(size=[3]) * 5, 0)\n",
        "M = np.round(np.random.uniform(size=[3, 4]) * 5, 0)\n",
        "\n",
        "r = np.einsum('j,ji->i', v, M)\n",
        "\n",
        "print(\"Row vector by matrix\")\n",
        "print(\"M{0}: \\n{1}\".format(M.shape, M))\n",
        "print(\"v{0}: \\n{1}\".format(v.shape, v))\n",
        "print((\"einsum: %s,%s->%s\" % (str(v.shape), str(M.shape), str(r.shape))).replace(\" \",\"\"))\n",
        "print(\"r{0}: \\n{1}\".format(r.shape, r))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "_ROTtbuT0nkP",
        "outputId": "b26e2cb1-9ba9-434d-ded0-0dd41aab5ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row vector by matrix\n",
            "M(3, 4): \n",
            "[[1. 2. 1. 1.]\n",
            " [5. 1. 4. 2.]\n",
            " [1. 3. 3. 2.]]\n",
            "v(3,): \n",
            "[1. 4. 1.]\n",
            "einsum:(3,),(3,4)->(4,)\n",
            "r(4,): \n",
            "[22.  9. 20. 11.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix by Column Vector:**\n",
        "\n",
        "a matrix $ \\mathbf{M} $ and a column vector $ \\mathbf{v} $, the result $ \\mathbf{r} $ is obtained by multiplying each row of the matrix by the column vector:\n",
        "\n",
        "$ \\mathbf{r}_i = \\sum_j \\mathbf{M}_{ij} \\cdot \\mathbf{v}_j $"
      ],
      "metadata": {
        "id": "KEvpXZRR11yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title numpy example\n",
        "\n",
        "import numpy  as np\n",
        "\n",
        "# Col vector by matrix\n",
        "v = np.round(np.random.uniform(size=[4]) * 5, 0)\n",
        "M = np.round(np.random.uniform(size=[3, 4]) * 5, 0)\n",
        "\n",
        "r = np.einsum('ij,j->i', M, v)\n",
        "\n",
        "print(\"Col vector by matrix\")\n",
        "print(\"M{0}: \\n{1}\".format(M.shape, M))\n",
        "print(\"v{0}: \\n{1}\".format(v.shape, v))\n",
        "print((\"einsum: %s,%s->%s\" % (str(M.shape), str(v.shape), str(r.shape))).replace(\" \",\"\"))\n",
        "print(\"r{0}: \\n{1}\".format(r.shape, r))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "ucjniNgJ133h",
        "outputId": "3c23b977-23d5-4d7d-82fa-037b261246e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Col vector by matrix\n",
            "M(3, 4): \n",
            "[[2. 3. 2. 3.]\n",
            " [4. 1. 2. 1.]\n",
            " [4. 5. 1. 0.]]\n",
            "v(4,): \n",
            "[2. 2. 0. 4.]\n",
            "einsum:(3,4),(4,)->(3,)\n",
            "r(3,): \n",
            "[22. 14. 18.]\n"
          ]
        }
      ]
    }
  ]
}