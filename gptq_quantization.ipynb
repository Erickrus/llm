{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjn7fCR3VKmbD9fC5PjT1/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erickrus/llm/blob/main/gptq_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPTQ\n",
        "\n",
        "https://arxiv.org/pdf/2210.17323\n",
        "\n",
        "GPTQ: ACCURATE POST-TRAINING QUANTIZATION FOR GENERATIVE PRE-TRAINED TRANSFORMERS\n",
        "\n",
        "Step 1: Arbitrary Order Insight.\n",
        "\n",
        "Step 2: Lazy Batch-Updates\n",
        "\n",
        "Step 3: Cholesky Reformulation\n",
        "\n",
        "leverage state-of-the-art Cholesky kernels to compute all information we will need from $H^{-1}$ upfront. In combination with mild dampening, the resulting method is robust enough to execute on huge models without issues. As a bonus, using a well-optimized Cholesky kernel also yields further speedup. We detail all small changes necessary for the Cholesky version of the algorithm next.\n",
        "\n",
        "$\\mathbf{Algorithm\\ 1}$ Quantize $\\mathbf{W}$ given inverse Hessian $H^{-1} = (2XX^\\top+λI)^{-1}$ and Blocksize $B$.\n",
        "\n",
        "\n",
        "$\\mathbf{Q} \\leftarrow \\mathbf{0}_{d_{\\text{row}} \\times d_{\\text{col}}}$ //quantize output\n",
        "\n",
        "$\\mathbf{E} \\leftarrow \\mathbf{0}_{d_{\\text{row}} \\times B}$ // block quantization errors\n",
        "\n",
        "$\\mathbf{H}^{-1} \\leftarrow \\text{Cholesky}(\\mathbf{H}^{-1})^\\top$ // Hessian inverse information\n",
        "\n",
        "$\\mathbf{for\\ } i = 0, B, 2B, \\ldots \\text{do}$\n",
        "\n",
        "$\\ \\ \\ \\ \\mathbf{for\\ } j = i, \\ldots, i + B - 1 \\mathbf{\\ do}$\n",
        "\n",
        "$\\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{Q}_{:,j} \\leftarrow \\text{quant}(\\mathbf{W}_{:,j})$ // quantize column\n",
        "\n",
        "$\\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{E}_{:,j-i} \\leftarrow (\\mathbf{W}_{:,j} - \\mathbf{Q}_{:,j}) / [\\mathbf{H}^{-1}]_{jj}$ // quantization error\n",
        "\n",
        "$\\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{W}_{:,j:(i+B)} \\leftarrow \\mathbf{W}_{:,j:(i+B)} - \\mathbf{E}_{:,j-i} \\cdot \\mathbf{H}^{-1}_{jj,j:(i+B)}$ // update weights in block\n",
        "\n",
        "$\\ \\ \\ \\ \\mathbf{end\\ for}$\n",
        "\n",
        "$\\ \\ \\ \\ \\mathbf{W}_{:,(i+B):} \\leftarrow \\mathbf{W}_{:,(i+B):} - \\mathbf{E} \\cdot \\mathbf{H}^{-1}_{i:(i+B),(i+B):}$ // update all remaning weights\n",
        "\n",
        "$\\mathbf{end\\ for}$\n"
      ],
      "metadata": {
        "id": "hDGm4woWOxnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pretrained model to do quantization requires CUDA available.\n",
        "\n",
        "High memory notebook setting, if needed"
      ],
      "metadata": {
        "id": "ZyKLjjPHE7Qi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/AutoGPTQ/AutoGPTQ"
      ],
      "metadata": {
        "id": "37KYPtToJGyV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y23LTFGk_-O0",
        "outputId": "a83e0045-d9b3-4e2a-96b2-49afcf07e462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 install -q transformers optimum accelerate auto-gptq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title quant_autogptq.py\n",
        "#!wget https://raw.githubusercontent.com/TheBlokeAI/AIScripts/main/quant_autogptq.py\n",
        "%%writefile /content/quant_autogptq.py\n",
        "#\n",
        "# A wrapper script to quantise models with GPTQ, from one of various datasets\n",
        "#\n",
        "\n",
        "import time\n",
        "import os\n",
        "import logging\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "\n",
        "class QuantAutoGPTQ:\n",
        "    def __init__(self, model_name_or_path, output_dir, dataset,\n",
        "                 num_samples=128, trust_remote_code=False, cache_examples=True,\n",
        "                 use_fast=True, use_triton=False, bits=[4], group_size=[128], damp=[0.01],\n",
        "                 desc_act=[False], dtype='float16', seqlen=2048, batch_size=1, stop_file=None,\n",
        "                 make_folder=False, GPU=0, cuda_alloc_conf=None):\n",
        "\n",
        "        # Limit visible GPU to the one specified\n",
        "        # We don't currently support multi-GPU, as AutoGPTQ can't use more than one GPU for quant anyway.\n",
        "        #os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)\n",
        "\n",
        "        # Allow specifying CUDA allocation config, eg PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32\n",
        "        # This can allow for quantising larger models without running out of VRAM\n",
        "        #if cuda_alloc_conf is not None:\n",
        "        #    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = cuda_alloc_conf\n",
        "\n",
        "        self.pretrained_model_dir = model_name_or_path\n",
        "        self.output_dir_base = output_dir\n",
        "        self.dataset = dataset\n",
        "        self.num_samples = num_samples\n",
        "        self.trust_remote_code = trust_remote_code\n",
        "        self.cache_examples = cache_examples\n",
        "        self.use_fast = use_fast\n",
        "        self.use_triton = use_triton\n",
        "\n",
        "        def check_list(item):\n",
        "            return item if isinstance(item, list) else [item]\n",
        "\n",
        "        self.bits = check_list(bits)\n",
        "        self.group_size = check_list(group_size)\n",
        "        self.desc_act = check_list(desc_act)\n",
        "        self.damp = check_list(damp)\n",
        "\n",
        "        self.dtype = dtype\n",
        "        self.seqlen = seqlen\n",
        "        self.batch_size = batch_size\n",
        "        self.stop_file = stop_file\n",
        "        self.make_folder = make_folder\n",
        "\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.propagate = True\n",
        "\n",
        "        from transformers import AutoTokenizer\n",
        "        self.logger.info(\"Loading tokenizer\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.pretrained_model_dir,\n",
        "                                                       use_fast=self.use_fast,\n",
        "                                                       trust_remote_code=self.trust_remote_code)\n",
        "\n",
        "    @staticmethod\n",
        "    def append_dataset(tokenized, num_samples, seqlen):\n",
        "        import numpy as np\n",
        "        import torch\n",
        "\n",
        "        random.seed(0)\n",
        "        np.random.seed(0)\n",
        "        torch.random.manual_seed(0)\n",
        "\n",
        "        traindataset = []\n",
        "        for _ in range(num_samples):\n",
        "            i = random.randint(0, tokenized.input_ids.shape[1] - seqlen - 1)\n",
        "            j = i + seqlen\n",
        "            inp = tokenized.input_ids[:, i:j]\n",
        "            attention_mask = torch.ones_like(inp)\n",
        "            traindataset.append({'input_ids':inp,'attention_mask': attention_mask})\n",
        "        return traindataset\n",
        "\n",
        "    #TODO: make a generic method that can load a dataset from HF hub and be told what column(s) to use\n",
        "    def get_math(self):\n",
        "        data = load_dataset('andersonbcdefg/math', split='train')\n",
        "\n",
        "        extract = data[0:2000]\n",
        "        text = ''\n",
        "        for input, output in zip(extract['message_1'], extract['message_2']):\n",
        "            text += input + ': ' + output + '\\n'\n",
        "\n",
        "        self.logger.info(\"Tokenising Maths dataset\")\n",
        "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
        "    def get_medical(self):\n",
        "        data = load_dataset('medalpaca/medical_meadow_wikidoc', split='train')\n",
        "\n",
        "        extract = data[0:1000]\n",
        "        text = ''\n",
        "        for input, output in zip(extract['input'], extract['output']):\n",
        "            text += input + ' ' + output + '\\n'\n",
        "\n",
        "        self.logger.info(\"Tokenising Medical dataset\")\n",
        "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
        "\n",
        "    def get_code(self):\n",
        "        data = load_dataset('nickrosh/Evol-Instruct-Code-80k-v1', split='train')\n",
        "\n",
        "        extract = data[0:1500]\n",
        "        text = '\\n'.join(extract['output'])\n",
        "        self.logger.info(\"Tokenising Code dataset\")\n",
        "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
        "\n",
        "    def get_spanish(self):\n",
        "        data = load_dataset('bertin-project/alpaca-spanish', split='train')\n",
        "\n",
        "        subset_data = data.select(range(5000))\n",
        "        text = '\\n'.join(item['output'] for item in subset_data)\n",
        "\n",
        "        self.logger.info(\"Tokenising Spanish dataset\")\n",
        "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
        "\n",
        "    def get_german(self):\n",
        "        data = load_dataset('deepset/germanquad', split='train')\n",
        "\n",
        "        def transform_context(sample):\n",
        "            split_context = sample['context'].split('===')\n",
        "            if len(split_context) >= 3:\n",
        "                trans_context = split_context[2]\n",
        "            else:\n",
        "                trans_context = sample['context']\n",
        "            return {'context': trans_context.strip()}\n",
        "\n",
        "        subset_data = data.select(range(2000))\n",
        "        transformed_subset = subset_data.map(transform_context)\n",
        "        text = '\\n'.join([item['context'] for item in transformed_subset])\n",
        "\n",
        "        self.logger.info(\"Tokenising German dataset\")\n",
        "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
        "\n",
        "    def get_french(self):\n",
        "        data = load_dataset('gustavecortal/diverse_french_news', split='train')\n",
        "\n",
        "        extract = data[0:700]\n",
        "        text = '\\n'.join(extract['text'])\n",
        "        self.logger.info(\"Tokenising French dataset\")\n",
        "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
        "\n",
        "    def get_wikitext2(self):\n",
        "        wikidata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "        wikilist = [' \\n' if s == '' else s for s in wikidata['text'] ]\n",
        "\n",
        "        text = ''.join(wikilist)\n",
        "        self.logger.info(\"Tokenising wikitext2\")\n",
        "        tokenized = self.tokenizer(text, return_tensors='pt')\n",
        "\n",
        "        return self.append_dataset(tokenized, self.num_samples, self.seqlen)\n",
        "\n",
        "    def get_c4(self):\n",
        "        import numpy as np\n",
        "        import torch\n",
        "        traindata = load_dataset(\n",
        "            'allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train', use_auth_token=False\n",
        "        )\n",
        "\n",
        "        trainloader = []\n",
        "        for _ in range(self.num_samples):\n",
        "            while True:\n",
        "                i = random.randint(0, len(traindata) - 1)\n",
        "                trainenc = self.tokenizer(traindata[i]['text'], return_tensors='pt')\n",
        "                if trainenc.input_ids.shape[1] >= self.seqlen:\n",
        "                    break\n",
        "            i = random.randint(0, trainenc.input_ids.shape[1] - self.seqlen - 1)\n",
        "            j = i + self.seqlen\n",
        "            inp = trainenc.input_ids[:, i:j]\n",
        "            attention_mask = torch.ones_like(inp)\n",
        "            trainloader.append({'input_ids':inp,'attention_mask': attention_mask})\n",
        "\n",
        "        return trainloader\n",
        "\n",
        "    def quantize(self, output_dir, traindataset, bits, group_size, desc_act, damp):\n",
        "        # Hide the super annoying bitsandbytes loading message. We don't even use BnB but I don't know if I can stop it loading entirely.\n",
        "        os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
        "\n",
        "        # We only import Torch and AutoGPTQ when needed, so that earlier set env vars will affect them.\n",
        "        import torch\n",
        "        from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
        "\n",
        "        quantize_config = BaseQuantizeConfig(\n",
        "            bits=bits,\n",
        "            group_size=group_size,\n",
        "            desc_act=desc_act,\n",
        "            damp_percent=damp\n",
        "        )\n",
        "\n",
        "        if self.dtype == 'float16':\n",
        "            torch_dtype  = torch.float16\n",
        "        elif self.dtype == 'float32':\n",
        "            torch_dtype  = torch.float32\n",
        "        elif self.dtype == 'bfloat16':\n",
        "            torch_dtype  = torch.bfloat16\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dtype: {self.dtype}\")\n",
        "\n",
        "        self.logger.info(f\"Loading model from {self.pretrained_model_dir} with trust_remote_code={self.trust_remote_code} and dtype={torch_dtype}\")\n",
        "        model = AutoGPTQForCausalLM.from_pretrained(self.pretrained_model_dir, quantize_config=quantize_config,\n",
        "                                                    low_cpu_mem_usage=True, torch_dtype=torch_dtype, trust_remote_code=self.trust_remote_code)\n",
        "\n",
        "        self.logger.info(f\"Starting quantization to {output_dir} with use_triton={self.use_triton}\")\n",
        "        start_time = time.time()\n",
        "        model.quantize(traindataset, use_triton=self.use_triton, batch_size=self.batch_size, cache_examples_on_gpu=self.cache_examples)\n",
        "\n",
        "        self.logger.info(f\"Time to quantize model at {output_dir} with use_triton={self.use_triton}: {time.time() - start_time:.2f}\")\n",
        "\n",
        "        self.logger.info(f\"Saving quantized model to {output_dir}\")\n",
        "        model.save_quantized(output_dir, use_safetensors=True)\n",
        "        self.logger.info(f\"Saving tokenizer to {output_dir}\")\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "        self.logger.info(\"Done.\")\n",
        "\n",
        "    def run_quantization(self):\n",
        "        #TODO: This is messy, should be dynamic\n",
        "        if self.dataset == 'wikitext':\n",
        "            traindataset = self.get_wikitext2()\n",
        "        elif self.dataset == 'code' or self.dataset == 'evol-instruct-code':\n",
        "            traindataset = self.get_code()\n",
        "        elif self.dataset == 'math' or self.dataset == 'maths' or self.dataset == 'camel-ai/math':\n",
        "            traindataset = self.get_math()\n",
        "        elif self.dataset == 'medical' or self.dataset == 'medical_meadow_wikidoc':\n",
        "            traindataset = self.get_medical()\n",
        "        elif self.dataset == 'spanish':\n",
        "            traindataset = self.get_spanish()\n",
        "        elif self.dataset == 'german' or self.dataset == 'germanquad':\n",
        "            traindataset = self.get_german()\n",
        "        elif self.dataset == 'french' or self.dataset == 'diverse_french_news':\n",
        "            traindataset = self.get_french()\n",
        "        elif self.dataset == 'c4':\n",
        "            traindataset = self.get_c4()\n",
        "        else:\n",
        "            self.logger.error(f\"Unsupported dataset: {self.dataset}\")\n",
        "            raise ValueError(f\"Unsupported dataset: {self.dataset}\")\n",
        "\n",
        "        abort = False\n",
        "        iterations=[]\n",
        "        for bits in self.bits:\n",
        "            for group_size in self.group_size:\n",
        "                for desc_act in self.desc_act:\n",
        "                    for damp in self.damp:\n",
        "                        desc_act = desc_act == 1 and True or False\n",
        "                        iterations.append({\"bits\": bits, \"group_size\": group_size, \"desc_act\": desc_act, \"damp\": damp})\n",
        "\n",
        "        num_iters = len(iterations)\n",
        "        if num_iters > 1:\n",
        "            logger.info(f\"Starting {num_iters} quantizations.\")\n",
        "        count=1\n",
        "        for iteration in iterations:\n",
        "            if abort:\n",
        "                break\n",
        "            if self.stop_file is not None and os.path.exists(self.stop_file):\n",
        "                self.logger.info(f\"Stopping as {self.stop_file} exists\")\n",
        "                abort = True\n",
        "                break\n",
        "\n",
        "            bits = iteration['bits']\n",
        "            group_size = iteration['group_size']\n",
        "            desc_act = iteration['desc_act']\n",
        "            damp = iteration['damp']\n",
        "\n",
        "        try:\n",
        "            if self.make_folder:\n",
        "                output_dir = os.path.join(self.output_dir_base, f\"{bits}bits-{group_size}g-desc_act_{desc_act}-damp_{damp}\")\n",
        "            else:\n",
        "                output_dir = self.output_dir_base\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            try:\n",
        "                if num_iters > 1:\n",
        "                    self.logger.info(f\"Starting quantization {count}/{num_iters}\")\n",
        "                self.logger.info(f\"Quantising with bits={bits} group_size={group_size} desc_act={desc_act} damp={damp} to {output_dir}\")\n",
        "                self.quantize(output_dir, traindataset, bits, group_size, desc_act, damp)\n",
        "            except KeyboardInterrupt:\n",
        "                logger.error(f\"Aborted. Will delete {output_dir}\")\n",
        "                os.rmdir(output_dir)\n",
        "                abort = True\n",
        "            except:\n",
        "                raise\n",
        "\n",
        "        finally:\n",
        "            count += 1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    logger = logging.getLogger()\n",
        "    logging.basicConfig(format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\",\n",
        "                        level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='AutoGPTQ quantize')\n",
        "    parser.add_argument('pretrained_model_dir', type=str, help='Repo name')\n",
        "    parser.add_argument('output_dir_base', type=str, help='Output base folder')\n",
        "    parser.add_argument('dataset', type=str, help='Quantisation dataset')\n",
        "    parser.add_argument('--num_samples', type=int, default=128, help='Number of dataset samples')\n",
        "    parser.add_argument('--trust_remote_code', action=\"store_true\", help='Trust remote code')\n",
        "    parser.add_argument('--cache_examples', type=int, default=1, help='Cache examples on GPU')\n",
        "    parser.add_argument('--use_fast', action=\"store_true\", help='Use fast tokenizer')\n",
        "    parser.add_argument('--use_triton', action=\"store_true\", help='Use Triton for quantization')\n",
        "    parser.add_argument('--bits', type=int, nargs='+', default=[4], help='Quantize bit(s)')\n",
        "    parser.add_argument('--group_size', type=int, nargs='+', default=[128], help='Quantize group size(s)')\n",
        "    parser.add_argument('--damp', type=float, nargs='+', default=[0.01], help='Quantize damp_percent(s)')\n",
        "    parser.add_argument('--desc_act', type=int, nargs='+', default=[0], help='Quantize desc_act(s) - 1 = True, 0 = False')\n",
        "    parser.add_argument('--dtype', type=str, choices=['float16', 'float32', 'bfloat16'], default='float16', help='Unquantised model dtype')\n",
        "    parser.add_argument('--seqlen', type=int, default=2048, help='Model sequence length')\n",
        "    parser.add_argument('--batch_size', type=int, default=1, help='Quantize batch size for processing dataset samples')\n",
        "    parser.add_argument('--stop_file', type=str, help='Filename to look for to stop inference, specific to this instance')\n",
        "    parser.add_argument('--make_folders', action=\"store_true\", help='Make folders for each quantization using params in folder name')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    quantizer = QuantAutoGPTQ(args.pretrained_model_dir,\n",
        "                              args.output_dir_base,\n",
        "                              args.dataset,\n",
        "                              num_samples=args.num_samples,\n",
        "                              trust_remote_code=args.trust_remote_code,\n",
        "                              cache_examples=args.cache_examples,\n",
        "                              use_fast=args.use_fast,\n",
        "                              use_triton=args.use_triton,\n",
        "                              bits=args.bits,\n",
        "                              group_size=args.group_size,\n",
        "                              desc_act=args.desc_act,\n",
        "                              damp=args.damp,\n",
        "                              dtype=args.dtype,\n",
        "                              seqlen=args.seqlen,\n",
        "                              batch_size=args.batch_size,\n",
        "                              stop_file=args.stop_file,\n",
        "                              make_folder=args.make_folders)\n",
        "    quantizer.run_quantization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "Ym5tWV5fBBGL",
        "outputId": "57e46d2e-ae7b-46b6-af7e-669dfe46497f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/quant_autogptq.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qwen/Qwen1.5-0.5B takes 6 mins. GPTQ converting is not efficient. For larger model, it takes ever longer"
      ],
      "metadata": {
        "id": "DFTotrF8O9nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title quantize using autogptq\n",
        "#def run_quantization(self):\n",
        "#    #TODO: This is messy, should be dynamic\n",
        "MODEL_NAME = \"Qwen/Qwen1.5-0.5B\" # @param {type: \"string\"}\n",
        "ACTL_MODEL_NAME = MODEL_NAME.split('/')[-1]\n",
        "\n",
        "print(f\"processing {MODEL_NAME}\")\n",
        "!python3 quant_autogptq.py  \\\n",
        "  {MODEL_NAME} \\\n",
        "  {ACTL_MODEL_NAME} \\\n",
        "  wikitext \\\n",
        "  --bits 4 \\\n",
        "  --group_size 128 \\\n",
        "  --desc_act 1 \\\n",
        "  --damp 0.1 \\\n",
        "  --seqlen 4096\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "m9zkPAF6AwO4",
        "outputId": "1b8f4792-a876-4f15-c266-5aec5ded1f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing Qwen/Qwen1.5-0.5B\n",
            "2024-05-15 03:03:08 INFO [__main__] Loading tokenizer\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "tokenizer_config.json: 100% 1.29k/1.29k [00:00<00:00, 8.02MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 23.6MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 24.8MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 31.6MB/s]\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "2024-05-15 03:03:11 INFO [__main__] Tokenising wikitext2\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (299078 > 32768). Running this sequence through the model will result in indexing errors\n",
            "2024-05-15 03:03:13 INFO [__main__] Quantising with bits=4 group_size=128 desc_act=True damp=0.1 to Qwen1.5-0.5B\n",
            "2024-05-15 03:03:15 INFO [__main__] Loading model from Qwen/Qwen1.5-0.5B with trust_remote_code=False and dtype=torch.float16\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "config.json: 100% 661/661 [00:00<00:00, 4.42MB/s]\n",
            "model.safetensors: 100% 1.24G/1.24G [00:09<00:00, 134MB/s]\n",
            "generation_config.json: 100% 138/138 [00:00<00:00, 889kB/s]\n",
            "2024-05-15 03:03:26 INFO [__main__] Starting quantization to Qwen1.5-0.5B with use_triton=False\n",
            "INFO - Start quantizing layer 1/24\n",
            "2024-05-15 03:03:27 INFO [auto_gptq.modeling._base] Start quantizing layer 1/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 1/24...\n",
            "2024-05-15 03:03:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 1/24...\n",
            "2024-05-15 03:03:30 INFO [auto_gptq.quantization.gptq] duration: 0.8540999889373779\n",
            "2024-05-15 03:03:30 INFO [auto_gptq.quantization.gptq] avg loss: 5.545071601867676\n",
            "INFO - Quantizing self_attn.v_proj in layer 1/24...\n",
            "2024-05-15 03:03:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 1/24...\n",
            "2024-05-15 03:03:31 INFO [auto_gptq.quantization.gptq] duration: 0.4001796245574951\n",
            "2024-05-15 03:03:31 INFO [auto_gptq.quantization.gptq] avg loss: 0.19078782200813293\n",
            "INFO - Quantizing self_attn.q_proj in layer 1/24...\n",
            "2024-05-15 03:03:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 1/24...\n",
            "2024-05-15 03:03:31 INFO [auto_gptq.quantization.gptq] duration: 0.4098320007324219\n",
            "2024-05-15 03:03:31 INFO [auto_gptq.quantization.gptq] avg loss: 8.011825561523438\n",
            "INFO - Quantizing self_attn.o_proj in layer 1/24...\n",
            "2024-05-15 03:03:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 1/24...\n",
            "2024-05-15 03:03:33 INFO [auto_gptq.quantization.gptq] duration: 0.6721529960632324\n",
            "2024-05-15 03:03:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.01088357251137495\n",
            "INFO - Quantizing mlp.up_proj in layer 1/24...\n",
            "2024-05-15 03:03:34 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 1/24...\n",
            "2024-05-15 03:03:35 INFO [auto_gptq.quantization.gptq] duration: 0.6699907779693604\n",
            "2024-05-15 03:03:35 INFO [auto_gptq.quantization.gptq] avg loss: 41.445701599121094\n",
            "INFO - Quantizing mlp.gate_proj in layer 1/24...\n",
            "2024-05-15 03:03:35 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 1/24...\n",
            "2024-05-15 03:03:35 INFO [auto_gptq.quantization.gptq] duration: 0.4093747138977051\n",
            "2024-05-15 03:03:35 INFO [auto_gptq.quantization.gptq] avg loss: 47.7689208984375\n",
            "INFO - Quantizing mlp.down_proj in layer 1/24...\n",
            "2024-05-15 03:03:38 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 1/24...\n",
            "2024-05-15 03:03:40 INFO [auto_gptq.quantization.gptq] duration: 1.7655222415924072\n",
            "2024-05-15 03:03:40 INFO [auto_gptq.quantization.gptq] avg loss: 1.9428107738494873\n",
            "INFO - Start quantizing layer 2/24\n",
            "2024-05-15 03:03:41 INFO [auto_gptq.modeling._base] Start quantizing layer 2/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 2/24...\n",
            "2024-05-15 03:03:43 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 2/24...\n",
            "2024-05-15 03:03:44 INFO [auto_gptq.quantization.gptq] duration: 0.688976526260376\n",
            "2024-05-15 03:03:44 INFO [auto_gptq.quantization.gptq] avg loss: 93.91813659667969\n",
            "INFO - Quantizing self_attn.v_proj in layer 2/24...\n",
            "2024-05-15 03:03:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 2/24...\n",
            "2024-05-15 03:03:44 INFO [auto_gptq.quantization.gptq] duration: 0.40116214752197266\n",
            "2024-05-15 03:03:44 INFO [auto_gptq.quantization.gptq] avg loss: 11.481698036193848\n",
            "INFO - Quantizing self_attn.q_proj in layer 2/24...\n",
            "2024-05-15 03:03:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 2/24...\n",
            "2024-05-15 03:03:45 INFO [auto_gptq.quantization.gptq] duration: 0.40104079246520996\n",
            "2024-05-15 03:03:45 INFO [auto_gptq.quantization.gptq] avg loss: 85.22100067138672\n",
            "INFO - Quantizing self_attn.o_proj in layer 2/24...\n",
            "2024-05-15 03:03:46 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 2/24...\n",
            "2024-05-15 03:03:46 INFO [auto_gptq.quantization.gptq] duration: 0.6748299598693848\n",
            "2024-05-15 03:03:46 INFO [auto_gptq.quantization.gptq] avg loss: 0.3342207074165344\n",
            "INFO - Quantizing mlp.up_proj in layer 2/24...\n",
            "2024-05-15 03:03:48 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 2/24...\n",
            "2024-05-15 03:03:49 INFO [auto_gptq.quantization.gptq] duration: 0.6952757835388184\n",
            "2024-05-15 03:03:49 INFO [auto_gptq.quantization.gptq] avg loss: 55.186317443847656\n",
            "INFO - Quantizing mlp.gate_proj in layer 2/24...\n",
            "2024-05-15 03:03:49 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 2/24...\n",
            "2024-05-15 03:03:49 INFO [auto_gptq.quantization.gptq] duration: 0.40296006202697754\n",
            "2024-05-15 03:03:49 INFO [auto_gptq.quantization.gptq] avg loss: 83.87664794921875\n",
            "INFO - Quantizing mlp.down_proj in layer 2/24...\n",
            "2024-05-15 03:03:52 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 2/24...\n",
            "2024-05-15 03:03:54 INFO [auto_gptq.quantization.gptq] duration: 1.7385950088500977\n",
            "2024-05-15 03:03:54 INFO [auto_gptq.quantization.gptq] avg loss: 1.3876993656158447\n",
            "INFO - Start quantizing layer 3/24\n",
            "2024-05-15 03:03:55 INFO [auto_gptq.modeling._base] Start quantizing layer 3/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 3/24...\n",
            "2024-05-15 03:03:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 3/24...\n",
            "2024-05-15 03:03:58 INFO [auto_gptq.quantization.gptq] duration: 0.7043142318725586\n",
            "2024-05-15 03:03:58 INFO [auto_gptq.quantization.gptq] avg loss: 181.75106811523438\n",
            "INFO - Quantizing self_attn.v_proj in layer 3/24...\n",
            "2024-05-15 03:03:58 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 3/24...\n",
            "2024-05-15 03:03:58 INFO [auto_gptq.quantization.gptq] duration: 0.4040718078613281\n",
            "2024-05-15 03:03:58 INFO [auto_gptq.quantization.gptq] avg loss: 29.481916427612305\n",
            "INFO - Quantizing self_attn.q_proj in layer 3/24...\n",
            "2024-05-15 03:03:58 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 3/24...\n",
            "2024-05-15 03:03:58 INFO [auto_gptq.quantization.gptq] duration: 0.4004032611846924\n",
            "2024-05-15 03:03:58 INFO [auto_gptq.quantization.gptq] avg loss: 168.6260986328125\n",
            "INFO - Quantizing self_attn.o_proj in layer 3/24...\n",
            "2024-05-15 03:04:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 3/24...\n",
            "2024-05-15 03:04:00 INFO [auto_gptq.quantization.gptq] duration: 0.6609644889831543\n",
            "2024-05-15 03:04:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.3637286424636841\n",
            "INFO - Quantizing mlp.up_proj in layer 3/24...\n",
            "2024-05-15 03:04:02 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 3/24...\n",
            "2024-05-15 03:04:03 INFO [auto_gptq.quantization.gptq] duration: 0.7102339267730713\n",
            "2024-05-15 03:04:03 INFO [auto_gptq.quantization.gptq] avg loss: 73.7072525024414\n",
            "INFO - Quantizing mlp.gate_proj in layer 3/24...\n",
            "2024-05-15 03:04:03 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 3/24...\n",
            "2024-05-15 03:04:03 INFO [auto_gptq.quantization.gptq] duration: 0.40817761421203613\n",
            "2024-05-15 03:04:03 INFO [auto_gptq.quantization.gptq] avg loss: 108.6416015625\n",
            "INFO - Quantizing mlp.down_proj in layer 3/24...\n",
            "2024-05-15 03:04:06 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 3/24...\n",
            "2024-05-15 03:04:08 INFO [auto_gptq.quantization.gptq] duration: 1.7419590950012207\n",
            "2024-05-15 03:04:08 INFO [auto_gptq.quantization.gptq] avg loss: 1.873477578163147\n",
            "INFO - Start quantizing layer 4/24\n",
            "2024-05-15 03:04:09 INFO [auto_gptq.modeling._base] Start quantizing layer 4/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 4/24...\n",
            "2024-05-15 03:04:11 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 4/24...\n",
            "2024-05-15 03:04:12 INFO [auto_gptq.quantization.gptq] duration: 0.7047290802001953\n",
            "2024-05-15 03:04:12 INFO [auto_gptq.quantization.gptq] avg loss: 176.00274658203125\n",
            "INFO - Quantizing self_attn.v_proj in layer 4/24...\n",
            "2024-05-15 03:04:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 4/24...\n",
            "2024-05-15 03:04:12 INFO [auto_gptq.quantization.gptq] duration: 0.4107844829559326\n",
            "2024-05-15 03:04:12 INFO [auto_gptq.quantization.gptq] avg loss: 36.83160400390625\n",
            "INFO - Quantizing self_attn.q_proj in layer 4/24...\n",
            "2024-05-15 03:04:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 4/24...\n",
            "2024-05-15 03:04:12 INFO [auto_gptq.quantization.gptq] duration: 0.40509891510009766\n",
            "2024-05-15 03:04:12 INFO [auto_gptq.quantization.gptq] avg loss: 161.192626953125\n",
            "INFO - Quantizing self_attn.o_proj in layer 4/24...\n",
            "2024-05-15 03:04:14 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 4/24...\n",
            "2024-05-15 03:04:14 INFO [auto_gptq.quantization.gptq] duration: 0.684406042098999\n",
            "2024-05-15 03:04:14 INFO [auto_gptq.quantization.gptq] avg loss: 0.48817816376686096\n",
            "INFO - Quantizing mlp.up_proj in layer 4/24...\n",
            "2024-05-15 03:04:16 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 4/24...\n",
            "2024-05-15 03:04:17 INFO [auto_gptq.quantization.gptq] duration: 0.7018873691558838\n",
            "2024-05-15 03:04:17 INFO [auto_gptq.quantization.gptq] avg loss: 87.87564086914062\n",
            "INFO - Quantizing mlp.gate_proj in layer 4/24...\n",
            "2024-05-15 03:04:17 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 4/24...\n",
            "2024-05-15 03:04:17 INFO [auto_gptq.quantization.gptq] duration: 0.4125809669494629\n",
            "2024-05-15 03:04:17 INFO [auto_gptq.quantization.gptq] avg loss: 111.3815689086914\n",
            "INFO - Quantizing mlp.down_proj in layer 4/24...\n",
            "2024-05-15 03:04:20 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 4/24...\n",
            "2024-05-15 03:04:22 INFO [auto_gptq.quantization.gptq] duration: 1.7496960163116455\n",
            "2024-05-15 03:04:22 INFO [auto_gptq.quantization.gptq] avg loss: 2.640920639038086\n",
            "INFO - Start quantizing layer 5/24\n",
            "2024-05-15 03:04:23 INFO [auto_gptq.modeling._base] Start quantizing layer 5/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 5/24...\n",
            "2024-05-15 03:04:25 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 5/24...\n",
            "2024-05-15 03:04:26 INFO [auto_gptq.quantization.gptq] duration: 0.7319333553314209\n",
            "2024-05-15 03:04:26 INFO [auto_gptq.quantization.gptq] avg loss: 135.30538940429688\n",
            "INFO - Quantizing self_attn.v_proj in layer 5/24...\n",
            "2024-05-15 03:04:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 5/24...\n",
            "2024-05-15 03:04:26 INFO [auto_gptq.quantization.gptq] duration: 0.41628122329711914\n",
            "2024-05-15 03:04:26 INFO [auto_gptq.quantization.gptq] avg loss: 25.308780670166016\n",
            "INFO - Quantizing self_attn.q_proj in layer 5/24...\n",
            "2024-05-15 03:04:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 5/24...\n",
            "2024-05-15 03:04:27 INFO [auto_gptq.quantization.gptq] duration: 0.41306185722351074\n",
            "2024-05-15 03:04:27 INFO [auto_gptq.quantization.gptq] avg loss: 126.19010925292969\n",
            "INFO - Quantizing self_attn.o_proj in layer 5/24...\n",
            "2024-05-15 03:04:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 5/24...\n",
            "2024-05-15 03:04:29 INFO [auto_gptq.quantization.gptq] duration: 0.6880171298980713\n",
            "2024-05-15 03:04:29 INFO [auto_gptq.quantization.gptq] avg loss: 0.9270937442779541\n",
            "INFO - Quantizing mlp.up_proj in layer 5/24...\n",
            "2024-05-15 03:04:30 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 5/24...\n",
            "2024-05-15 03:04:31 INFO [auto_gptq.quantization.gptq] duration: 0.7137191295623779\n",
            "2024-05-15 03:04:31 INFO [auto_gptq.quantization.gptq] avg loss: 101.88079071044922\n",
            "INFO - Quantizing mlp.gate_proj in layer 5/24...\n",
            "2024-05-15 03:04:31 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 5/24...\n",
            "2024-05-15 03:04:31 INFO [auto_gptq.quantization.gptq] duration: 0.40967297554016113\n",
            "2024-05-15 03:04:31 INFO [auto_gptq.quantization.gptq] avg loss: 133.136474609375\n",
            "INFO - Quantizing mlp.down_proj in layer 5/24...\n",
            "2024-05-15 03:04:34 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 5/24...\n",
            "2024-05-15 03:04:36 INFO [auto_gptq.quantization.gptq] duration: 1.759986400604248\n",
            "2024-05-15 03:04:36 INFO [auto_gptq.quantization.gptq] avg loss: 3.006199836730957\n",
            "INFO - Start quantizing layer 6/24\n",
            "2024-05-15 03:04:38 INFO [auto_gptq.modeling._base] Start quantizing layer 6/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 6/24...\n",
            "2024-05-15 03:04:39 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 6/24...\n",
            "2024-05-15 03:04:40 INFO [auto_gptq.quantization.gptq] duration: 0.7051582336425781\n",
            "2024-05-15 03:04:40 INFO [auto_gptq.quantization.gptq] avg loss: 172.6698455810547\n",
            "INFO - Quantizing self_attn.v_proj in layer 6/24...\n",
            "2024-05-15 03:04:40 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 6/24...\n",
            "2024-05-15 03:04:41 INFO [auto_gptq.quantization.gptq] duration: 0.40392208099365234\n",
            "2024-05-15 03:04:41 INFO [auto_gptq.quantization.gptq] avg loss: 38.71511459350586\n",
            "INFO - Quantizing self_attn.q_proj in layer 6/24...\n",
            "2024-05-15 03:04:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 6/24...\n",
            "2024-05-15 03:04:41 INFO [auto_gptq.quantization.gptq] duration: 0.3979640007019043\n",
            "2024-05-15 03:04:41 INFO [auto_gptq.quantization.gptq] avg loss: 163.11265563964844\n",
            "INFO - Quantizing self_attn.o_proj in layer 6/24...\n",
            "2024-05-15 03:04:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 6/24...\n",
            "2024-05-15 03:04:43 INFO [auto_gptq.quantization.gptq] duration: 0.6883432865142822\n",
            "2024-05-15 03:04:43 INFO [auto_gptq.quantization.gptq] avg loss: 1.5033864974975586\n",
            "INFO - Quantizing mlp.up_proj in layer 6/24...\n",
            "2024-05-15 03:04:45 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 6/24...\n",
            "2024-05-15 03:04:45 INFO [auto_gptq.quantization.gptq] duration: 0.7172403335571289\n",
            "2024-05-15 03:04:45 INFO [auto_gptq.quantization.gptq] avg loss: 126.80630493164062\n",
            "INFO - Quantizing mlp.gate_proj in layer 6/24...\n",
            "2024-05-15 03:04:45 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 6/24...\n",
            "2024-05-15 03:04:46 INFO [auto_gptq.quantization.gptq] duration: 0.3986344337463379\n",
            "2024-05-15 03:04:46 INFO [auto_gptq.quantization.gptq] avg loss: 154.59840393066406\n",
            "INFO - Quantizing mlp.down_proj in layer 6/24...\n",
            "2024-05-15 03:04:49 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 6/24...\n",
            "2024-05-15 03:04:50 INFO [auto_gptq.quantization.gptq] duration: 1.7695729732513428\n",
            "2024-05-15 03:04:50 INFO [auto_gptq.quantization.gptq] avg loss: 35.53428649902344\n",
            "INFO - Start quantizing layer 7/24\n",
            "2024-05-15 03:04:52 INFO [auto_gptq.modeling._base] Start quantizing layer 7/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 7/24...\n",
            "2024-05-15 03:04:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 7/24...\n",
            "2024-05-15 03:04:55 INFO [auto_gptq.quantization.gptq] duration: 0.7144045829772949\n",
            "2024-05-15 03:04:55 INFO [auto_gptq.quantization.gptq] avg loss: 170.7969512939453\n",
            "INFO - Quantizing self_attn.v_proj in layer 7/24...\n",
            "2024-05-15 03:04:55 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 7/24...\n",
            "2024-05-15 03:04:55 INFO [auto_gptq.quantization.gptq] duration: 0.4105560779571533\n",
            "2024-05-15 03:04:55 INFO [auto_gptq.quantization.gptq] avg loss: 60.64095687866211\n",
            "INFO - Quantizing self_attn.q_proj in layer 7/24...\n",
            "2024-05-15 03:04:55 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 7/24...\n",
            "2024-05-15 03:04:55 INFO [auto_gptq.quantization.gptq] duration: 0.4091668128967285\n",
            "2024-05-15 03:04:55 INFO [auto_gptq.quantization.gptq] avg loss: 176.83596801757812\n",
            "INFO - Quantizing self_attn.o_proj in layer 7/24...\n",
            "2024-05-15 03:04:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 7/24...\n",
            "2024-05-15 03:04:57 INFO [auto_gptq.quantization.gptq] duration: 0.684424877166748\n",
            "2024-05-15 03:04:57 INFO [auto_gptq.quantization.gptq] avg loss: 1.9545893669128418\n",
            "INFO - Quantizing mlp.up_proj in layer 7/24...\n",
            "2024-05-15 03:04:59 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 7/24...\n",
            "2024-05-15 03:05:00 INFO [auto_gptq.quantization.gptq] duration: 0.7123749256134033\n",
            "2024-05-15 03:05:00 INFO [auto_gptq.quantization.gptq] avg loss: 139.50186157226562\n",
            "INFO - Quantizing mlp.gate_proj in layer 7/24...\n",
            "2024-05-15 03:05:00 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 7/24...\n",
            "2024-05-15 03:05:00 INFO [auto_gptq.quantization.gptq] duration: 0.4183177947998047\n",
            "2024-05-15 03:05:00 INFO [auto_gptq.quantization.gptq] avg loss: 182.63023376464844\n",
            "INFO - Quantizing mlp.down_proj in layer 7/24...\n",
            "2024-05-15 03:05:03 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 7/24...\n",
            "2024-05-15 03:05:05 INFO [auto_gptq.quantization.gptq] duration: 1.7525711059570312\n",
            "2024-05-15 03:05:05 INFO [auto_gptq.quantization.gptq] avg loss: 4.999265193939209\n",
            "INFO - Start quantizing layer 8/24\n",
            "2024-05-15 03:05:06 INFO [auto_gptq.modeling._base] Start quantizing layer 8/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 8/24...\n",
            "2024-05-15 03:05:08 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 8/24...\n",
            "2024-05-15 03:05:09 INFO [auto_gptq.quantization.gptq] duration: 0.7047684192657471\n",
            "2024-05-15 03:05:09 INFO [auto_gptq.quantization.gptq] avg loss: 197.92376708984375\n",
            "INFO - Quantizing self_attn.v_proj in layer 8/24...\n",
            "2024-05-15 03:05:09 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 8/24...\n",
            "2024-05-15 03:05:09 INFO [auto_gptq.quantization.gptq] duration: 0.397611141204834\n",
            "2024-05-15 03:05:09 INFO [auto_gptq.quantization.gptq] avg loss: 76.96829223632812\n",
            "INFO - Quantizing self_attn.q_proj in layer 8/24...\n",
            "2024-05-15 03:05:09 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 8/24...\n",
            "2024-05-15 03:05:10 INFO [auto_gptq.quantization.gptq] duration: 0.42058300971984863\n",
            "2024-05-15 03:05:10 INFO [auto_gptq.quantization.gptq] avg loss: 205.28399658203125\n",
            "INFO - Quantizing self_attn.o_proj in layer 8/24...\n",
            "2024-05-15 03:05:11 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 8/24...\n",
            "2024-05-15 03:05:12 INFO [auto_gptq.quantization.gptq] duration: 0.6877548694610596\n",
            "2024-05-15 03:05:12 INFO [auto_gptq.quantization.gptq] avg loss: 2.2636749744415283\n",
            "INFO - Quantizing mlp.up_proj in layer 8/24...\n",
            "2024-05-15 03:05:13 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 8/24...\n",
            "2024-05-15 03:05:14 INFO [auto_gptq.quantization.gptq] duration: 0.6995589733123779\n",
            "2024-05-15 03:05:14 INFO [auto_gptq.quantization.gptq] avg loss: 149.04830932617188\n",
            "INFO - Quantizing mlp.gate_proj in layer 8/24...\n",
            "2024-05-15 03:05:14 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 8/24...\n",
            "2024-05-15 03:05:14 INFO [auto_gptq.quantization.gptq] duration: 0.40856432914733887\n",
            "2024-05-15 03:05:14 INFO [auto_gptq.quantization.gptq] avg loss: 176.56558227539062\n",
            "INFO - Quantizing mlp.down_proj in layer 8/24...\n",
            "2024-05-15 03:05:17 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 8/24...\n",
            "2024-05-15 03:05:19 INFO [auto_gptq.quantization.gptq] duration: 1.7473044395446777\n",
            "2024-05-15 03:05:19 INFO [auto_gptq.quantization.gptq] avg loss: 5.591194152832031\n",
            "INFO - Start quantizing layer 9/24\n",
            "2024-05-15 03:05:20 INFO [auto_gptq.modeling._base] Start quantizing layer 9/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 9/24...\n",
            "2024-05-15 03:05:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 9/24...\n",
            "2024-05-15 03:05:23 INFO [auto_gptq.quantization.gptq] duration: 0.715418815612793\n",
            "2024-05-15 03:05:23 INFO [auto_gptq.quantization.gptq] avg loss: 164.751220703125\n",
            "INFO - Quantizing self_attn.v_proj in layer 9/24...\n",
            "2024-05-15 03:05:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 9/24...\n",
            "2024-05-15 03:05:23 INFO [auto_gptq.quantization.gptq] duration: 0.40824127197265625\n",
            "2024-05-15 03:05:23 INFO [auto_gptq.quantization.gptq] avg loss: 59.916934967041016\n",
            "INFO - Quantizing self_attn.q_proj in layer 9/24...\n",
            "2024-05-15 03:05:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 9/24...\n",
            "2024-05-15 03:05:24 INFO [auto_gptq.quantization.gptq] duration: 0.42095398902893066\n",
            "2024-05-15 03:05:24 INFO [auto_gptq.quantization.gptq] avg loss: 166.87400817871094\n",
            "INFO - Quantizing self_attn.o_proj in layer 9/24...\n",
            "2024-05-15 03:05:25 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 9/24...\n",
            "2024-05-15 03:05:26 INFO [auto_gptq.quantization.gptq] duration: 0.6780622005462646\n",
            "2024-05-15 03:05:26 INFO [auto_gptq.quantization.gptq] avg loss: 3.076972007751465\n",
            "INFO - Quantizing mlp.up_proj in layer 9/24...\n",
            "2024-05-15 03:05:27 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 9/24...\n",
            "2024-05-15 03:05:28 INFO [auto_gptq.quantization.gptq] duration: 0.7067735195159912\n",
            "2024-05-15 03:05:28 INFO [auto_gptq.quantization.gptq] avg loss: 146.90484619140625\n",
            "INFO - Quantizing mlp.gate_proj in layer 9/24...\n",
            "2024-05-15 03:05:28 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 9/24...\n",
            "2024-05-15 03:05:28 INFO [auto_gptq.quantization.gptq] duration: 0.4091470241546631\n",
            "2024-05-15 03:05:28 INFO [auto_gptq.quantization.gptq] avg loss: 166.76358032226562\n",
            "INFO - Quantizing mlp.down_proj in layer 9/24...\n",
            "2024-05-15 03:05:31 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 9/24...\n",
            "2024-05-15 03:05:33 INFO [auto_gptq.quantization.gptq] duration: 1.743302583694458\n",
            "2024-05-15 03:05:33 INFO [auto_gptq.quantization.gptq] avg loss: 6.0698699951171875\n",
            "INFO - Start quantizing layer 10/24\n",
            "2024-05-15 03:05:34 INFO [auto_gptq.modeling._base] Start quantizing layer 10/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 10/24...\n",
            "2024-05-15 03:05:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 10/24...\n",
            "2024-05-15 03:05:37 INFO [auto_gptq.quantization.gptq] duration: 0.705263614654541\n",
            "2024-05-15 03:05:37 INFO [auto_gptq.quantization.gptq] avg loss: 171.9335479736328\n",
            "INFO - Quantizing self_attn.v_proj in layer 10/24...\n",
            "2024-05-15 03:05:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 10/24...\n",
            "2024-05-15 03:05:37 INFO [auto_gptq.quantization.gptq] duration: 0.39852333068847656\n",
            "2024-05-15 03:05:37 INFO [auto_gptq.quantization.gptq] avg loss: 60.33595657348633\n",
            "INFO - Quantizing self_attn.q_proj in layer 10/24...\n",
            "2024-05-15 03:05:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 10/24...\n",
            "2024-05-15 03:05:38 INFO [auto_gptq.quantization.gptq] duration: 0.41091394424438477\n",
            "2024-05-15 03:05:38 INFO [auto_gptq.quantization.gptq] avg loss: 173.16659545898438\n",
            "INFO - Quantizing self_attn.o_proj in layer 10/24...\n",
            "2024-05-15 03:05:39 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 10/24...\n",
            "2024-05-15 03:05:40 INFO [auto_gptq.quantization.gptq] duration: 0.6818821430206299\n",
            "2024-05-15 03:05:40 INFO [auto_gptq.quantization.gptq] avg loss: 4.175799369812012\n",
            "INFO - Quantizing mlp.up_proj in layer 10/24...\n",
            "2024-05-15 03:05:41 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 10/24...\n",
            "2024-05-15 03:05:42 INFO [auto_gptq.quantization.gptq] duration: 0.7161257266998291\n",
            "2024-05-15 03:05:42 INFO [auto_gptq.quantization.gptq] avg loss: 134.76434326171875\n",
            "INFO - Quantizing mlp.gate_proj in layer 10/24...\n",
            "2024-05-15 03:05:42 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 10/24...\n",
            "2024-05-15 03:05:42 INFO [auto_gptq.quantization.gptq] duration: 0.4067962169647217\n",
            "2024-05-15 03:05:42 INFO [auto_gptq.quantization.gptq] avg loss: 152.3896942138672\n",
            "INFO - Quantizing mlp.down_proj in layer 10/24...\n",
            "2024-05-15 03:05:45 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 10/24...\n",
            "2024-05-15 03:05:47 INFO [auto_gptq.quantization.gptq] duration: 1.8012049198150635\n",
            "2024-05-15 03:05:47 INFO [auto_gptq.quantization.gptq] avg loss: 5.166007041931152\n",
            "INFO - Start quantizing layer 11/24\n",
            "2024-05-15 03:05:49 INFO [auto_gptq.modeling._base] Start quantizing layer 11/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 11/24...\n",
            "2024-05-15 03:05:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 11/24...\n",
            "2024-05-15 03:05:51 INFO [auto_gptq.quantization.gptq] duration: 0.7122402191162109\n",
            "2024-05-15 03:05:51 INFO [auto_gptq.quantization.gptq] avg loss: 228.48587036132812\n",
            "INFO - Quantizing self_attn.v_proj in layer 11/24...\n",
            "2024-05-15 03:05:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 11/24...\n",
            "2024-05-15 03:05:52 INFO [auto_gptq.quantization.gptq] duration: 0.4040505886077881\n",
            "2024-05-15 03:05:52 INFO [auto_gptq.quantization.gptq] avg loss: 98.95181274414062\n",
            "INFO - Quantizing self_attn.q_proj in layer 11/24...\n",
            "2024-05-15 03:05:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 11/24...\n",
            "2024-05-15 03:05:52 INFO [auto_gptq.quantization.gptq] duration: 0.40411853790283203\n",
            "2024-05-15 03:05:52 INFO [auto_gptq.quantization.gptq] avg loss: 237.12852478027344\n",
            "INFO - Quantizing self_attn.o_proj in layer 11/24...\n",
            "2024-05-15 03:05:53 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 11/24...\n",
            "2024-05-15 03:05:54 INFO [auto_gptq.quantization.gptq] duration: 0.6881589889526367\n",
            "2024-05-15 03:05:54 INFO [auto_gptq.quantization.gptq] avg loss: 3.719405174255371\n",
            "INFO - Quantizing mlp.up_proj in layer 11/24...\n",
            "2024-05-15 03:05:56 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 11/24...\n",
            "2024-05-15 03:05:56 INFO [auto_gptq.quantization.gptq] duration: 0.7094101905822754\n",
            "2024-05-15 03:05:56 INFO [auto_gptq.quantization.gptq] avg loss: 128.16749572753906\n",
            "INFO - Quantizing mlp.gate_proj in layer 11/24...\n",
            "2024-05-15 03:05:56 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 11/24...\n",
            "2024-05-15 03:05:57 INFO [auto_gptq.quantization.gptq] duration: 0.4081144332885742\n",
            "2024-05-15 03:05:57 INFO [auto_gptq.quantization.gptq] avg loss: 138.57354736328125\n",
            "INFO - Quantizing mlp.down_proj in layer 11/24...\n",
            "2024-05-15 03:06:00 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 11/24...\n",
            "2024-05-15 03:06:01 INFO [auto_gptq.quantization.gptq] duration: 1.7506256103515625\n",
            "2024-05-15 03:06:01 INFO [auto_gptq.quantization.gptq] avg loss: 5.290987014770508\n",
            "INFO - Start quantizing layer 12/24\n",
            "2024-05-15 03:06:03 INFO [auto_gptq.modeling._base] Start quantizing layer 12/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 12/24...\n",
            "2024-05-15 03:06:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 12/24...\n",
            "2024-05-15 03:06:05 INFO [auto_gptq.quantization.gptq] duration: 0.704038143157959\n",
            "2024-05-15 03:06:05 INFO [auto_gptq.quantization.gptq] avg loss: 182.0843505859375\n",
            "INFO - Quantizing self_attn.v_proj in layer 12/24...\n",
            "2024-05-15 03:06:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 12/24...\n",
            "2024-05-15 03:06:06 INFO [auto_gptq.quantization.gptq] duration: 0.40310215950012207\n",
            "2024-05-15 03:06:06 INFO [auto_gptq.quantization.gptq] avg loss: 114.5020751953125\n",
            "INFO - Quantizing self_attn.q_proj in layer 12/24...\n",
            "2024-05-15 03:06:06 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 12/24...\n",
            "2024-05-15 03:06:06 INFO [auto_gptq.quantization.gptq] duration: 0.3993875980377197\n",
            "2024-05-15 03:06:06 INFO [auto_gptq.quantization.gptq] avg loss: 210.697509765625\n",
            "INFO - Quantizing self_attn.o_proj in layer 12/24...\n",
            "2024-05-15 03:06:08 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 12/24...\n",
            "2024-05-15 03:06:08 INFO [auto_gptq.quantization.gptq] duration: 0.6832387447357178\n",
            "2024-05-15 03:06:08 INFO [auto_gptq.quantization.gptq] avg loss: 4.717772483825684\n",
            "INFO - Quantizing mlp.up_proj in layer 12/24...\n",
            "2024-05-15 03:06:10 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 12/24...\n",
            "2024-05-15 03:06:11 INFO [auto_gptq.quantization.gptq] duration: 0.7171127796173096\n",
            "2024-05-15 03:06:11 INFO [auto_gptq.quantization.gptq] avg loss: 146.24635314941406\n",
            "INFO - Quantizing mlp.gate_proj in layer 12/24...\n",
            "2024-05-15 03:06:11 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 12/24...\n",
            "2024-05-15 03:06:11 INFO [auto_gptq.quantization.gptq] duration: 0.40967750549316406\n",
            "2024-05-15 03:06:11 INFO [auto_gptq.quantization.gptq] avg loss: 157.76417541503906\n",
            "INFO - Quantizing mlp.down_proj in layer 12/24...\n",
            "2024-05-15 03:06:14 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 12/24...\n",
            "2024-05-15 03:06:16 INFO [auto_gptq.quantization.gptq] duration: 1.7410409450531006\n",
            "2024-05-15 03:06:16 INFO [auto_gptq.quantization.gptq] avg loss: 7.361429214477539\n",
            "INFO - Start quantizing layer 13/24\n",
            "2024-05-15 03:06:17 INFO [auto_gptq.modeling._base] Start quantizing layer 13/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 13/24...\n",
            "2024-05-15 03:06:19 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 13/24...\n",
            "2024-05-15 03:06:20 INFO [auto_gptq.quantization.gptq] duration: 0.7112224102020264\n",
            "2024-05-15 03:06:20 INFO [auto_gptq.quantization.gptq] avg loss: 200.536376953125\n",
            "INFO - Quantizing self_attn.v_proj in layer 13/24...\n",
            "2024-05-15 03:06:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 13/24...\n",
            "2024-05-15 03:06:20 INFO [auto_gptq.quantization.gptq] duration: 0.411651611328125\n",
            "2024-05-15 03:06:20 INFO [auto_gptq.quantization.gptq] avg loss: 107.53105926513672\n",
            "INFO - Quantizing self_attn.q_proj in layer 13/24...\n",
            "2024-05-15 03:06:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 13/24...\n",
            "2024-05-15 03:06:20 INFO [auto_gptq.quantization.gptq] duration: 0.40097904205322266\n",
            "2024-05-15 03:06:20 INFO [auto_gptq.quantization.gptq] avg loss: 232.24090576171875\n",
            "INFO - Quantizing self_attn.o_proj in layer 13/24...\n",
            "2024-05-15 03:06:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 13/24...\n",
            "2024-05-15 03:06:22 INFO [auto_gptq.quantization.gptq] duration: 0.6873838901519775\n",
            "2024-05-15 03:06:22 INFO [auto_gptq.quantization.gptq] avg loss: 6.542915344238281\n",
            "INFO - Quantizing mlp.up_proj in layer 13/24...\n",
            "2024-05-15 03:06:24 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 13/24...\n",
            "2024-05-15 03:06:25 INFO [auto_gptq.quantization.gptq] duration: 0.7006349563598633\n",
            "2024-05-15 03:06:25 INFO [auto_gptq.quantization.gptq] avg loss: 141.06832885742188\n",
            "INFO - Quantizing mlp.gate_proj in layer 13/24...\n",
            "2024-05-15 03:06:25 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 13/24...\n",
            "2024-05-15 03:06:25 INFO [auto_gptq.quantization.gptq] duration: 0.40561389923095703\n",
            "2024-05-15 03:06:25 INFO [auto_gptq.quantization.gptq] avg loss: 148.70603942871094\n",
            "INFO - Quantizing mlp.down_proj in layer 13/24...\n",
            "2024-05-15 03:06:28 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 13/24...\n",
            "2024-05-15 03:06:30 INFO [auto_gptq.quantization.gptq] duration: 1.7581994533538818\n",
            "2024-05-15 03:06:30 INFO [auto_gptq.quantization.gptq] avg loss: 7.57159423828125\n",
            "INFO - Start quantizing layer 14/24\n",
            "2024-05-15 03:06:31 INFO [auto_gptq.modeling._base] Start quantizing layer 14/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 14/24...\n",
            "2024-05-15 03:06:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 14/24...\n",
            "2024-05-15 03:06:34 INFO [auto_gptq.quantization.gptq] duration: 0.7082016468048096\n",
            "2024-05-15 03:06:34 INFO [auto_gptq.quantization.gptq] avg loss: 205.72494506835938\n",
            "INFO - Quantizing self_attn.v_proj in layer 14/24...\n",
            "2024-05-15 03:06:34 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 14/24...\n",
            "2024-05-15 03:06:34 INFO [auto_gptq.quantization.gptq] duration: 0.4082658290863037\n",
            "2024-05-15 03:06:34 INFO [auto_gptq.quantization.gptq] avg loss: 88.14295959472656\n",
            "INFO - Quantizing self_attn.q_proj in layer 14/24...\n",
            "2024-05-15 03:06:34 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 14/24...\n",
            "2024-05-15 03:06:35 INFO [auto_gptq.quantization.gptq] duration: 0.412691593170166\n",
            "2024-05-15 03:06:35 INFO [auto_gptq.quantization.gptq] avg loss: 206.50367736816406\n",
            "INFO - Quantizing self_attn.o_proj in layer 14/24...\n",
            "2024-05-15 03:06:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 14/24...\n",
            "2024-05-15 03:06:37 INFO [auto_gptq.quantization.gptq] duration: 0.6781964302062988\n",
            "2024-05-15 03:06:37 INFO [auto_gptq.quantization.gptq] avg loss: 4.729917049407959\n",
            "INFO - Quantizing mlp.up_proj in layer 14/24...\n",
            "2024-05-15 03:06:38 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 14/24...\n",
            "2024-05-15 03:06:39 INFO [auto_gptq.quantization.gptq] duration: 0.6899919509887695\n",
            "2024-05-15 03:06:39 INFO [auto_gptq.quantization.gptq] avg loss: 144.9805450439453\n",
            "INFO - Quantizing mlp.gate_proj in layer 14/24...\n",
            "2024-05-15 03:06:39 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 14/24...\n",
            "2024-05-15 03:06:39 INFO [auto_gptq.quantization.gptq] duration: 0.4017457962036133\n",
            "2024-05-15 03:06:39 INFO [auto_gptq.quantization.gptq] avg loss: 156.25289916992188\n",
            "INFO - Quantizing mlp.down_proj in layer 14/24...\n",
            "2024-05-15 03:06:42 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 14/24...\n",
            "2024-05-15 03:06:44 INFO [auto_gptq.quantization.gptq] duration: 1.7527446746826172\n",
            "2024-05-15 03:06:44 INFO [auto_gptq.quantization.gptq] avg loss: 7.281262397766113\n",
            "INFO - Start quantizing layer 15/24\n",
            "2024-05-15 03:06:45 INFO [auto_gptq.modeling._base] Start quantizing layer 15/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 15/24...\n",
            "2024-05-15 03:06:47 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 15/24...\n",
            "2024-05-15 03:06:48 INFO [auto_gptq.quantization.gptq] duration: 0.7089650630950928\n",
            "2024-05-15 03:06:48 INFO [auto_gptq.quantization.gptq] avg loss: 206.19589233398438\n",
            "INFO - Quantizing self_attn.v_proj in layer 15/24...\n",
            "2024-05-15 03:06:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 15/24...\n",
            "2024-05-15 03:06:48 INFO [auto_gptq.quantization.gptq] duration: 0.4125385284423828\n",
            "2024-05-15 03:06:48 INFO [auto_gptq.quantization.gptq] avg loss: 124.72256469726562\n",
            "INFO - Quantizing self_attn.q_proj in layer 15/24...\n",
            "2024-05-15 03:06:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 15/24...\n",
            "2024-05-15 03:06:49 INFO [auto_gptq.quantization.gptq] duration: 0.4110374450683594\n",
            "2024-05-15 03:06:49 INFO [auto_gptq.quantization.gptq] avg loss: 228.9474639892578\n",
            "INFO - Quantizing self_attn.o_proj in layer 15/24...\n",
            "2024-05-15 03:06:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 15/24...\n",
            "2024-05-15 03:06:51 INFO [auto_gptq.quantization.gptq] duration: 0.6849460601806641\n",
            "2024-05-15 03:06:51 INFO [auto_gptq.quantization.gptq] avg loss: 5.007606029510498\n",
            "INFO - Quantizing mlp.up_proj in layer 15/24...\n",
            "2024-05-15 03:06:52 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 15/24...\n",
            "2024-05-15 03:06:53 INFO [auto_gptq.quantization.gptq] duration: 0.7051093578338623\n",
            "2024-05-15 03:06:53 INFO [auto_gptq.quantization.gptq] avg loss: 163.93325805664062\n",
            "INFO - Quantizing mlp.gate_proj in layer 15/24...\n",
            "2024-05-15 03:06:53 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 15/24...\n",
            "2024-05-15 03:06:54 INFO [auto_gptq.quantization.gptq] duration: 0.39931631088256836\n",
            "2024-05-15 03:06:54 INFO [auto_gptq.quantization.gptq] avg loss: 174.53793334960938\n",
            "INFO - Quantizing mlp.down_proj in layer 15/24...\n",
            "2024-05-15 03:06:56 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 15/24...\n",
            "2024-05-15 03:06:58 INFO [auto_gptq.quantization.gptq] duration: 1.7415165901184082\n",
            "2024-05-15 03:06:58 INFO [auto_gptq.quantization.gptq] avg loss: 10.385062217712402\n",
            "INFO - Start quantizing layer 16/24\n",
            "2024-05-15 03:06:59 INFO [auto_gptq.modeling._base] Start quantizing layer 16/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 16/24...\n",
            "2024-05-15 03:07:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 16/24...\n",
            "2024-05-15 03:07:02 INFO [auto_gptq.quantization.gptq] duration: 0.7046380043029785\n",
            "2024-05-15 03:07:02 INFO [auto_gptq.quantization.gptq] avg loss: 189.6847686767578\n",
            "INFO - Quantizing self_attn.v_proj in layer 16/24...\n",
            "2024-05-15 03:07:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 16/24...\n",
            "2024-05-15 03:07:02 INFO [auto_gptq.quantization.gptq] duration: 0.4011380672454834\n",
            "2024-05-15 03:07:02 INFO [auto_gptq.quantization.gptq] avg loss: 137.51800537109375\n",
            "INFO - Quantizing self_attn.q_proj in layer 16/24...\n",
            "2024-05-15 03:07:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 16/24...\n",
            "2024-05-15 03:07:03 INFO [auto_gptq.quantization.gptq] duration: 0.39771580696105957\n",
            "2024-05-15 03:07:03 INFO [auto_gptq.quantization.gptq] avg loss: 203.83387756347656\n",
            "INFO - Quantizing self_attn.o_proj in layer 16/24...\n",
            "2024-05-15 03:07:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 16/24...\n",
            "2024-05-15 03:07:05 INFO [auto_gptq.quantization.gptq] duration: 0.6871094703674316\n",
            "2024-05-15 03:07:05 INFO [auto_gptq.quantization.gptq] avg loss: 4.983136177062988\n",
            "INFO - Quantizing mlp.up_proj in layer 16/24...\n",
            "2024-05-15 03:07:07 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 16/24...\n",
            "2024-05-15 03:07:07 INFO [auto_gptq.quantization.gptq] duration: 0.7330174446105957\n",
            "2024-05-15 03:07:07 INFO [auto_gptq.quantization.gptq] avg loss: 192.52432250976562\n",
            "INFO - Quantizing mlp.gate_proj in layer 16/24...\n",
            "2024-05-15 03:07:07 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 16/24...\n",
            "2024-05-15 03:07:08 INFO [auto_gptq.quantization.gptq] duration: 0.4194529056549072\n",
            "2024-05-15 03:07:08 INFO [auto_gptq.quantization.gptq] avg loss: 192.76461791992188\n",
            "INFO - Quantizing mlp.down_proj in layer 16/24...\n",
            "2024-05-15 03:07:11 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 16/24...\n",
            "2024-05-15 03:07:12 INFO [auto_gptq.quantization.gptq] duration: 1.7665760517120361\n",
            "2024-05-15 03:07:12 INFO [auto_gptq.quantization.gptq] avg loss: 14.755127906799316\n",
            "INFO - Start quantizing layer 17/24\n",
            "2024-05-15 03:07:14 INFO [auto_gptq.modeling._base] Start quantizing layer 17/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 17/24...\n",
            "2024-05-15 03:07:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 17/24...\n",
            "2024-05-15 03:07:16 INFO [auto_gptq.quantization.gptq] duration: 0.7091023921966553\n",
            "2024-05-15 03:07:16 INFO [auto_gptq.quantization.gptq] avg loss: 212.62460327148438\n",
            "INFO - Quantizing self_attn.v_proj in layer 17/24...\n",
            "2024-05-15 03:07:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 17/24...\n",
            "2024-05-15 03:07:17 INFO [auto_gptq.quantization.gptq] duration: 0.41408252716064453\n",
            "2024-05-15 03:07:17 INFO [auto_gptq.quantization.gptq] avg loss: 166.66281127929688\n",
            "INFO - Quantizing self_attn.q_proj in layer 17/24...\n",
            "2024-05-15 03:07:17 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 17/24...\n",
            "2024-05-15 03:07:17 INFO [auto_gptq.quantization.gptq] duration: 0.402864933013916\n",
            "2024-05-15 03:07:17 INFO [auto_gptq.quantization.gptq] avg loss: 226.7498321533203\n",
            "INFO - Quantizing self_attn.o_proj in layer 17/24...\n",
            "2024-05-15 03:07:18 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 17/24...\n",
            "2024-05-15 03:07:19 INFO [auto_gptq.quantization.gptq] duration: 0.6905949115753174\n",
            "2024-05-15 03:07:19 INFO [auto_gptq.quantization.gptq] avg loss: 7.06404972076416\n",
            "INFO - Quantizing mlp.up_proj in layer 17/24...\n",
            "2024-05-15 03:07:21 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 17/24...\n",
            "2024-05-15 03:07:21 INFO [auto_gptq.quantization.gptq] duration: 0.7062258720397949\n",
            "2024-05-15 03:07:21 INFO [auto_gptq.quantization.gptq] avg loss: 235.09156799316406\n",
            "INFO - Quantizing mlp.gate_proj in layer 17/24...\n",
            "2024-05-15 03:07:21 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 17/24...\n",
            "2024-05-15 03:07:22 INFO [auto_gptq.quantization.gptq] duration: 0.4060323238372803\n",
            "2024-05-15 03:07:22 INFO [auto_gptq.quantization.gptq] avg loss: 246.2373046875\n",
            "INFO - Quantizing mlp.down_proj in layer 17/24...\n",
            "2024-05-15 03:07:25 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 17/24...\n",
            "2024-05-15 03:07:26 INFO [auto_gptq.quantization.gptq] duration: 1.7496442794799805\n",
            "2024-05-15 03:07:26 INFO [auto_gptq.quantization.gptq] avg loss: 20.0142822265625\n",
            "INFO - Start quantizing layer 18/24\n",
            "2024-05-15 03:07:28 INFO [auto_gptq.modeling._base] Start quantizing layer 18/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 18/24...\n",
            "2024-05-15 03:07:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 18/24...\n",
            "2024-05-15 03:07:31 INFO [auto_gptq.quantization.gptq] duration: 0.722830057144165\n",
            "2024-05-15 03:07:31 INFO [auto_gptq.quantization.gptq] avg loss: 229.48074340820312\n",
            "INFO - Quantizing self_attn.v_proj in layer 18/24...\n",
            "2024-05-15 03:07:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 18/24...\n",
            "2024-05-15 03:07:31 INFO [auto_gptq.quantization.gptq] duration: 0.4131655693054199\n",
            "2024-05-15 03:07:31 INFO [auto_gptq.quantization.gptq] avg loss: 217.31964111328125\n",
            "INFO - Quantizing self_attn.q_proj in layer 18/24...\n",
            "2024-05-15 03:07:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 18/24...\n",
            "2024-05-15 03:07:31 INFO [auto_gptq.quantization.gptq] duration: 0.4171879291534424\n",
            "2024-05-15 03:07:31 INFO [auto_gptq.quantization.gptq] avg loss: 249.9691925048828\n",
            "INFO - Quantizing self_attn.o_proj in layer 18/24...\n",
            "2024-05-15 03:07:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 18/24...\n",
            "2024-05-15 03:07:33 INFO [auto_gptq.quantization.gptq] duration: 0.6727089881896973\n",
            "2024-05-15 03:07:33 INFO [auto_gptq.quantization.gptq] avg loss: 12.44618034362793\n",
            "INFO - Quantizing mlp.up_proj in layer 18/24...\n",
            "2024-05-15 03:07:35 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 18/24...\n",
            "2024-05-15 03:07:36 INFO [auto_gptq.quantization.gptq] duration: 0.709378719329834\n",
            "2024-05-15 03:07:36 INFO [auto_gptq.quantization.gptq] avg loss: 269.640380859375\n",
            "INFO - Quantizing mlp.gate_proj in layer 18/24...\n",
            "2024-05-15 03:07:36 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 18/24...\n",
            "2024-05-15 03:07:36 INFO [auto_gptq.quantization.gptq] duration: 0.4013383388519287\n",
            "2024-05-15 03:07:36 INFO [auto_gptq.quantization.gptq] avg loss: 264.12823486328125\n",
            "INFO - Quantizing mlp.down_proj in layer 18/24...\n",
            "2024-05-15 03:07:39 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 18/24...\n",
            "2024-05-15 03:07:41 INFO [auto_gptq.quantization.gptq] duration: 1.7390532493591309\n",
            "2024-05-15 03:07:41 INFO [auto_gptq.quantization.gptq] avg loss: 27.60830307006836\n",
            "INFO - Start quantizing layer 19/24\n",
            "2024-05-15 03:07:42 INFO [auto_gptq.modeling._base] Start quantizing layer 19/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 19/24...\n",
            "2024-05-15 03:07:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 19/24...\n",
            "2024-05-15 03:07:45 INFO [auto_gptq.quantization.gptq] duration: 0.7146413326263428\n",
            "2024-05-15 03:07:45 INFO [auto_gptq.quantization.gptq] avg loss: 232.94129943847656\n",
            "INFO - Quantizing self_attn.v_proj in layer 19/24...\n",
            "2024-05-15 03:07:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 19/24...\n",
            "2024-05-15 03:07:45 INFO [auto_gptq.quantization.gptq] duration: 0.40226244926452637\n",
            "2024-05-15 03:07:45 INFO [auto_gptq.quantization.gptq] avg loss: 255.7532196044922\n",
            "INFO - Quantizing self_attn.q_proj in layer 19/24...\n",
            "2024-05-15 03:07:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 19/24...\n",
            "2024-05-15 03:07:46 INFO [auto_gptq.quantization.gptq] duration: 0.40465497970581055\n",
            "2024-05-15 03:07:46 INFO [auto_gptq.quantization.gptq] avg loss: 239.146240234375\n",
            "INFO - Quantizing self_attn.o_proj in layer 19/24...\n",
            "2024-05-15 03:07:47 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 19/24...\n",
            "2024-05-15 03:07:48 INFO [auto_gptq.quantization.gptq] duration: 0.6737878322601318\n",
            "2024-05-15 03:07:48 INFO [auto_gptq.quantization.gptq] avg loss: 10.72952651977539\n",
            "INFO - Quantizing mlp.up_proj in layer 19/24...\n",
            "2024-05-15 03:07:49 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 19/24...\n",
            "2024-05-15 03:07:50 INFO [auto_gptq.quantization.gptq] duration: 0.7029509544372559\n",
            "2024-05-15 03:07:50 INFO [auto_gptq.quantization.gptq] avg loss: 323.2818908691406\n",
            "INFO - Quantizing mlp.gate_proj in layer 19/24...\n",
            "2024-05-15 03:07:50 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 19/24...\n",
            "2024-05-15 03:07:50 INFO [auto_gptq.quantization.gptq] duration: 0.4031805992126465\n",
            "2024-05-15 03:07:50 INFO [auto_gptq.quantization.gptq] avg loss: 312.7361145019531\n",
            "INFO - Quantizing mlp.down_proj in layer 19/24...\n",
            "2024-05-15 03:07:53 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 19/24...\n",
            "2024-05-15 03:07:55 INFO [auto_gptq.quantization.gptq] duration: 1.7750160694122314\n",
            "2024-05-15 03:07:55 INFO [auto_gptq.quantization.gptq] avg loss: 39.290306091308594\n",
            "INFO - Start quantizing layer 20/24\n",
            "2024-05-15 03:07:56 INFO [auto_gptq.modeling._base] Start quantizing layer 20/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 20/24...\n",
            "2024-05-15 03:07:58 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 20/24...\n",
            "2024-05-15 03:07:59 INFO [auto_gptq.quantization.gptq] duration: 0.7001473903656006\n",
            "2024-05-15 03:07:59 INFO [auto_gptq.quantization.gptq] avg loss: 236.3485565185547\n",
            "INFO - Quantizing self_attn.v_proj in layer 20/24...\n",
            "2024-05-15 03:07:59 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 20/24...\n",
            "2024-05-15 03:07:59 INFO [auto_gptq.quantization.gptq] duration: 0.38645243644714355\n",
            "2024-05-15 03:07:59 INFO [auto_gptq.quantization.gptq] avg loss: 282.0042419433594\n",
            "INFO - Quantizing self_attn.q_proj in layer 20/24...\n",
            "2024-05-15 03:07:59 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 20/24...\n",
            "2024-05-15 03:08:00 INFO [auto_gptq.quantization.gptq] duration: 0.4008610248565674\n",
            "2024-05-15 03:08:00 INFO [auto_gptq.quantization.gptq] avg loss: 237.04795837402344\n",
            "INFO - Quantizing self_attn.o_proj in layer 20/24...\n",
            "2024-05-15 03:08:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 20/24...\n",
            "2024-05-15 03:08:02 INFO [auto_gptq.quantization.gptq] duration: 0.6783459186553955\n",
            "2024-05-15 03:08:02 INFO [auto_gptq.quantization.gptq] avg loss: 17.11379051208496\n",
            "INFO - Quantizing mlp.up_proj in layer 20/24...\n",
            "2024-05-15 03:08:03 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 20/24...\n",
            "2024-05-15 03:08:04 INFO [auto_gptq.quantization.gptq] duration: 0.6880981922149658\n",
            "2024-05-15 03:08:04 INFO [auto_gptq.quantization.gptq] avg loss: 340.94451904296875\n",
            "INFO - Quantizing mlp.gate_proj in layer 20/24...\n",
            "2024-05-15 03:08:04 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 20/24...\n",
            "2024-05-15 03:08:04 INFO [auto_gptq.quantization.gptq] duration: 0.39548730850219727\n",
            "2024-05-15 03:08:04 INFO [auto_gptq.quantization.gptq] avg loss: 322.51800537109375\n",
            "INFO - Quantizing mlp.down_proj in layer 20/24...\n",
            "2024-05-15 03:08:07 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 20/24...\n",
            "2024-05-15 03:08:09 INFO [auto_gptq.quantization.gptq] duration: 1.7524969577789307\n",
            "2024-05-15 03:08:09 INFO [auto_gptq.quantization.gptq] avg loss: 48.0966796875\n",
            "INFO - Start quantizing layer 21/24\n",
            "2024-05-15 03:08:10 INFO [auto_gptq.modeling._base] Start quantizing layer 21/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 21/24...\n",
            "2024-05-15 03:08:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 21/24...\n",
            "2024-05-15 03:08:13 INFO [auto_gptq.quantization.gptq] duration: 0.7062437534332275\n",
            "2024-05-15 03:08:13 INFO [auto_gptq.quantization.gptq] avg loss: 244.49925231933594\n",
            "INFO - Quantizing self_attn.v_proj in layer 21/24...\n",
            "2024-05-15 03:08:13 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 21/24...\n",
            "2024-05-15 03:08:13 INFO [auto_gptq.quantization.gptq] duration: 0.396665096282959\n",
            "2024-05-15 03:08:13 INFO [auto_gptq.quantization.gptq] avg loss: 323.91986083984375\n",
            "INFO - Quantizing self_attn.q_proj in layer 21/24...\n",
            "2024-05-15 03:08:13 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 21/24...\n",
            "2024-05-15 03:08:14 INFO [auto_gptq.quantization.gptq] duration: 0.40172314643859863\n",
            "2024-05-15 03:08:14 INFO [auto_gptq.quantization.gptq] avg loss: 242.6769561767578\n",
            "INFO - Quantizing self_attn.o_proj in layer 21/24...\n",
            "2024-05-15 03:08:15 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 21/24...\n",
            "2024-05-15 03:08:16 INFO [auto_gptq.quantization.gptq] duration: 0.6847593784332275\n",
            "2024-05-15 03:08:16 INFO [auto_gptq.quantization.gptq] avg loss: 14.634856224060059\n",
            "INFO - Quantizing mlp.up_proj in layer 21/24...\n",
            "2024-05-15 03:08:17 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 21/24...\n",
            "2024-05-15 03:08:18 INFO [auto_gptq.quantization.gptq] duration: 0.701514482498169\n",
            "2024-05-15 03:08:18 INFO [auto_gptq.quantization.gptq] avg loss: 366.2491455078125\n",
            "INFO - Quantizing mlp.gate_proj in layer 21/24...\n",
            "2024-05-15 03:08:18 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 21/24...\n",
            "2024-05-15 03:08:19 INFO [auto_gptq.quantization.gptq] duration: 0.4106266498565674\n",
            "2024-05-15 03:08:19 INFO [auto_gptq.quantization.gptq] avg loss: 330.2254638671875\n",
            "INFO - Quantizing mlp.down_proj in layer 21/24...\n",
            "2024-05-15 03:08:21 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 21/24...\n",
            "2024-05-15 03:08:23 INFO [auto_gptq.quantization.gptq] duration: 1.7390871047973633\n",
            "2024-05-15 03:08:23 INFO [auto_gptq.quantization.gptq] avg loss: 65.61421966552734\n",
            "INFO - Start quantizing layer 22/24\n",
            "2024-05-15 03:08:25 INFO [auto_gptq.modeling._base] Start quantizing layer 22/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 22/24...\n",
            "2024-05-15 03:08:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 22/24...\n",
            "2024-05-15 03:08:27 INFO [auto_gptq.quantization.gptq] duration: 0.706129789352417\n",
            "2024-05-15 03:08:27 INFO [auto_gptq.quantization.gptq] avg loss: 224.8109130859375\n",
            "INFO - Quantizing self_attn.v_proj in layer 22/24...\n",
            "2024-05-15 03:08:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 22/24...\n",
            "2024-05-15 03:08:28 INFO [auto_gptq.quantization.gptq] duration: 0.3967092037200928\n",
            "2024-05-15 03:08:28 INFO [auto_gptq.quantization.gptq] avg loss: 369.2146301269531\n",
            "INFO - Quantizing self_attn.q_proj in layer 22/24...\n",
            "2024-05-15 03:08:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 22/24...\n",
            "2024-05-15 03:08:28 INFO [auto_gptq.quantization.gptq] duration: 0.4179375171661377\n",
            "2024-05-15 03:08:28 INFO [auto_gptq.quantization.gptq] avg loss: 237.5941162109375\n",
            "INFO - Quantizing self_attn.o_proj in layer 22/24...\n",
            "2024-05-15 03:08:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 22/24...\n",
            "2024-05-15 03:08:30 INFO [auto_gptq.quantization.gptq] duration: 0.6733818054199219\n",
            "2024-05-15 03:08:30 INFO [auto_gptq.quantization.gptq] avg loss: 26.867889404296875\n",
            "INFO - Quantizing mlp.up_proj in layer 22/24...\n",
            "2024-05-15 03:08:32 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 22/24...\n",
            "2024-05-15 03:08:32 INFO [auto_gptq.quantization.gptq] duration: 0.7007393836975098\n",
            "2024-05-15 03:08:32 INFO [auto_gptq.quantization.gptq] avg loss: 390.0470886230469\n",
            "INFO - Quantizing mlp.gate_proj in layer 22/24...\n",
            "2024-05-15 03:08:32 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 22/24...\n",
            "2024-05-15 03:08:33 INFO [auto_gptq.quantization.gptq] duration: 0.40544700622558594\n",
            "2024-05-15 03:08:33 INFO [auto_gptq.quantization.gptq] avg loss: 343.10400390625\n",
            "INFO - Quantizing mlp.down_proj in layer 22/24...\n",
            "2024-05-15 03:08:36 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 22/24...\n",
            "2024-05-15 03:08:37 INFO [auto_gptq.quantization.gptq] duration: 1.748671293258667\n",
            "2024-05-15 03:08:37 INFO [auto_gptq.quantization.gptq] avg loss: 90.67803955078125\n",
            "INFO - Start quantizing layer 23/24\n",
            "2024-05-15 03:08:39 INFO [auto_gptq.modeling._base] Start quantizing layer 23/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 23/24...\n",
            "2024-05-15 03:08:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 23/24...\n",
            "2024-05-15 03:08:41 INFO [auto_gptq.quantization.gptq] duration: 0.7137126922607422\n",
            "2024-05-15 03:08:41 INFO [auto_gptq.quantization.gptq] avg loss: 227.23272705078125\n",
            "INFO - Quantizing self_attn.v_proj in layer 23/24...\n",
            "2024-05-15 03:08:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 23/24...\n",
            "2024-05-15 03:08:42 INFO [auto_gptq.quantization.gptq] duration: 0.40532660484313965\n",
            "2024-05-15 03:08:42 INFO [auto_gptq.quantization.gptq] avg loss: 428.3846740722656\n",
            "INFO - Quantizing self_attn.q_proj in layer 23/24...\n",
            "2024-05-15 03:08:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 23/24...\n",
            "2024-05-15 03:08:42 INFO [auto_gptq.quantization.gptq] duration: 0.40631628036499023\n",
            "2024-05-15 03:08:42 INFO [auto_gptq.quantization.gptq] avg loss: 284.2809143066406\n",
            "INFO - Quantizing self_attn.o_proj in layer 23/24...\n",
            "2024-05-15 03:08:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 23/24...\n",
            "2024-05-15 03:08:44 INFO [auto_gptq.quantization.gptq] duration: 0.6817219257354736\n",
            "2024-05-15 03:08:44 INFO [auto_gptq.quantization.gptq] avg loss: 81.06524658203125\n",
            "INFO - Quantizing mlp.up_proj in layer 23/24...\n",
            "2024-05-15 03:08:46 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 23/24...\n",
            "2024-05-15 03:08:47 INFO [auto_gptq.quantization.gptq] duration: 0.696854829788208\n",
            "2024-05-15 03:08:47 INFO [auto_gptq.quantization.gptq] avg loss: 415.24560546875\n",
            "INFO - Quantizing mlp.gate_proj in layer 23/24...\n",
            "2024-05-15 03:08:47 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 23/24...\n",
            "2024-05-15 03:08:47 INFO [auto_gptq.quantization.gptq] duration: 0.4025294780731201\n",
            "2024-05-15 03:08:47 INFO [auto_gptq.quantization.gptq] avg loss: 366.85076904296875\n",
            "INFO - Quantizing mlp.down_proj in layer 23/24...\n",
            "2024-05-15 03:08:50 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 23/24...\n",
            "2024-05-15 03:08:52 INFO [auto_gptq.quantization.gptq] duration: 1.7821381092071533\n",
            "2024-05-15 03:08:52 INFO [auto_gptq.quantization.gptq] avg loss: 220.4973907470703\n",
            "INFO - Start quantizing layer 24/24\n",
            "2024-05-15 03:08:53 INFO [auto_gptq.modeling._base] Start quantizing layer 24/24\n",
            "INFO - Quantizing self_attn.k_proj in layer 24/24...\n",
            "2024-05-15 03:08:55 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 24/24...\n",
            "2024-05-15 03:08:56 INFO [auto_gptq.quantization.gptq] duration: 0.7023429870605469\n",
            "2024-05-15 03:08:56 INFO [auto_gptq.quantization.gptq] avg loss: 201.63217163085938\n",
            "INFO - Quantizing self_attn.v_proj in layer 24/24...\n",
            "2024-05-15 03:08:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 24/24...\n",
            "2024-05-15 03:08:56 INFO [auto_gptq.quantization.gptq] duration: 0.39889001846313477\n",
            "2024-05-15 03:08:56 INFO [auto_gptq.quantization.gptq] avg loss: 257.8051452636719\n",
            "INFO - Quantizing self_attn.q_proj in layer 24/24...\n",
            "2024-05-15 03:08:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 24/24...\n",
            "2024-05-15 03:08:56 INFO [auto_gptq.quantization.gptq] duration: 0.40741395950317383\n",
            "2024-05-15 03:08:56 INFO [auto_gptq.quantization.gptq] avg loss: 198.11529541015625\n",
            "INFO - Quantizing self_attn.o_proj in layer 24/24...\n",
            "2024-05-15 03:08:58 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 24/24...\n",
            "2024-05-15 03:08:58 INFO [auto_gptq.quantization.gptq] duration: 0.6861412525177002\n",
            "2024-05-15 03:08:58 INFO [auto_gptq.quantization.gptq] avg loss: 42.64664840698242\n",
            "INFO - Quantizing mlp.up_proj in layer 24/24...\n",
            "2024-05-15 03:09:00 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 24/24...\n",
            "2024-05-15 03:09:01 INFO [auto_gptq.quantization.gptq] duration: 0.7054805755615234\n",
            "2024-05-15 03:09:01 INFO [auto_gptq.quantization.gptq] avg loss: 540.4581909179688\n",
            "INFO - Quantizing mlp.gate_proj in layer 24/24...\n",
            "2024-05-15 03:09:01 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 24/24...\n",
            "2024-05-15 03:09:01 INFO [auto_gptq.quantization.gptq] duration: 0.4028818607330322\n",
            "2024-05-15 03:09:01 INFO [auto_gptq.quantization.gptq] avg loss: 478.0340881347656\n",
            "INFO - Quantizing mlp.down_proj in layer 24/24...\n",
            "2024-05-15 03:09:04 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 24/24...\n",
            "2024-05-15 03:09:06 INFO [auto_gptq.quantization.gptq] duration: 1.7311036586761475\n",
            "2024-05-15 03:09:06 INFO [auto_gptq.quantization.gptq] avg loss: 495.03387451171875\n",
            "2024-05-15 03:09:07 INFO [auto_gptq.modeling._utils] Packing model...\n",
            "2024-05-15 03:09:07 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.q_proj\n",
            "2024-05-15 03:09:07 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.k_proj\n",
            "2024-05-15 03:09:07 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.v_proj\n",
            "2024-05-15 03:09:07 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.o_proj\n",
            "2024-05-15 03:09:08 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.gate_proj\n",
            "2024-05-15 03:09:08 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.up_proj\n",
            "2024-05-15 03:09:08 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.down_proj\n",
            "2024-05-15 03:09:08 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.q_proj\n",
            "2024-05-15 03:09:08 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.k_proj\n",
            "2024-05-15 03:09:08 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.v_proj\n",
            "2024-05-15 03:09:08 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.o_proj\n",
            "2024-05-15 03:09:08 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.gate_proj\n",
            "2024-05-15 03:09:08 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.up_proj\n",
            "2024-05-15 03:09:09 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.down_proj\n",
            "2024-05-15 03:09:09 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.q_proj\n",
            "2024-05-15 03:09:09 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.k_proj\n",
            "2024-05-15 03:09:09 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.v_proj\n",
            "2024-05-15 03:09:09 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.o_proj\n",
            "2024-05-15 03:09:09 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.gate_proj\n",
            "2024-05-15 03:09:09 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.up_proj\n",
            "2024-05-15 03:09:09 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.down_proj\n",
            "2024-05-15 03:09:10 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.q_proj\n",
            "2024-05-15 03:09:10 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.k_proj\n",
            "2024-05-15 03:09:10 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.v_proj\n",
            "2024-05-15 03:09:10 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.o_proj\n",
            "2024-05-15 03:09:10 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.gate_proj\n",
            "2024-05-15 03:09:10 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.up_proj\n",
            "2024-05-15 03:09:10 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.down_proj\n",
            "2024-05-15 03:09:10 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.q_proj\n",
            "2024-05-15 03:09:10 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.k_proj\n",
            "2024-05-15 03:09:11 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.v_proj\n",
            "2024-05-15 03:09:11 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.o_proj\n",
            "2024-05-15 03:09:11 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.gate_proj\n",
            "2024-05-15 03:09:11 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.up_proj\n",
            "2024-05-15 03:09:11 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.down_proj\n",
            "2024-05-15 03:09:11 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.q_proj\n",
            "2024-05-15 03:09:11 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.k_proj\n",
            "2024-05-15 03:09:11 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.v_proj\n",
            "2024-05-15 03:09:11 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.o_proj\n",
            "2024-05-15 03:09:11 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.gate_proj\n",
            "2024-05-15 03:09:12 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.up_proj\n",
            "2024-05-15 03:09:12 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.down_proj\n",
            "2024-05-15 03:09:12 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.q_proj\n",
            "2024-05-15 03:09:12 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.k_proj\n",
            "2024-05-15 03:09:12 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.v_proj\n",
            "2024-05-15 03:09:12 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.o_proj\n",
            "2024-05-15 03:09:12 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.gate_proj\n",
            "2024-05-15 03:09:12 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.up_proj\n",
            "2024-05-15 03:09:13 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.down_proj\n",
            "2024-05-15 03:09:13 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.q_proj\n",
            "2024-05-15 03:09:13 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.k_proj\n",
            "2024-05-15 03:09:13 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.v_proj\n",
            "2024-05-15 03:09:13 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.o_proj\n",
            "2024-05-15 03:09:13 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.gate_proj\n",
            "2024-05-15 03:09:13 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.up_proj\n",
            "2024-05-15 03:09:13 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.down_proj\n",
            "2024-05-15 03:09:14 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.q_proj\n",
            "2024-05-15 03:09:14 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.k_proj\n",
            "2024-05-15 03:09:14 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.v_proj\n",
            "2024-05-15 03:09:14 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.o_proj\n",
            "2024-05-15 03:09:14 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.gate_proj\n",
            "2024-05-15 03:09:14 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.up_proj\n",
            "2024-05-15 03:09:14 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.down_proj\n",
            "2024-05-15 03:09:14 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.q_proj\n",
            "2024-05-15 03:09:14 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.k_proj\n",
            "2024-05-15 03:09:15 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.v_proj\n",
            "2024-05-15 03:09:15 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.o_proj\n",
            "2024-05-15 03:09:15 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.gate_proj\n",
            "2024-05-15 03:09:15 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.up_proj\n",
            "2024-05-15 03:09:15 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.down_proj\n",
            "2024-05-15 03:09:15 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.q_proj\n",
            "2024-05-15 03:09:15 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.k_proj\n",
            "2024-05-15 03:09:15 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.v_proj\n",
            "2024-05-15 03:09:15 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.o_proj\n",
            "2024-05-15 03:09:16 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.gate_proj\n",
            "2024-05-15 03:09:16 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.up_proj\n",
            "2024-05-15 03:09:16 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.down_proj\n",
            "2024-05-15 03:09:16 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.q_proj\n",
            "2024-05-15 03:09:16 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.k_proj\n",
            "2024-05-15 03:09:16 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.v_proj\n",
            "2024-05-15 03:09:16 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.o_proj\n",
            "2024-05-15 03:09:16 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.gate_proj\n",
            "2024-05-15 03:09:16 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.up_proj\n",
            "2024-05-15 03:09:17 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.down_proj\n",
            "2024-05-15 03:09:17 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.q_proj\n",
            "2024-05-15 03:09:17 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.k_proj\n",
            "2024-05-15 03:09:17 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.v_proj\n",
            "2024-05-15 03:09:17 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.o_proj\n",
            "2024-05-15 03:09:17 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.gate_proj\n",
            "2024-05-15 03:09:17 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.up_proj\n",
            "2024-05-15 03:09:17 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.down_proj\n",
            "2024-05-15 03:09:18 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.q_proj\n",
            "2024-05-15 03:09:18 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.k_proj\n",
            "2024-05-15 03:09:18 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.v_proj\n",
            "2024-05-15 03:09:18 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.o_proj\n",
            "2024-05-15 03:09:18 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.gate_proj\n",
            "2024-05-15 03:09:18 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.up_proj\n",
            "2024-05-15 03:09:18 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.down_proj\n",
            "2024-05-15 03:09:18 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.q_proj\n",
            "2024-05-15 03:09:19 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.k_proj\n",
            "2024-05-15 03:09:19 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.v_proj\n",
            "2024-05-15 03:09:19 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.o_proj\n",
            "2024-05-15 03:09:19 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.gate_proj\n",
            "2024-05-15 03:09:19 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.up_proj\n",
            "2024-05-15 03:09:19 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.down_proj\n",
            "2024-05-15 03:09:19 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.q_proj\n",
            "2024-05-15 03:09:19 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.k_proj\n",
            "2024-05-15 03:09:19 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.v_proj\n",
            "2024-05-15 03:09:19 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.o_proj\n",
            "2024-05-15 03:09:19 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.gate_proj\n",
            "2024-05-15 03:09:20 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.up_proj\n",
            "2024-05-15 03:09:20 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.down_proj\n",
            "2024-05-15 03:09:20 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.q_proj\n",
            "2024-05-15 03:09:20 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.k_proj\n",
            "2024-05-15 03:09:20 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.v_proj\n",
            "2024-05-15 03:09:20 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.o_proj\n",
            "2024-05-15 03:09:20 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.gate_proj\n",
            "2024-05-15 03:09:20 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.up_proj\n",
            "2024-05-15 03:09:21 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.down_proj\n",
            "2024-05-15 03:09:21 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.q_proj\n",
            "2024-05-15 03:09:21 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.k_proj\n",
            "2024-05-15 03:09:21 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.v_proj\n",
            "2024-05-15 03:09:21 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.o_proj\n",
            "2024-05-15 03:09:21 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.gate_proj\n",
            "2024-05-15 03:09:21 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.up_proj\n",
            "2024-05-15 03:09:21 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.down_proj\n",
            "2024-05-15 03:09:22 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.q_proj\n",
            "2024-05-15 03:09:22 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.k_proj\n",
            "2024-05-15 03:09:22 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.v_proj\n",
            "2024-05-15 03:09:22 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.o_proj\n",
            "2024-05-15 03:09:22 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.gate_proj\n",
            "2024-05-15 03:09:22 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.up_proj\n",
            "2024-05-15 03:09:22 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.down_proj\n",
            "2024-05-15 03:09:22 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.q_proj\n",
            "2024-05-15 03:09:23 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.k_proj\n",
            "2024-05-15 03:09:23 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.v_proj\n",
            "2024-05-15 03:09:23 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.o_proj\n",
            "2024-05-15 03:09:23 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.gate_proj\n",
            "2024-05-15 03:09:23 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.up_proj\n",
            "2024-05-15 03:09:23 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.down_proj\n",
            "2024-05-15 03:09:23 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.q_proj\n",
            "2024-05-15 03:09:23 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.k_proj\n",
            "2024-05-15 03:09:23 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.v_proj\n",
            "2024-05-15 03:09:23 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.o_proj\n",
            "2024-05-15 03:09:24 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.gate_proj\n",
            "2024-05-15 03:09:24 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.up_proj\n",
            "2024-05-15 03:09:24 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.down_proj\n",
            "2024-05-15 03:09:24 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.q_proj\n",
            "2024-05-15 03:09:24 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.k_proj\n",
            "2024-05-15 03:09:24 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.v_proj\n",
            "2024-05-15 03:09:24 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.o_proj\n",
            "2024-05-15 03:09:24 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.gate_proj\n",
            "2024-05-15 03:09:24 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.up_proj\n",
            "2024-05-15 03:09:25 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.down_proj\n",
            "2024-05-15 03:09:25 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.q_proj\n",
            "2024-05-15 03:09:25 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.k_proj\n",
            "2024-05-15 03:09:25 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.v_proj\n",
            "2024-05-15 03:09:25 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.o_proj\n",
            "2024-05-15 03:09:25 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.gate_proj\n",
            "2024-05-15 03:09:25 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.up_proj\n",
            "2024-05-15 03:09:25 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.down_proj\n",
            "2024-05-15 03:09:26 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.q_proj\n",
            "2024-05-15 03:09:26 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.k_proj\n",
            "2024-05-15 03:09:26 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.v_proj\n",
            "2024-05-15 03:09:26 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.o_proj\n",
            "2024-05-15 03:09:26 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.gate_proj\n",
            "2024-05-15 03:09:26 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.up_proj\n",
            "2024-05-15 03:09:26 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.down_proj\n",
            "2024-05-15 03:09:26 INFO [auto_gptq.modeling._utils] Model packed.\n",
            "2024-05-15 03:09:26 INFO [__main__] Time to quantize model at Qwen1.5-0.5B with use_triton=False: 360.81\n",
            "2024-05-15 03:09:26 INFO [__main__] Saving quantized model to Qwen1.5-0.5B\n",
            "2024-05-15 03:09:28 INFO [__main__] Saving tokenizer to Qwen1.5-0.5B\n",
            "2024-05-15 03:09:29 INFO [__main__] Done.\n"
          ]
        }
      ]
    }
  ]
}