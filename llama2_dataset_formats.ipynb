{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXVGk22shl0s9gXyAxtw3g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erickrus/llm/blob/main/llama2_dataset_formats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama 2 Dataset Formats\n",
        "\n",
        "\n",
        "Video: The 4 Essential Dataset Types for LLMs: A Deep Dive [here](https://www.youtube.com/watch?v=3y4io94ptrw)\n",
        "\n",
        "\n",
        "When creating a dataset for Llama2(or most of GPT based models for that matter), there are typically four different dataset formats in my experience.\n",
        "\n",
        "While these datasets can have subtypes of formats, they generally fit into one of these 4.\n",
        "\n",
        "Each of these formats varies in their difficulty and their flexibility.  With the easier formats being less flexible in their abilities\n",
        "\n",
        "```what's up guys today I want to make a quick video going over the different types of data set formats you can have when you are fine-tuning llms uh in this case we're going to use llama 2 as an example. so I am one to believe that most data set types for llms can be broken into one of **4** data set types those are the pre-training format the simple format the instruct format and then the chat format```\n",
        "\n",
        "## Pretraining Format\n",
        "\n",
        "This format is the format used to actually pretrain GPT-like models.\n",
        "\n",
        "It's simply a whole bunch of text with a BOS and EOS token to mark the beginning of the text.\n",
        "\n",
        "Base models are trained with this format of dataset.\n",
        "\n",
        "Models generated with these datasets are not typically as useful outside of few-shot and zero-shot learning(with creative prompts)\n",
        "\n",
        "\n",
        "```Going over the pre-training format. This format is used to pre-train gpt like models. It's basically that you just have terabytes of text and all the work you do is breaking them into chunks of the context window, add a BOS and EOS token to mark the start and end of that chunk. And then, just train on ton of text, so that the model learns uh patterns that can be found in English or other languages or even just patterns in sequences basically```\n",
        "\n",
        "\n",
        "```So the pre-training format isn't really useful outside of few shot and zero shot learning. This would be like the original gpt3 where you'd have to give a few examples of what you want or have some creative prompting to really get the models to work. Well, they weren't super easy to use um but they were still somewhat useful, but they definitely are the least useful uh format that you can train a model on but they are needed to get the base models```\n",
        "\n",
        "## Simple Format\n",
        "\n",
        "The simple format is the easiest one to do that can provide value with finetuning.  \n",
        "\n",
        "You use this one when you want to use these LLMs to accomplish a few tasks and you have input and output pairs for your dataset.\n",
        "\n",
        "The model will learn to generate an output given an input.\n",
        "\n",
        "The following is an example of the format:\n",
        "\n",
        "This example is from a quotes dataset.  The input would be the type of quote, and the actual quote would be the output.\n",
        "\n",
        "```\n",
        "<s>success: The successful cannot be unhappy -- it was a contradiction in terms.</s>\n",
        "```\n",
        "\n",
        "See a video on this dataset [here](https://www.youtube.com/watch?v=07ppAKvOhqk&ab_channel=Brillibits)\n",
        "\n",
        "```The next format which I'm calling the simple format is a format that's one of the easiest to do that can provide value and fine tuning rather than just basic pre-training uh and to use this format you just need input and output pairs. So you have a problem some big document and you want you know there's some then structured output that a human normally would you know read the input and then generate that output that is open to automation if you have a big enough data set and you can then structure it in such a way to do that. You just basically give the input, and then give the output. So for example, I have a quotes data set that I reference many times in my videos and um what in this case what the input would be is just the type of quote here which is a quote about success and then the output would be this quarterback success and the idea being is that if you have enough of different types of quotes and their respective output that you can then generate new quotes given a different word```\n",
        "\n",
        "### Simple Format With Tags\n",
        "\n",
        "You could also use tags to clearly mark the start and end of different parts of your input and output.  \n",
        "\n",
        "Doing this can allow you to easily train a model to do different tasks and allows easy parsing of the output.\n",
        "\n",
        "You use models trained with this format like functions.\n",
        "\n",
        "```\n",
        "<s><|START TASK 1|><|START TASK 1 INPUT|>Task 1 input data is here<|END TASK 1 INPUT|><|START TASK 1 OUTPUT|>The correct output given the task and input data<|END TASK 1 OUTPUT|><|END TASK 1|></s>\n",
        "```\n",
        "\n",
        "```A variation of this data set type would be what I call Simple format with tags. And so the idea is still the same you have the basic input output pairs. But you add tags to allow easier parsing of the output and it also allows you to more easily mark multiple tasks. so have a model be able to do multiple things. So for example start of task 1 and then a tag to Mark the start or the input and you can have other tags in here as well and then the end of task one input starter task one output and then this would be the output that you would want to be generated given an input and then tags to Mark the end of the task output and then tags to Mark the end of that task so you could have a model trend on five tasks and if you want to change the mode it's operating in just change these tags here and again if you have enough input output pairs you can generate a model to automate these different tasks and switch between them by changing these tags how I always tell my clients is that models like this are to be treated like functions uh not like uh chat Bots how chat GPT is like it's meant to be used on the back end and a user should never direct uh should never directly interact with a model like this.```\n",
        "\n",
        "## Instruct Format\n",
        "\n",
        "The instruct dataset format takes more work but is great in allowing you to give instructions to LLM and have it perform those tasks.\n",
        "\n",
        "These models can be flexible on a variety of tasks, and you can also include your own custom tasks to the dataset to have it both be flexible, but good at your custom tasks.\n",
        "\n",
        "There are many different types of instruct dataset formats.  Here is an example of an instruct dataset with and without context.\n",
        "\n",
        "The following is an example of the format with context:\n",
        "```\n",
        "<s>From the text below, tell me where Mount Balinhard got its name.\n",
        "\n",
        "Input:\n",
        "Mount Balinhard is a summit in Alberta, Canada.\n",
        "\n",
        "Mount Balinhard was named for a title bestowed on the Earl of Southesk.\n",
        "\n",
        "Output:\n",
        "Balinhard was a title bestowed on the Earl of Southesk, from which Mount Balinhard gets its name.</s>\n",
        "```\n",
        "\n",
        "Without context:\n",
        "\n",
        "```\n",
        "<s>How many syllables are in the word smarter?\n",
        "\n",
        "Output:\n",
        "There are two syllables in the word smarter: smart-er.</s>\n",
        "```\n",
        "\n",
        "`\n",
        "the next format I want to talk about is the instruct format the instruct format allows your models to be much more flexible in their abilities and how you prompt them and use them but they are harder to get training on your own data because you have to format your data as an instruction uh what I like to do when making a custom instruct model especially if we are low on uh instructions for a custom task is use an instruction data set like the Dall-E 15K data set and then add custom tasks that we care more about and maybe add like a two to one ratio to then add our custom tasks and that allows the model to be flexible while um learning the tasks that we really care about um there are many different variations of an instruct format you could have things like Chain of Thought which sometimes shows to improve the results you can have context you can have no context here we see an example with context so here's the instruction and then here's the input context and then the output and then without contacts we just have the instruction and then the output so when using this model you would create a prompt based upon whether you have context or not first get the instruction if you have contacts add the input tag add the context and then the output so you see this example here and here but for example very common use case is summarize some article that'd be the instruction here the input would be the article and then you'd give everything up to output and then the model would generate the summary I think instruct models are a good balance between Simplicity and their flexibility um they're obviously more flexible than the simple format\n",
        "`\n",
        "\n",
        "## Chat Format\n",
        "\n",
        "Chat is the hardest to get working well.  This is due to the fact that conversations have a high level of variance.  \n",
        "\n",
        "Having a large diverse dataset, and then using RLHF is typically key to getting good results.\n",
        "\n",
        "The following is an example of the format for chat llama2 models:\n",
        "\n",
        "```\n",
        "[INST]<<SYS>>\n",
        "You are a friendly chatbot that gives helpful answers\n",
        "<</SYS>>\n",
        "\n",
        "Hello[/INST]Hello, how are you?</s><s>[INST]Good, please tell me what 1+1 is.[/INST]1+1=2. Please let me know if you need anything else!</s>\n",
        "```\n",
        "\n",
        "Chat models can be with or without system prompts, but having a well-working system prompt can give you more control of how the model works.\n",
        "\n",
        "Thus, having a large diverse dataset, with a large diverse system prompts can be very hard.\n",
        "\n",
        "`\n",
        "but as we will go over now the chat format they are much simpler than a chat model and we'll go over that now so it's my opinion that the chat format is the hardest to get working well uh this is due in part to the fact that conversations have a lot of variance you could think of an instruction as a single interaction chat but chat needs to have um you know multiple interactions and that just means that you need more data you also need a diverse data set to reflect all the different types of conversations you could have and then typically since you also want to use a chat model as a consumer facing product you want it to you know not say some awful stuff and then that means that you have to do reinforcement learning to get good results that you would feel comfortable enough to have it face a product chat models can also sometimes come with a system prompt like chassis BT and the chat llamas do um those are very very useful but it's hard again that's another layer of having to collect data because for example a system prompt saying respond as a pirate versus you know respond as normal uh well now you need all your data sets to be in Pirate format and so that just makes the the breadth of your data set needs to be that much bigger there's also unlike the instruct data sets out there not really any data set that I have found that has both system prompts and multi-turn conversations I've seen data sets like open Orca that has system prompts but that's only one interaction a human says something and the model responds um so that's not ideal and I've seen data sets that are multi-turned but then don't have system prompts so unlike how I said I was building off of like Dall-E 15K for custom instruct models I have not found a chat data set yet that is a good um data set to build on top of there are many ways you can structure a chat model there's no hard rule on how to do so this is just the format that the Llama models were trained on the chat llama models um but the core of a chat model is just having you know human and then AI human and AI so here's here how you set this is imprompt for the chat model this is the first human message AI message and then human human message and then the AI message and so an ideal data set would be a diverse amount of system prompts not only in their meaning but how they're said as well as the conversations and how they relate to the system prompts so as I've said um due to those two factors the amount of day you need grows if you say that you don't need a system prompt which maybe you don't need a system prompt then the problem does get easier so I have included some examples of these data sets so of course you'll need to pip install the requirements and I have examples for the symbol data set which is the quotes data set that I've gone over many times at this point there's an instruct data set based on the Dall-E 15K data set and then for now there's no chat data set but I plan on adding one at a later date `"
      ],
      "metadata": {
        "id": "Sx3LdjoXUMti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/datasets/databricks/databricks-dolly-15k"
      ],
      "metadata": {
        "id": "kq6Vgep5d9FO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33iQgQCUSBmw"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mallorbc/llama_dataset_formats\n",
        "!cd /content/llama_dataset_formats && pip3 install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/llama_dataset_formats/instruct/ && python3 make_dataset.py"
      ],
      "metadata": {
        "id": "W_v-IsEacikg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}