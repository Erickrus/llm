{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMBNgzCoHt1gi0LAbFUuuVx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erickrus/llm/blob/main/power_infer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU\n",
        "\n",
        "PowerInfer is a CPU/GPU LLM inference engine leveraging activation locality for your device.\n",
        "\n"
      ],
      "metadata": {
        "id": "SIScF8HEADlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtDt4oQRFykX",
        "outputId": "7f4b33fb-0681-4084-f943-3c6207e7f2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:             x86_64\n",
            "  CPU op-mode(s):         32-bit, 64-bit\n",
            "  Address sizes:          46 bits physical, 48 bits virtual\n",
            "  Byte Order:             Little Endian\n",
            "CPU(s):                   2\n",
            "  On-line CPU(s) list:    0,1\n",
            "Vendor ID:                GenuineIntel\n",
            "  Model name:             Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "    CPU family:           6\n",
            "    Model:                85\n",
            "    Thread(s) per core:   2\n",
            "    Core(s) per socket:   1\n",
            "    Socket(s):            1\n",
            "    Stepping:             3\n",
            "    BogoMIPS:             4000.31\n",
            "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 cl\n",
            "                          flush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc re\n",
            "                          p_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3\n",
            "                           fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand\n",
            "                           hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp \n",
            "                          fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f a\n",
            "                          vx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveop\n",
            "                          t xsavec xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Virtualization features:  \n",
            "  Hypervisor vendor:      KVM\n",
            "  Virtualization type:    full\n",
            "Caches (sum of all):      \n",
            "  L1d:                    32 KiB (1 instance)\n",
            "  L1i:                    32 KiB (1 instance)\n",
            "  L2:                     1 MiB (1 instance)\n",
            "  L3:                     38.5 MiB (1 instance)\n",
            "NUMA:                     \n",
            "  NUMA node(s):           1\n",
            "  NUMA node0 CPU(s):      0,1\n",
            "Vulnerabilities:          \n",
            "  Gather data sampling:   Not affected\n",
            "  Itlb multihit:          Not affected\n",
            "  L1tf:                   Mitigation; PTE Inversion\n",
            "  Mds:                    Vulnerable; SMT Host state unknown\n",
            "  Meltdown:               Vulnerable\n",
            "  Mmio stale data:        Vulnerable\n",
            "  Reg file data sampling: Not affected\n",
            "  Retbleed:               Vulnerable\n",
            "  Spec rstack overflow:   Not affected\n",
            "  Spec store bypass:      Vulnerable\n",
            "  Spectre v1:             Vulnerable: __user pointer sanitization and usercopy barriers only; no swa\n",
            "                          pgs barriers\n",
            "  Spectre v2:             Vulnerable; IBPB: disabled; STIBP: disabled; PBRSB-eIBRS: Not affected; BH\n",
            "                          I: Vulnerable (Syscall hardening enabled)\n",
            "  Srbds:                  Not affected\n",
            "  Tsx async abort:        Vulnerable\n",
            "Mon Jun 17 03:22:56 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKap1owe1Wz2",
        "outputId": "5c0540aa-58b8-400d-fac6-9ee72cc811f1",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PowerInfer'...\n",
            "remote: Enumerating objects: 8876, done.\u001b[K\n",
            "remote: Counting objects: 100% (659/659), done.\u001b[K\n",
            "remote: Compressing objects: 100% (386/386), done.\u001b[K\n",
            "remote: Total 8876 (delta 366), reused 524 (delta 263), pack-reused 8217\u001b[K\n",
            "Receiving objects: 100% (8876/8876), 12.19 MiB | 7.11 MiB/s, done.\n",
            "Resolving deltas: 100% (6180/6180), done.\n",
            "/content/PowerInfer\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gguf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for powerinfer (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#@title installation\n",
        "!git clone https://github.com/SJTU-IPADS/PowerInfer\n",
        "%cd /content/PowerInfer\n",
        "!pip3 install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title build binary\n",
        "%cd /content/PowerInfer\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"build with GPU\")\n",
        "    !cmake -S . -B build -DLLAMA_CUBLAS=ON\n",
        "    !cmake --build build --config Release\n",
        "else:\n",
        "    print(\"build with CPU\")\n",
        "    !cmake -S . -B build\n",
        "    !cmake --build build --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "W_Qjgi-xBho6",
        "outputId": "bdcbf81d-b5cc-446c-c08c-8cce1cf33681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PowerInfer\n",
            "build with GPU\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE  \n",
            "-- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.2.140\") \n",
            "-- cuBLAS found\n",
            "-- The CUDA compiler identification is NVIDIA 12.2.140\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Using CUDA architectures: 52;61;70\n",
            "GNU ld (GNU Binutils for Ubuntu) 2.38\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done (5.9s)\n",
            "-- Generating done (0.1s)\n",
            "-- Build files have been written to: /content/PowerInfer/build\n",
            "[  1%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_get_n_tasks\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml.c:2006:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Karray subscript 71 is above array bounds of ‘\u001b[01m\u001b[Kconst char *[70]\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Warray-bounds\u0007-Warray-bounds\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2006 |     return \u001b[01;35m\u001b[KGGML_OP_NAME[op]\u001b[m\u001b[K;\n",
            "      |            \u001b[01;35m\u001b[K~~~~~~~~~~~~^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml.c:1586:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kwhile referencing ‘\u001b[01m\u001b[KGGML_OP_NAME\u001b[m\u001b[K’\n",
            " 1586 | static const char * \u001b[01;36m\u001b[KGGML_OP_NAME\u001b[m\u001b[K[GGML_OP_COUNT] = {\n",
            "      |                     \u001b[01;36m\u001b[K^~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/include/stdio.h:894\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml.c:21\u001b[m\u001b[K:\n",
            "In function ‘\u001b[01m\u001b[Kprintf\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kggml_graph_print\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/ggml.c:18011:9\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/x86_64-linux-gnu/bits/stdio2.h:112:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K%16s\u001b[m\u001b[K’ directive argument is null [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wformat-overflow=\u0007-Wformat-overflow=\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  112 |   return \u001b[01;35m\u001b[K__printf_chk (__USE_FORTIFY_LEVEL - 1, __fmt, __va_arg_pack ())\u001b[m\u001b[K;\n",
            "      |          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[  2%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-backend.c.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml-quants.c.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_axpy_q4_0_q8_0\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2457:54:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2457 |         __m256 by = _mm256_loadu_ps((const __m256 *)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar *)vy+i*128));\n",
            "      |                                                      \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2457:37:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_loadu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2457 |         __m256 by = _mm256_loadu_ps(\u001b[01;35m\u001b[K(const __m256 *)((char *)vy+i*128)\u001b[m\u001b[K);\n",
            "      |                                     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                                     \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                                     \u001b[01;35m\u001b[Kconst __m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:903:31:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kconst float *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kconst __m256 *\u001b[m\u001b[K’\n",
            "  903 | _mm256_loadu_ps (\u001b[01;36m\u001b[Kfloat const *__P\u001b[m\u001b[K)\n",
            "      |                  \u001b[01;36m\u001b[K~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2460:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2460 |         _mm256_storeu_ps((__m256*)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vz + i*128), by);\n",
            "      |                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2460:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_storeu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2460 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(__m256*)((char*)vz + i*128)\u001b[m\u001b[K, by);\n",
            "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K__m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:909:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kfloat *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[K__m256 *\u001b[m\u001b[K’\n",
            "  909 | _mm256_storeu_ps (\u001b[01;36m\u001b[Kfloat *__P\u001b[m\u001b[K, __m256 __A)\n",
            "      |                   \u001b[01;36m\u001b[K~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2467:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2467 |         by = _mm256_loadu_ps((const __m256 *)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vy+i*128+32));\n",
            "      |                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2467:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_loadu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2467 |         by = _mm256_loadu_ps(\u001b[01;35m\u001b[K(const __m256 *)((char*)vy+i*128+32)\u001b[m\u001b[K);\n",
            "      |                              \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[Kconst __m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:903:31:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kconst float *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kconst __m256 *\u001b[m\u001b[K’\n",
            "  903 | _mm256_loadu_ps (\u001b[01;36m\u001b[Kfloat const *__P\u001b[m\u001b[K)\n",
            "      |                  \u001b[01;36m\u001b[K~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2469:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2469 |         _mm256_storeu_ps((__m256*)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vz + i*128+32), by);\n",
            "      |                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2469:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_storeu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2469 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(__m256*)((char*)vz + i*128+32)\u001b[m\u001b[K, by);\n",
            "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K__m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:909:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kfloat *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[K__m256 *\u001b[m\u001b[K’\n",
            "  909 | _mm256_storeu_ps (\u001b[01;36m\u001b[Kfloat *__P\u001b[m\u001b[K, __m256 __A)\n",
            "      |                   \u001b[01;36m\u001b[K~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2479:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2479 |         by = _mm256_loadu_ps((const __m256 *)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vy+i*128+64));\n",
            "      |                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2479:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_loadu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2479 |         by = _mm256_loadu_ps(\u001b[01;35m\u001b[K(const __m256 *)((char*)vy+i*128+64)\u001b[m\u001b[K);\n",
            "      |                              \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[Kconst __m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:903:31:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kconst float *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kconst __m256 *\u001b[m\u001b[K’\n",
            "  903 | _mm256_loadu_ps (\u001b[01;36m\u001b[Kfloat const *__P\u001b[m\u001b[K)\n",
            "      |                  \u001b[01;36m\u001b[K~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2482:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2482 |         _mm256_storeu_ps((__m256*)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vz + i*128+64), by);\n",
            "      |                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2482:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_storeu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2482 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(__m256*)((char*)vz + i*128+64)\u001b[m\u001b[K, by);\n",
            "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K__m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:909:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kfloat *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[K__m256 *\u001b[m\u001b[K’\n",
            "  909 | _mm256_storeu_ps (\u001b[01;36m\u001b[Kfloat *__P\u001b[m\u001b[K, __m256 __A)\n",
            "      |                   \u001b[01;36m\u001b[K~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2489:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2489 |         by = _mm256_loadu_ps((const __m256 *)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vy+i*128+96));\n",
            "      |                                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2489:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_loadu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2489 |         by = _mm256_loadu_ps(\u001b[01;35m\u001b[K(const __m256 *)((char*)vy+i*128+96)\u001b[m\u001b[K);\n",
            "      |                              \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[Kconst __m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:903:31:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kconst float *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[Kconst __m256 *\u001b[m\u001b[K’\n",
            "  903 | _mm256_loadu_ps (\u001b[01;36m\u001b[Kfloat const *__P\u001b[m\u001b[K)\n",
            "      |                  \u001b[01;36m\u001b[K~~~~~~~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2491:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast discards ‘\u001b[01m\u001b[Kconst\u001b[m\u001b[K’ qualifier from pointer target type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2491 |         _mm256_storeu_ps((__m256*)(\u001b[01;35m\u001b[K(\u001b[m\u001b[Kchar*)vz + i*128+96), by);\n",
            "      |                                    \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2491:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kpassing argument 1 of ‘\u001b[01m\u001b[K_mm256_storeu_ps\u001b[m\u001b[K’ from incompatible pointer type [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wincompatible-pointer-types\u0007-Wincompatible-pointer-types\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2491 |         _mm256_storeu_ps(\u001b[01;35m\u001b[K(__m256*)((char*)vz + i*128+96)\u001b[m\u001b[K, by);\n",
            "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K|\u001b[m\u001b[K\n",
            "      |                          \u001b[01;35m\u001b[K__m256 *\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/immintrin.h:43\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-impl.h:74\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.h:3\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:1\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/lib/gcc/x86_64-linux-gnu/11/include/avxintrin.h:909:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kexpected ‘\u001b[01m\u001b[Kfloat *\u001b[m\u001b[K’ but argument is of type ‘\u001b[01m\u001b[K__m256 *\u001b[m\u001b[K’\n",
            "  909 | _mm256_storeu_ps (\u001b[01;36m\u001b[Kfloat *__P\u001b[m\u001b[K, __m256 __A)\n",
            "      |                   \u001b[01;36m\u001b[K~~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-quants.c:2435:12:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kacc\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2435 |     __m256 \u001b[01;35m\u001b[Kacc\u001b[m\u001b[K = _mm256_setzero_ps();\n",
            "      |            \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "[  5%] \u001b[32mBuilding CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o\u001b[0m\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(6717)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne0\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne0 = src->ne[0];\n",
            "                    ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(6979)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"nrows_dst\"\u001b[0m was declared but never referenced\n",
            "      const int64_t nrows_dst = dst->backend == GGML_BACKEND_GPU && id == g_main_device ? ne0 : row_diff;\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7204)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne10\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne10 = src1->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7226)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"src1_dfloat\"\u001b[0m was declared but never referenced\n",
            "      const dfloat * src1_dfloat = (const dfloat *) src1_ddf_i;\n",
            "                     ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7255)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne10\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne10 = src1->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7357)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"predict_idx\"\u001b[0m was declared but never referenced\n",
            "      int predict_idx = idx;\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(8430)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne01\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne01 = src0->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(8773)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"all_on_device\"\u001b[0m was declared but never referenced\n",
            "      bool all_on_device = (src0->backend == GGML_BACKEND_GPU || src0->backend == GGML_BACKEND_GPU_SPLIT) &&\n",
            "           ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4421)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4549)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4484)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"d\"\u001b[0m was declared but never referenced\n",
            "      short *d = (short *)((char *)vx + ncols * gpu_row * 2);\n",
            "             ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4492)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(579)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"sigmoid_f32\"\u001b[0m was declared but never referenced\n",
            "                   void sigmoid_f32(const float * x, float * dst, const int k) {\n",
            "                        ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(5353)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"dequantize_mul_mat_vec_q4_0_cuda_sparse\"\u001b[0m was declared but never referenced\n",
            "  static void dequantize_mul_mat_vec_q4_0_cuda_sparse(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream, int *lst, float *idx) {\n",
            "              ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(6717)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne0\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne0 = src->ne[0];\n",
            "                    ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(6979)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"nrows_dst\"\u001b[0m was declared but never referenced\n",
            "      const int64_t nrows_dst = dst->backend == GGML_BACKEND_GPU && id == g_main_device ? ne0 : row_diff;\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7204)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne10\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne10 = src1->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7226)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"src1_dfloat\"\u001b[0m was declared but never referenced\n",
            "      const dfloat * src1_dfloat = (const dfloat *) src1_ddf_i;\n",
            "                     ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7255)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne10\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne10 = src1->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7357)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"predict_idx\"\u001b[0m was declared but never referenced\n",
            "      int predict_idx = idx;\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(8430)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne01\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne01 = src0->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(8773)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"all_on_device\"\u001b[0m was declared but never referenced\n",
            "      bool all_on_device = (src0->backend == GGML_BACKEND_GPU || src0->backend == GGML_BACKEND_GPU_SPLIT) &&\n",
            "           ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4421)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4549)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4484)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"d\"\u001b[0m was declared but never referenced\n",
            "      short *d = (short *)((char *)vx + ncols * gpu_row * 2);\n",
            "             ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4492)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(579)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"sigmoid_f32\"\u001b[0m was declared but never referenced\n",
            "                   void sigmoid_f32(const float * x, float * dst, const int k) {\n",
            "                        ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(5353)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"dequantize_mul_mat_vec_q4_0_cuda_sparse\"\u001b[0m was declared but never referenced\n",
            "  static void dequantize_mul_mat_vec_q4_0_cuda_sparse(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream, int *lst, float *idx) {\n",
            "              ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(6717)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne0\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne0 = src->ne[0];\n",
            "                    ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(6979)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"nrows_dst\"\u001b[0m was declared but never referenced\n",
            "      const int64_t nrows_dst = dst->backend == GGML_BACKEND_GPU && id == g_main_device ? ne0 : row_diff;\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7204)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne10\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne10 = src1->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7226)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"src1_dfloat\"\u001b[0m was declared but never referenced\n",
            "      const dfloat * src1_dfloat = (const dfloat *) src1_ddf_i;\n",
            "                     ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7255)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne10\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne10 = src1->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(7357)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"predict_idx\"\u001b[0m was declared but never referenced\n",
            "      int predict_idx = idx;\n",
            "          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(8430)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"ne01\"\u001b[0m was declared but never referenced\n",
            "      const int64_t ne01 = src0->ne[1];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(8773)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"all_on_device\"\u001b[0m was declared but never referenced\n",
            "      bool all_on_device = (src0->backend == GGML_BACKEND_GPU || src0->backend == GGML_BACKEND_GPU_SPLIT) &&\n",
            "           ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4421)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4549)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4484)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"d\"\u001b[0m was declared but never referenced\n",
            "      short *d = (short *)((char *)vx + ncols * gpu_row * 2);\n",
            "             ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(4492)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"bid\"\u001b[0m was declared but never referenced\n",
            "      const int bid = blockIdx.y;\n",
            "                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(579)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"sigmoid_f32\"\u001b[0m was declared but never referenced\n",
            "                   void sigmoid_f32(const float * x, float * dst, const int k) {\n",
            "                        ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PowerInfer/ggml-cuda.cu(5353)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: function \u001b[01m\"dequantize_mul_mat_vec_q4_0_cuda_sparse\"\u001b[0m was declared but never referenced\n",
            "  static void dequantize_mul_mat_vec_q4_0_cuda_sparse(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream, int *lst, float *idx) {\n",
            "              ^\n",
            "\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul_mat_batch_sparse(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:6962:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ksrc1_ddq_i\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 6961 |     const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i\u001b[01;35m\u001b[K, const float * src1_ddf_i,\u001b[m\u001b[K\n",
            "      |                                                                                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            " 6962 | \u001b[01;35m\u001b[K    const \u001b[m\u001b[Kchar * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n",
            "      | \u001b[01;35m\u001b[K^\u001b[m\u001b[K   \u001b[01;35m\u001b[K~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:6963:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ksrc1_padded_row_size\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 6962 |     const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const in\u001b[01;35m\u001b[Kt64_t src1_ncols,\u001b[m\u001b[K\n",
            "      |                                                                                                       \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            " 6963 | \u001b[01;35m\u001b[K    const int64_t sr\u001b[m\u001b[Kc1_padded_row_size, const cudaStream_t & stream) {\n",
            "      | \u001b[01;35m\u001b[K^\u001b[m\u001b[K   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul_mat_vec_sparse_dequantized(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:7251:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ksrc1_ddq_i\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 7250 |     const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i\u001b[01;35m\u001b[K, const float * src1_ddf_i,\u001b[m\u001b[K\n",
            "      |                                                                                                  \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            " 7251 | \u001b[01;35m\u001b[K    const \u001b[m\u001b[Kchar * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n",
            "      | \u001b[01;35m\u001b[K^\u001b[m\u001b[K   \u001b[01;35m\u001b[K~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul_mat_transpose_select_gemm(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:7452:91:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcast from type ‘\u001b[01m\u001b[Kconst float*\u001b[m\u001b[K’ to type ‘\u001b[01m\u001b[Kfloat*\u001b[m\u001b[K’ casts away qualifiers [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcast-qual\u0007-Wcast-qual\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 7452 | ose_cont<<< numBlocks, blockSize, 0, stream>>>((float *)src0_ddf_i, transpose, n\u001b[01;35m\u001b[Ke00, ne01, 1, ne00,\u001b[m\u001b[K ne01,NULL);\n",
            "      |                                                                                 \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K At global scope:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:8771:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kno previous declaration for ‘\u001b[01m\u001b[Kvoid ggml_cuda_axpy(const ggml_tensor*, const ggml_tensor*, ggml_tensor*)\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-declarations\u0007-Wmissing-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 8771 | void \u001b[01;35m\u001b[Kggml_cuda_axpy\u001b[m\u001b[K(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n",
            "      |      \u001b[01;35m\u001b[K^~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_mul_mat_transpose_gemm(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, CUstream_st* const&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:7550:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Ksrc0_ddq_as_f32\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 7550 | \u001b[01;35m\u001b[K        ggml_cuda_pool_free(src0_ddq_as_f32, \u001b[m\u001b[Ksrc0_as);\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/ggml-cuda.cu:7501:8:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Ksrc0_ddq_as_f32\u001b[m\u001b[K’ was declared here\n",
            " 7501 |     flo\u001b[01;36m\u001b[Kat * src0_ddq_a\u001b[m\u001b[Ks_f32;\n",
            "      |        \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[  5%] Built target ggml\n",
            "[  6%] \u001b[32m\u001b[1mLinking CUDA static library libggml_static.a\u001b[0m\n",
            "[  6%] Built target ggml_static\n",
            "[  7%] \u001b[32mBuilding CXX object CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:632:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kno previous declaration for ‘\u001b[01m\u001b[Ktensor_offloading_levels get_offloading_level(llm_tensor)\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-declarations\u0007-Wmissing-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  632 | tensor_offloading_levels \u001b[01;35m\u001b[Kget_offloading_level\u001b[m\u001b[K(llm_tensor tensor) {\n",
            "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint64_t sum_gpu_index(ggml_tensor*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:2722:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kggml_init_params::mem_buffer\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2722 |     ggml_context * ctx_aux = \u001b[01;35m\u001b[Kggml_init({\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~^~\u001b[m\u001b[K\n",
            " 2723 | \u001b[01;35m\u001b[K        /* mem_size */ 1 << 10,\u001b[m\u001b[K\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K        \n",
            " 2724 | \u001b[01;35m\u001b[K    })\u001b[m\u001b[K;\n",
            "      |     \u001b[01;35m\u001b[K~~\u001b[m\u001b[K                                 \n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:2722:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kggml_init_params::no_alloc\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:2805:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kprogress\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2805 |         llama_progress_callback cb = [](\u001b[01;35m\u001b[Kfloat progress\u001b[m\u001b[K, void *ctx) {\n",
            "      |                                         \u001b[01;35m\u001b[K~~~~~~^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:2805:63:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kctx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2805 |         llama_progress_callback cb = [](float progress, \u001b[01;35m\u001b[Kvoid *ctx\u001b[m\u001b[K) {\n",
            "      |                                                         \u001b[01;35m\u001b[K~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Ksize_t llama_augmentation_model_loader::slice_ffn_mat_to_gpu(llama_layer&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:2909:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kgpu_idx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2909 |         ggml_tensor * \u001b[01;35m\u001b[Kgpu_idx\u001b[m\u001b[K = layer.gpu_idx;\n",
            "      |                       \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llm_load_sparse_model_tensors(llama_model_loader&, llama_model&, const llama_context_params*, int, long int, bool, bool, bool, llama_progress_callback, void*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:3165:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kllama_backend_offload\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 3165 |     enum ggml_backend_type \u001b[01;35m\u001b[Kllama_backend_offload\u001b[m\u001b[K = GGML_BACKEND_CPU;\n",
            "      |                            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:3166:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kllama_backend_offload_split\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 3166 |     enum ggml_backend_type \u001b[01;35m\u001b[Kllama_backend_offload_split\u001b[m\u001b[K = GGML_BACKEND_CPU;\n",
            "      |                            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llama_reserve_model_kv_cache(llama_model*, const llama_context_params*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:3319:29:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kunsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 3319 |     if (\u001b[01;35m\u001b[Kmodel->n_gpu_layers < hparams.n_layer + 1\u001b[m\u001b[K) {\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::pair<ggml_tensor*, ggml_tensor*> llm_build_kv_store(ggml_context*, const llama_hparams&, const llama_kv_cache&, ggml_cgraph*, ggml_tensor*, ggml_tensor*, int64_t, int32_t, int32_t, const llm_build_cb&, int64_t)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:4232:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kgraph\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 4232 |          \u001b[01;35m\u001b[Kstruct ggml_cgraph * graph\u001b[m\u001b[K,\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:4677:88:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Knl\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 4677 | onst llm_build_cb no_offload_cb = [](struct ggml_tensor * cur, const char * name, \u001b[01;35m\u001b[Kint nl\u001b[m\u001b[K) {\n",
            "      |                                                                                   \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
            "\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint llama_decode_internal(llama_context&, llama_batch)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:6592:16:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kfull_offload_supported\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 6592 |     const bool \u001b[01;35m\u001b[Kfull_offload_supported\u001b[m\u001b[K =\n",
            "      |                \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kllama_model_params llama_model_default_params()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:9400:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kllama_model_params::reset_gpu_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 9400 |     \u001b[01;35m\u001b[K}\u001b[m\u001b[K;\n",
            "      |     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/llama.cpp:9400:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kllama_model_params::disable_gpu_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "[  8%] \u001b[32m\u001b[1mLinking CXX static library libllama.a\u001b[0m\n",
            "[  8%] Built target llama\n",
            "[  9%] \u001b[34m\u001b[1mGenerating build details from Git\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
            "[ 10%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[ 10%] Built target build_info\n",
            "[ 12%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/train.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 17%] Built target common\n",
            "[ 18%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 19%] Built target test-quantize-fns\n",
            "[ 20%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 21%] Built target test-quantize-perf\n",
            "[ 23%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 24%] Built target test-sampling\n",
            "[ 25%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-llama.dir/test-tokenizer-0-llama.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-llama\u001b[0m\n",
            "[ 26%] Built target test-tokenizer-0-llama\n",
            "[ 27%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0-falcon.dir/test-tokenizer-0-falcon.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0-falcon\u001b[0m\n",
            "[ 28%] Built target test-tokenizer-0-falcon\n",
            "[ 29%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-llama.dir/test-tokenizer-1-llama.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-llama\u001b[0m\n",
            "[ 30%] Built target test-tokenizer-1-llama\n",
            "[ 31%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 32%] Built target test-tokenizer-1-bpe\n",
            "[ 34%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 35%] Built target test-grammar-parser\n",
            "[ 36%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/content/PowerInfer/tests/test-llama-grammar.cpp:5\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:632:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kno previous declaration for ‘\u001b[01m\u001b[Ktensor_offloading_levels get_offloading_level(llm_tensor)\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-declarations\u0007-Wmissing-declarations\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  632 | tensor_offloading_levels \u001b[01;35m\u001b[Kget_offloading_level\u001b[m\u001b[K(llm_tensor tensor) {\n",
            "      |                          \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/content/PowerInfer/tests/test-llama-grammar.cpp:5\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint64_t sum_gpu_index(ggml_tensor*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:2722:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kggml_init_params::mem_buffer\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2722 |     ggml_context * ctx_aux = \u001b[01;35m\u001b[Kggml_init({\u001b[m\u001b[K\n",
            "      |                              \u001b[01;35m\u001b[K~~~~~~~~~^~\u001b[m\u001b[K\n",
            " 2723 | \u001b[01;35m\u001b[K        /* mem_size */ 1 << 10,\u001b[m\u001b[K\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K        \n",
            " 2724 | \u001b[01;35m\u001b[K    })\u001b[m\u001b[K;\n",
            "      |     \u001b[01;35m\u001b[K~~\u001b[m\u001b[K                                 \n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:2722:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kggml_init_params::no_alloc\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:2805:47:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kprogress\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2805 |         llama_progress_callback cb = [](\u001b[01;35m\u001b[Kfloat progress\u001b[m\u001b[K, void *ctx) {\n",
            "      |                                         \u001b[01;35m\u001b[K~~~~~~^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:2805:63:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kctx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2805 |         llama_progress_callback cb = [](float progress, \u001b[01;35m\u001b[Kvoid *ctx\u001b[m\u001b[K) {\n",
            "      |                                                         \u001b[01;35m\u001b[K~~~~~~^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Ksize_t llama_augmentation_model_loader::slice_ffn_mat_to_gpu(llama_layer&)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:2909:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kgpu_idx\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 2909 |         ggml_tensor * \u001b[01;35m\u001b[Kgpu_idx\u001b[m\u001b[K = layer.gpu_idx;\n",
            "      |                       \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llm_load_sparse_model_tensors(llama_model_loader&, llama_model&, const llama_context_params*, int, long int, bool, bool, bool, llama_progress_callback, void*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:3165:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kllama_backend_offload\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 3165 |     enum ggml_backend_type \u001b[01;35m\u001b[Kllama_backend_offload\u001b[m\u001b[K = GGML_BACKEND_CPU;\n",
            "      |                            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:3166:28:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kllama_backend_offload_split\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-but-set-variable\u0007-Wunused-but-set-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 3166 |     enum ggml_backend_type \u001b[01;35m\u001b[Kllama_backend_offload_split\u001b[m\u001b[K = GGML_BACKEND_CPU;\n",
            "      |                            \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid llama_reserve_model_kv_cache(llama_model*, const llama_context_params*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:3319:29:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kunsigned int\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 3319 |     if (\u001b[01;35m\u001b[Kmodel->n_gpu_layers < hparams.n_layer + 1\u001b[m\u001b[K) {\n",
            "      |         \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::pair<ggml_tensor*, ggml_tensor*> llm_build_kv_store(ggml_context*, const llama_hparams&, const llama_kv_cache&, ggml_cgraph*, ggml_tensor*, ggml_tensor*, int64_t, int32_t, int32_t, const llm_build_cb&, int64_t)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:4232:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Kgraph\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 4232 |          \u001b[01;35m\u001b[Kstruct ggml_cgraph * graph\u001b[m\u001b[K,\n",
            "      |          \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:4677:88:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Knl\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-parameter\u0007-Wunused-parameter\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 4677 | onst llm_build_cb no_offload_cb = [](struct ggml_tensor * cur, const char * name, \u001b[01;35m\u001b[Kint nl\u001b[m\u001b[K) {\n",
            "      |                                                                                   \u001b[01;35m\u001b[K~~~~^~\u001b[m\u001b[K\n",
            "\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint llama_decode_internal(llama_context&, llama_batch)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:6592:16:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kfull_offload_supported\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-variable\u0007-Wunused-variable\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 6592 |     const bool \u001b[01;35m\u001b[Kfull_offload_supported\u001b[m\u001b[K =\n",
            "      |                \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kllama_model_params llama_model_default_params()\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:9400:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kllama_model_params::reset_gpu_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            " 9400 |     \u001b[01;35m\u001b[K}\u001b[m\u001b[K;\n",
            "      |     \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/./llama.cpp:9400:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kmissing initializer for member ‘\u001b[01m\u001b[Kllama_model_params::disable_gpu_index\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmissing-field-initializers\u0007-Wmissing-field-initializers\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 37%] Built target test-llama-grammar\n",
            "[ 38%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grad0\u001b[0m\n",
            "[ 39%] Built target test-grad0\n",
            "[ 40%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 41%] Built target test-rope\n",
            "[ 42%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 43%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-c\u001b[0m\n",
            "[ 43%] Built target test-c\n",
            "[ 45%] \u001b[32mBuilding CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/baby-llama\u001b[0m\n",
            "[ 46%] Built target baby-llama\n",
            "[ 47%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched\u001b[0m\n",
            "[ 48%] Built target batched\n",
            "[ 49%] \u001b[32mBuilding CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/batched-bench\u001b[0m\n",
            "[ 50%] Built target batched-bench\n",
            "[ 51%] \u001b[32mBuilding CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/beam-search\u001b[0m\n",
            "[ 52%] Built target beam-search\n",
            "[ 53%] \u001b[32mBuilding CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/benchmark\u001b[0m\n",
            "[ 54%] Built target benchmark\n",
            "[ 56%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/convert-llama2c-to-ggml\u001b[0m\n",
            "[ 57%] Built target convert-llama2c-to-ggml\n",
            "[ 58%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/embedding\u001b[0m\n",
            "[ 59%] Built target embedding\n",
            "[ 60%] \u001b[32mBuilding CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/finetune\u001b[0m\n",
            "[ 61%] Built target finetune\n",
            "[ 62%] \u001b[32mBuilding CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/infill\u001b[0m\n",
            "[ 63%] Built target infill\n",
            "[ 64%] \u001b[32mBuilding CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 65%] Built target llama-bench\n",
            "[ 67%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/llava/llava.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool load_file_to_bytes(const char*, unsigned char**, long int*)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/llava/llava.cpp:130:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ksize_t fread(void*, size_t, size_t, FILE*)\u001b[m\u001b[K’ declared with attribute ‘\u001b[01m\u001b[Kwarn_unused_result\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wunused-result\u0007-Wunused-result\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  130 |     \u001b[01;35m\u001b[Kfread(buffer, 1, fileSize, file)\u001b[m\u001b[K; // Read the file into the buffer\n",
            "      |     \u001b[01;35m\u001b[K~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "[ 68%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o\u001b[0m\n",
            "[ 68%] Built target llava\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX static library libllava_static.a\u001b[0m\n",
            "[ 69%] Built target llava_static\n",
            "[ 70%] \u001b[32mBuilding CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llava-cli\u001b[0m\n",
            "[ 71%] Built target llava-cli\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/main.dir/main.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/main\u001b[0m\n",
            "[ 73%] Built target main\n",
            "[ 74%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/parallel\u001b[0m\n",
            "[ 75%] Built target parallel\n",
            "[ 76%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/perplexity\u001b[0m\n",
            "[ 78%] Built target perplexity\n",
            "[ 79%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize\u001b[0m\n",
            "[ 80%] Built target quantize\n",
            "[ 81%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize-stats\u001b[0m\n",
            "[ 82%] Built target quantize-stats\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/save-load-state\u001b[0m\n",
            "[ 84%] Built target save-load-state\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/simple\u001b[0m\n",
            "[ 86%] Built target simple\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/speculative\u001b[0m\n",
            "[ 89%] Built target speculative\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/train-text-from-scratch\u001b[0m\n",
            "[ 91%] Built target train-text-from-scratch\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/server/CMakeFiles/server.dir/server.cpp.o\u001b[0m\n",
            "In copy constructor ‘\u001b[01m\u001b[Ktask_result::task_result(const task_result&)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = task_result; _Args = {const task_result&}; _Tp = task_result]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/ext/new_allocator.h:162:4\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kstatic void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = task_result; _Args = {const task_result&}; _Tp = task_result]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/alloc_traits.h:516:17\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = task_result; _Alloc = std::allocator<task_result>]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/stl_vector.h:1192:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid llama_server_context::send_error(int, std::string)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1097:32\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:154:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kres.task_result::stop\u001b[m\u001b[K’ may be used uninitialized [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  154 | struct \u001b[01;35m\u001b[Ktask_result\u001b[m\u001b[K {\n",
            "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:\u001b[m\u001b[K In member function ‘\u001b[01m\u001b[Kvoid llama_server_context::send_error(int, std::string)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1093:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Kres\u001b[m\u001b[K’ declared here\n",
            " 1093 |         task_result \u001b[01;36m\u001b[Kres\u001b[m\u001b[K;\n",
            "      |                     \u001b[01;36m\u001b[K^~~\u001b[m\u001b[K\n",
            "In copy constructor ‘\u001b[01m\u001b[Ktask_server::task_server(const task_server&)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/ext/new_allocator.h:162:4\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kstatic void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/alloc_traits.h:516:17\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = task_server; _Alloc = std::allocator<task_server>]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/stl_vector.h:1192:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kint llama_server_context::request_completion(json, bool, bool)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1259:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kmain(int, char**)::<lambda(const httplib::Request&, httplib::Response&)>\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:2355:61\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:145:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask.task_server::target_id\u001b[m\u001b[K’ may be used uninitialized [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  145 | struct \u001b[01;35m\u001b[Ktask_server\u001b[m\u001b[K {\n",
            "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1253:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask\u001b[m\u001b[K’ declared here\n",
            " 1253 |         task_server \u001b[01;36m\u001b[Ktask\u001b[m\u001b[K;\n",
            "      |                     \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In copy constructor ‘\u001b[01m\u001b[Ktask_server::task_server(const task_server&)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/ext/new_allocator.h:162:4\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kstatic void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/alloc_traits.h:516:17\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = task_server; _Alloc = std::allocator<task_server>]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/stl_vector.h:1192:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kint llama_server_context::request_completion(json, bool, bool)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1259:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kmain(int, char**)::<lambda(const httplib::Request&, httplib::Response&)>\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:2410:61\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:145:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask.task_server::target_id\u001b[m\u001b[K’ may be used uninitialized [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  145 | struct \u001b[01;35m\u001b[Ktask_server\u001b[m\u001b[K {\n",
            "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1253:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask\u001b[m\u001b[K’ declared here\n",
            " 1253 |         task_server \u001b[01;36m\u001b[Ktask\u001b[m\u001b[K;\n",
            "      |                     \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "In copy constructor ‘\u001b[01m\u001b[Ktask_server::task_server(const task_server&)\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid __gnu_cxx::new_allocator<_Tp>::construct(_Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/ext/new_allocator.h:162:4\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kstatic void std::allocator_traits<std::allocator<_Tp1> >::construct(std::allocator_traits<std::allocator<_Tp1> >::allocator_type&, _Up*, _Args&& ...) [with _Up = task_server; _Args = {const task_server&}; _Tp = task_server]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/alloc_traits.h:516:17\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid std::vector<_Tp, _Alloc>::push_back(const value_type&) [with _Tp = task_server; _Alloc = std::allocator<task_server>]\u001b[m\u001b[K’ at \u001b[01m\u001b[K/usr/include/c++/11/bits/stl_vector.h:1192:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kint llama_server_context::request_completion(json, bool, bool)\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1259:30\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kmain(int, char**)::<lambda(const httplib::Request&, httplib::Response&)>\u001b[m\u001b[K’ at \u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:2514:61\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:145:8:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask.task_server::target_id\u001b[m\u001b[K’ may be used uninitialized [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmaybe-uninitialized\u0007-Wmaybe-uninitialized\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  145 | struct \u001b[01;35m\u001b[Ktask_server\u001b[m\u001b[K {\n",
            "      |        \u001b[01;35m\u001b[K^~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[K/content/PowerInfer/examples/server/server.cpp:1253:21:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K‘\u001b[01m\u001b[Ktask\u001b[m\u001b[K’ declared here\n",
            " 1253 |         task_server \u001b[01;36m\u001b[Ktask\u001b[m\u001b[K;\n",
            "      |                     \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/server\u001b[0m\n",
            "[ 93%] Built target server\n",
            "[ 94%] \u001b[32mBuilding CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/export-lora\u001b[0m\n",
            "[ 95%] Built target export-lora\n",
            "[ 96%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/vdot\u001b[0m\n",
            "[ 97%] Built target vdot\n",
            "[ 98%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/q8dot\u001b[0m\n",
            "[100%] Built target q8dot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title download Llama3-8B-Chinese-Chat-GGUF-4bit weights\n",
        "%cd /content/PowerInfer\n",
        "!mkdir -p /content/PowerInfer/Llama3-8B-Chinese-Chat-GGUF-4bit\n",
        "\n",
        "!cd Llama3-8B-Chinese-Chat-GGUF-4bit && wget https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-4bit/resolve/main/config.json\n",
        "!cd Llama3-8B-Chinese-Chat-GGUF-4bit && wget https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-4bit/resolve/main/Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf\n"
      ],
      "metadata": {
        "id": "KcGkK2jq7cQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "46a8f5f2-992d-4da2-c8ba-8f205f86df1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PowerInfer\n",
            "--2024-06-17 03:27:28--  https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-4bit/resolve/main/config.json\n",
            "Resolving huggingface.co (huggingface.co)... 13.33.30.114, 13.33.30.23, 13.33.30.76, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.33.30.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 649 [text/plain]\n",
            "Saving to: ‘config.json’\n",
            "\n",
            "config.json         100%[===================>]     649  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-17 03:27:29 (348 MB/s) - ‘config.json’ saved [649/649]\n",
            "\n",
            "--2024-06-17 03:27:29--  https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-4bit/resolve/main/Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.33.30.114, 13.33.30.23, 13.33.30.76, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.33.30.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/b8/19/b8190a9fdff5739e6064ef01f1c841ca5d0daea0b272c428cb794098b2b4748d/242ac8dd3eabcb1e5fcd3d78912eaf904f08bb6ecfed8bac9ac9a0b7a837fcb8?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf%3B+filename%3D%22Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf%22%3B&Expires=1718854049&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODg1NDA0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I4LzE5L2I4MTkwYTlmZGZmNTczOWU2MDY0ZWYwMWYxYzg0MWNhNWQwZGFlYTBiMjcyYzQyOGNiNzk0MDk4YjJiNDc0OGQvMjQyYWM4ZGQzZWFiY2IxZTVmY2QzZDc4OTEyZWFmOTA0ZjA4YmI2ZWNmZWQ4YmFjOWFjOWEwYjdhODM3ZmNiOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=EDzIh3zZOINORUGqy2nWdyhh7DE0SX-1aqVq3KK%7Ev0XhlDgIM2RcjZ0Xs-18EWEwmwPLtlN-PLczdHY9Mw%7E0de1a96M0S4USpWgvU3iclc1epDMptbDi2BSfBiPBvVUhIN4cJP7gvfVcsW-m0Fkd8pvVKDOu7ebPkeqkQLu0o2q9i1t%7EabuzyeGk3PpkfKFTtqvfwYaXPfWbfguO5URCjnLqgArxD%7EshEWRhKNMxFFZf%7E6VKyThC0ufYpJJvmGsFDk-BNt%7EWopInd55TLGHz0H%7EDvLyBMkHWvR3TuoiekNAOBTKCAZ3vNKh117XdKb6lLPFPir5KmFVhIChPgcc4TA__&Key-Pair-Id=K2FPYV99P2N66Q [following]\n",
            "--2024-06-17 03:27:29--  https://cdn-lfs-us-1.huggingface.co/repos/b8/19/b8190a9fdff5739e6064ef01f1c841ca5d0daea0b272c428cb794098b2b4748d/242ac8dd3eabcb1e5fcd3d78912eaf904f08bb6ecfed8bac9ac9a0b7a837fcb8?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf%3B+filename%3D%22Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf%22%3B&Expires=1718854049&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODg1NDA0OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2I4LzE5L2I4MTkwYTlmZGZmNTczOWU2MDY0ZWYwMWYxYzg0MWNhNWQwZGFlYTBiMjcyYzQyOGNiNzk0MDk4YjJiNDc0OGQvMjQyYWM4ZGQzZWFiY2IxZTVmY2QzZDc4OTEyZWFmOTA0ZjA4YmI2ZWNmZWQ4YmFjOWFjOWEwYjdhODM3ZmNiOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=EDzIh3zZOINORUGqy2nWdyhh7DE0SX-1aqVq3KK%7Ev0XhlDgIM2RcjZ0Xs-18EWEwmwPLtlN-PLczdHY9Mw%7E0de1a96M0S4USpWgvU3iclc1epDMptbDi2BSfBiPBvVUhIN4cJP7gvfVcsW-m0Fkd8pvVKDOu7ebPkeqkQLu0o2q9i1t%7EabuzyeGk3PpkfKFTtqvfwYaXPfWbfguO5URCjnLqgArxD%7EshEWRhKNMxFFZf%7E6VKyThC0ufYpJJvmGsFDk-BNt%7EWopInd55TLGHz0H%7EDvLyBMkHWvR3TuoiekNAOBTKCAZ3vNKh117XdKb6lLPFPir5KmFVhIChPgcc4TA__&Key-Pair-Id=K2FPYV99P2N66Q\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.165.102.80, 3.165.102.95, 3.165.102.25, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.165.102.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4661724704 (4.3G) [binary/octet-stream]\n",
            "Saving to: ‘Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf’\n",
            "\n",
            "Llama3-8B-Chinese-C 100%[===================>]   4.34G  22.1MB/s    in 3m 37s  \n",
            "\n",
            "2024-06-17 03:31:06 (20.5 MB/s) - ‘Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf’ saved [4661724704/4661724704]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title inference\n",
        "%cd /content/PowerInfer\n",
        "PROMPT=\"你好啊，请给我讲个简短的笑话吧\" #111@param {type:\"string\"}\n",
        "PROMPT += \"\\n\"\n",
        "import datetime\n",
        "print(datetime.datetime.now())\n",
        "# https://huggingface.co/shenzhi-wang/Llama3-70B-Chinese-Chat-GGUF-4bit\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"run with GPU\")\n",
        "    !./build/bin/main -m ./Llama3-8B-Chinese-Chat-GGUF-4bit/Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf -n 128 -t 8 -p {PROMPT} --vram-budget 15\n",
        "else:\n",
        "    print(\"run with CPU\")\n",
        "    !./build/bin/main -m ./Llama3-8B-Chinese-Chat-GGUF-4bit/Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf -n 128 -t 8 -p {PROMPT}\n",
        "\n",
        "\n",
        "print(datetime.datetime.now())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB238QCj29A-",
        "outputId": "c78774c9-7bd9-4457-a0e9-b74f4c8ec73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PowerInfer\n",
            "2024-06-17 03:31:06.590677\n",
            "using GPU\n",
            "Log start\n",
            "main: build = 1581 (4896dfb)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1718595066\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ./Llama3-8B-Chinese-Chat-GGUF-4bit/Llama3-8B-Chinese-Chat-q4_0-v2_1.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 128256,     1,     1 ]\n",
            "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   19:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   20:            blk.2.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   21:            blk.2.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   22:              blk.2.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   23:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   24:              blk.2.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   25:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   26:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   27:              blk.2.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   28:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   29:            blk.3.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   30:            blk.3.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   31:              blk.3.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   32:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   33:              blk.3.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   34:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   35:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   36:              blk.3.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   37:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   38:            blk.4.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   39:            blk.4.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   40:              blk.4.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   41:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   42:              blk.4.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   43:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   44:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   45:              blk.4.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   46:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   47:            blk.5.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   48:            blk.5.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   49:              blk.5.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   50:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   51:              blk.5.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   52:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   53:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   54:              blk.5.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   55:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   56:            blk.6.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   57:            blk.6.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   58:              blk.6.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   59:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   60:              blk.6.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   61:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   62:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   63:              blk.6.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   64:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   65:            blk.7.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   66:            blk.7.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   67:              blk.7.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   68:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   69:              blk.7.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   70:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   71:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   72:              blk.7.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   73:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   74:            blk.8.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   75:            blk.8.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   76:              blk.8.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   77:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   78:              blk.8.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   79:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   80:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   81:              blk.8.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   82:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   83:           blk.10.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   84:           blk.10.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   85:             blk.10.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   86:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   87:             blk.10.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   88:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   89:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   90:             blk.10.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   91:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   92:           blk.11.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   93:           blk.11.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   94:             blk.11.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor   95:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor   96:             blk.11.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor   97:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   98:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor   99:             blk.11.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  100:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  101:           blk.12.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  102:           blk.12.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  103:             blk.12.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  104:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  105:             blk.12.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  106:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  107:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  108:             blk.12.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  109:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  110:           blk.13.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  111:           blk.13.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  112:             blk.13.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  113:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  114:             blk.13.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  115:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  116:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  117:             blk.13.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  118:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  119:           blk.14.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  120:           blk.14.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  121:             blk.14.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  122:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  123:             blk.14.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  124:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  125:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  126:             blk.14.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  127:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  128:           blk.15.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  129:           blk.15.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  130:             blk.15.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  131:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  132:             blk.15.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  133:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  134:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  135:             blk.15.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  136:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  137:           blk.16.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  138:           blk.16.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  139:             blk.16.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  140:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  141:             blk.16.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  142:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  143:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  144:             blk.16.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  145:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  146:           blk.17.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  147:           blk.17.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  148:             blk.17.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  149:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  150:             blk.17.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  151:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  152:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  153:             blk.17.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  154:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  155:           blk.18.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  156:           blk.18.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  157:             blk.18.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  158:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  159:             blk.18.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  160:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  161:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  162:             blk.18.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  163:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  164:           blk.19.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  165:           blk.19.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  166:             blk.19.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  167:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  168:             blk.19.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  169:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  170:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  171:             blk.19.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  172:           blk.20.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  173:             blk.20.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  174:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  175:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  176:             blk.20.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  177:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  178:            blk.9.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  179:            blk.9.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  180:              blk.9.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  181:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  182:              blk.9.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  183:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  184:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  185:              blk.9.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  186:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  187:           blk.20.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  188:             blk.20.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  190:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  191:           blk.21.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  192:           blk.21.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  193:             blk.21.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  194:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  195:             blk.21.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  196:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  197:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  198:             blk.21.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  199:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  200:           blk.22.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  201:           blk.22.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  202:             blk.22.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  203:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  204:             blk.22.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  205:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  206:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  207:             blk.22.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  208:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  209:           blk.23.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  210:           blk.23.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  211:             blk.23.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  212:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  213:             blk.23.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  214:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  215:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  216:             blk.23.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  218:           blk.24.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  220:             blk.24.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  222:             blk.24.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  223:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  224:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  225:             blk.24.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  226:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  227:           blk.25.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  228:           blk.25.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  229:             blk.25.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  230:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  231:             blk.25.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  232:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  233:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  234:             blk.25.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  235:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  236:           blk.26.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  237:           blk.26.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  238:             blk.26.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  239:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  240:             blk.26.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  241:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  242:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  243:             blk.26.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  244:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  245:           blk.27.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  246:           blk.27.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  247:             blk.27.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  248:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  249:             blk.27.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  250:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  251:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  252:             blk.27.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  253:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  254:           blk.28.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  255:           blk.28.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  256:             blk.28.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  257:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  258:             blk.28.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  259:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  260:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  261:             blk.28.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  262:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  263:           blk.29.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  264:           blk.29.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  265:             blk.29.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  266:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  267:             blk.29.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  268:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  269:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  270:             blk.29.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  271:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  272:           blk.30.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  273:           blk.30.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  274:             blk.30.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  275:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  276:             blk.30.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  277:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  278:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  279:             blk.30.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  280:           blk.31.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  281:             blk.31.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
            "llama_model_loader: - tensor  282:             blk.31.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  284:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  285:             blk.31.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
            "llama_model_loader: - tensor  286:                    output.weight q6_K     [  4096, 128256,     1,     1 ]\n",
            "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  288:           blk.31.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
            "llama_model_loader: - tensor  289:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
            "llama_model_loader: - kv   0:                       general.architecture str     \n",
            "llama_model_loader: - kv   1:                               general.name str     \n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32     \n",
            "llama_model_loader: - kv   3:                       llama.context_length u32     \n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32     \n",
            "llama_model_loader: - kv   5:                          llama.block_count u32     \n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32     \n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32     \n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32     \n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32     \n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32     \n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32     \n",
            "llama_model_loader: - kv  12:                          general.file_type u32     \n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str     \n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr     \n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr     \n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr     \n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr     \n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32     \n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32     \n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32     \n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str     \n",
            "llama_model_loader: - kv  22:               general.quantization_version u32     \n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_0:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = mostly Q4_0\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \n",
            "llm_load_print_meta: general.name   = outputs\n",
            "llm_load_print_meta: BOS token = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: PAD token = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: LF token  = 128 'Ä'\n",
            "llm_load_print_meta: sparse_pred_threshold = 0.00\n",
            "llm_load_tensors: ggml ctx size =    0.11 MB\n",
            "llm_load_tensors: using CUDA for GPU acceleration\n",
            "llm_load_tensors: mem required  = 4437.91 MB\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/35 layers to GPU\n",
            "llm_load_tensors: VRAM used: 0.00 MB\n",
            ".......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_new_context_with_model: kv self size  =   64.00 MB\n",
            "llama_build_graph: non-view tensors processed: 708/740\n",
            "llama_build_graph: ****************************************************************\n",
            "llama_build_graph: not all non-view tensors have been processed with a callback\n",
            "llama_build_graph: this can indicate an inefficiency in the graph implementation\n",
            "llama_build_graph: build with LLAMA_OFFLOAD_DEBUG for more info\n",
            "llama_build_graph: ref: https://github.com/ggerganov/llama.cpp/pull/3837\n",
            "llama_build_graph: ****************************************************************\n",
            "llama_new_context_with_model: compute buffer total size = 260.06 MB\n",
            "llama_new_context_with_model: VRAM scratch buffer: 258.50 MB\n",
            "llama_new_context_with_model: total VRAM used: 258.50 MB (model: 0.00 MB, context: 258.50 MB)\n",
            "\n",
            "system_info: n_threads = 8 / 2 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n",
            "\n",
            "\n",
            "你好啊，请给我讲个简短的笑话吧！](https://i.stack^C\n",
            "2024-06-17 03:32:30.454580\n"
          ]
        }
      ]
    }
  ]
}