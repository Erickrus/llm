{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "w6oKmwJWWIbO"
      ],
      "authorship_tag": "ABX9TyOTc9KvQPyUkfcWHBu7sISW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erickrus/llm/blob/main/beam_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Greedy Search Decoder and Beam Search Decoder implementation\n",
        "\n",
        "https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/"
      ],
      "metadata": {
        "id": "J4zgOsyZVrGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Search Decoder\n",
        "\n",
        "A simple approximation is to use a greedy search that selects the most likely word at each step in the output sequence.\n",
        "\n",
        "This approach has the benefit that it is very fast, but the quality of the final output sequences may be far from optimal.\n",
        "\n",
        "We can demonstrate the greedy search approach to decoding with a small contrived example in Python.\n",
        "\n",
        "We can start off with a prediction problem that involves a sequence of 10 words. Each word is predicted as a probability distribution over a vocabulary of 5 words.\n",
        "\n",
        "We will assume that the words have been integer encoded, such that the column index can be used to look-up the associated word in the vocabulary. Therefore, the task of decoding becomes the task of selecting a sequence of integers from the probability distributions.\n",
        "\n",
        "The argmax() mathematical function can be used to select the index of an array that has the largest value. We can use this function to select the word index that is most likely at each step in the sequence. This function is provided directly in numpy.\n",
        "\n",
        "The greedy_decoder() function below implements this decoder strategy using the argmax function."
      ],
      "metadata": {
        "id": "w6oKmwJWWIbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title greedy_decoder()\n",
        "import numpy as np\n",
        "\n",
        "# greedy decoder\n",
        "def greedy_decoder(time_steps):\n",
        "    sequence = []\n",
        "    # index for largest probability each row\n",
        "    for step_id, logits in enumerate(time_steps):\n",
        "        sequence.append(greedy_decode(logits))\n",
        "        print(f\"step {step_id}:\", sequence)\n",
        "    return sequence\n",
        "\n",
        "def greedy_decode(logits):\n",
        "    return np.argmax(logits)\n",
        "\n",
        "# define a sequence of 10 words over a vocab of 5 words\n",
        "data = [\n",
        "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "    [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "    [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "    [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "    [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "    [0.5, 0.4, 0.3, 0.2, 0.1]]\n",
        "data = np.array(data)\n",
        "# decode sequence\n",
        "result = greedy_decoder(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "d5RVOExDUliC",
        "outputId": "4b7943dc-34c4-450a-c4b0-eaf41e9f0a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: [4]\n",
            "step 1: [4, 0]\n",
            "step 2: [4, 0, 4]\n",
            "step 3: [4, 0, 4, 0]\n",
            "step 4: [4, 0, 4, 0, 4]\n",
            "step 5: [4, 0, 4, 0, 4, 0]\n",
            "step 6: [4, 0, 4, 0, 4, 0, 4]\n",
            "step 7: [4, 0, 4, 0, 4, 0, 4, 0]\n",
            "step 8: [4, 0, 4, 0, 4, 0, 4, 0, 4]\n",
            "step 9: [4, 0, 4, 0, 4, 0, 4, 0, 4, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search Decoder\n",
        "\n",
        "Another popular heuristic is the beam search that expands upon the greedy search and returns a list of most likely output sequences.\n",
        "\n",
        "Instead of greedily choosing the most likely next step as the sequence is constructed, the beam search expands all possible next steps and keeps the k most likely, where k is a user-specified parameter and controls the number of beams or parallel searches through the sequence of probabilities.\n",
        "\n",
        "The local beam search algorithm keeps track of k states rather than just one. It begins with k randomly generated states. At each step, all the successors of all k states are generated. If any one is a goal, the algorithm halts. Otherwise, it selects the k best successors from the complete list and repeats.\n",
        "\n",
        "— Pages 125-126, Artificial Intelligence: A Modern Approach (3rd Edition), 2009.\n",
        "\n",
        "\n",
        "We can define a function to perform the beam search for a given sequence of probabilities and beam width parameter k. At each step, each candidate sequence is expanded with all possible next steps. Each candidate step is scored by multiplying the probabilities together. The k sequences with the most likely probabilities are selected and all other candidates are pruned. The process then repeats until the end of the sequence.\n",
        "\n",
        "Probabilities are small numbers and multiplying small numbers together creates very small numbers. To avoid underflowing the floating point numbers, the natural logarithm of the probabilities are added together, which keeps the numbers larger and manageable. Further, it is also common to perform the search by minimizing the score. This final tweak means that we can sort all candidate sequences in ascending order by their score and select the first k as the most likely candidate sequences.\n",
        "\n",
        "The beam_search_decoder() function below implements the beam search decoder."
      ],
      "metadata": {
        "id": "j-LW5iO9Ws0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title beam_search_decoder()\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def beam_search_decoder(time_steps, beta):\n",
        "    # sequence maintain a data structure of [[sequence list], score]\n",
        "    sequences = [[list(), 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for step_id, logits in enumerate(time_steps):\n",
        "        sequences = beam_search(sequences, logits, beta, step_id)\n",
        "    return sequences\n",
        "\n",
        "# beam search\n",
        "def beam_search(sequences, logits, beta, step_id=0):\n",
        "    all_candidates = list()\n",
        "    # expand each current candidate\n",
        "    for i in range(len(sequences)):\n",
        "        seq, score = sequences[i]\n",
        "        for j in range(len(logits)):\n",
        "            candidate = [seq + [j], score - math.log(logits[j])]\n",
        "            all_candidates.append(candidate)\n",
        "    # order all candidates by score\n",
        "    ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "    # trim β best\n",
        "    sequences = ordered[:beta]\n",
        "    print(f\"step {step_id}:\",)\n",
        "    for seq in sequences:\n",
        "        print(seq)\n",
        "    return sequences\n",
        "\n",
        "# define a sequence of 10 words over a vocab of 5 words\n",
        "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "    [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "    [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "    [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "    [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "    [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "    [0.5, 0.4, 0.3, 0.2, 0.1]]\n",
        "data = np.array(data)\n",
        "# decode sequence\n",
        "result = beam_search_decoder(data, beta=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "f7o7sbEsSAD7",
        "outputId": "cac16c81-9729-48b9-898e-0856a19dae39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0:\n",
            "[[4], 0.6931471805599453]\n",
            "[[3], 0.916290731874155]\n",
            "[[2], 1.2039728043259361]\n",
            "step 1:\n",
            "[[4, 0], 1.3862943611198906]\n",
            "[[4, 1], 1.6094379124341003]\n",
            "[[3, 0], 1.6094379124341003]\n",
            "step 2:\n",
            "[[4, 0, 4], 2.0794415416798357]\n",
            "[[4, 0, 3], 2.3025850929940455]\n",
            "[[4, 1, 4], 2.3025850929940455]\n",
            "step 3:\n",
            "[[4, 0, 4, 0], 2.772588722239781]\n",
            "[[4, 0, 4, 1], 2.995732273553991]\n",
            "[[4, 0, 3, 0], 2.995732273553991]\n",
            "step 4:\n",
            "[[4, 0, 4, 0, 4], 3.4657359027997265]\n",
            "[[4, 0, 4, 0, 3], 3.6888794541139363]\n",
            "[[4, 0, 4, 1, 4], 3.6888794541139363]\n",
            "step 5:\n",
            "[[4, 0, 4, 0, 4, 0], 4.1588830833596715]\n",
            "[[4, 0, 4, 0, 4, 1], 4.382026634673881]\n",
            "[[4, 0, 4, 0, 3, 0], 4.382026634673881]\n",
            "step 6:\n",
            "[[4, 0, 4, 0, 4, 0, 4], 4.852030263919617]\n",
            "[[4, 0, 4, 0, 4, 0, 3], 5.075173815233827]\n",
            "[[4, 0, 4, 0, 4, 1, 4], 5.075173815233827]\n",
            "step 7:\n",
            "[[4, 0, 4, 0, 4, 0, 4, 0], 5.545177444479562]\n",
            "[[4, 0, 4, 0, 4, 0, 4, 1], 5.768320995793772]\n",
            "[[4, 0, 4, 0, 4, 0, 3, 0], 5.768320995793772]\n",
            "step 8:\n",
            "[[4, 0, 4, 0, 4, 0, 4, 0, 4], 6.238324625039508]\n",
            "[[4, 0, 4, 0, 4, 0, 4, 0, 3], 6.461468176353717]\n",
            "[[4, 0, 4, 0, 4, 0, 4, 1, 4], 6.461468176353717]\n",
            "step 9:\n",
            "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 6.931471805599453]\n",
            "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 7.154615356913663]\n",
            "[[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 7.154615356913663]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contrastive Search: Theory and Implementation\n",
        "Theory\n",
        "\n",
        "Contrastive search is a method for generating sequences (e.g., text) that balances diversity and quality. This approach aims to avoid repetitive and degenerate text generation by penalizing less diverse sequences. Here are the\n",
        "key elements:\n",
        "\n",
        "- Candidate Recall: At each step, generate a set of candidate tokens using the model's logits.\n",
        "- Degeneration Penalty: Apply a penalty to the candidate tokens to prevent repetitive patterns.\n",
        "- Re-ranking: Re-rank the candidates based on the model's confidence (probability) and the degeneration penalty.\n",
        "\n",
        "\n",
        "#### Key Parameters:\n",
        "\n",
        "- top_k: The number of top candidates considered at each step.\n",
        "- penalty_alpha: The weight of the degeneration penalty. A higher value discourages repetitive sequences more strongly.\n",
        "\n",
        "\n",
        "#### Implementation\n",
        "The provided code implements contrastive search in TensorFlow for text generation using a Transformer-based model. Here’s a detailed breakdown of the process:\n",
        "\n",
        "##### Initialization:\n",
        "\n",
        "Various configurations like logits_processor, logits_warper, max_length, pad_token_id, eos_token_id, etc., are initialized.\n",
        "Initialize tensors for storing scores, attention weights, hidden states, and the generated sequence.\n",
        "\n",
        "##### First Step Preparation:\n",
        "\n",
        "Encode the initial input sequence to get the past key values (cached states), logits for the next step, and hidden states.\n",
        "These are used for the first contrastive search step.\n",
        "\n",
        "##### Contrastive Search Loop:\n",
        "\n",
        "Candidate Recall:\n",
        "Process logits using logits_processor and logits_warper.\n",
        "Apply softmax to get probabilities and select top-k candidates.\n",
        "Re-ranking with Degeneration Penalty:\n",
        "Compute cosine similarity between the hidden states of the last generated token and the candidate tokens.\n",
        "Apply the degeneration penalty and re-rank the candidates.\n",
        "Selection and Update:\n",
        "Select the best candidate based on the re-ranked scores.\n",
        "Update the sequence, hidden states, and cached values for the next step.\n",
        "Termination Check:\n",
        "Check if the end-of-sequence token is generated or the maximum length is reached.\n",
        "\n",
        "##### Output Preparation:\n",
        "\n",
        "Collect and return the generated sequence along with optional scores, attentions, and hidden states if requested.\n",
        "\n",
        "#### Key Functions and Code Explanation\n",
        "gather_best_candidate:\n",
        "\n",
        "Gathers slices indexed by selected_idx_stacked from a nested structure of tensors.\n",
        "This is used to select the best candidate sequences at each step.\n",
        "contrastive_search_cond_fn:\n",
        "\n",
        "Checks whether the generation process should continue based on unfinished sequences.\n",
        "contrastive_search_body_fn:\n",
        "\n",
        "The core function implementing the contrastive search logic.\n",
        "Handles candidate recall, re-ranking, and updates the sequence, hidden states, and model cache.\n",
        "Main Logic Flow\n",
        "First Step:\n",
        "\n",
        "Prepare inputs and encode the initial prefix.\n",
        "Obtain past key values, logits, and hidden states.\n",
        "Process logits and get top-k candidates.\n",
        "Compute the degeneration penalty and select the best candidate.\n",
        "Update the sequence and prepare for the next step.\n",
        "Subsequent Steps:\n",
        "\n",
        "For each step, recall top-k candidates.\n",
        "Apply degeneration penalty and re-rank.\n",
        "Select the best candidate and update the sequence.\n",
        "Continue until the end-of-sequence token is generated or the maximum length is reached.\n",
        "Final Output:\n",
        "\n",
        "Return the generated sequence, optionally with scores, attentions, and hidden states.\n",
        "Example Usage\n",
        "Here’s an example usage provided in the docstring:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
        "model = TFAutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
        "# Set pad_token_id to eos_token_id because OPT does not have a PAD token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "input_prompt = \"DeepMind Company is\"\n",
        "input_ids = tokenizer(input_prompt, return_tensors=\"tf\")\n",
        "outputs = model.contrastive_search(input_ids, penalty_alpha=0.6, top_k=4, max_length=64)\n",
        "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "This example initializes the model and tokenizer, sets the input prompt, and performs contrastive search to generate a sequence. The result is decoded and printed.\n",
        "\n",
        "Summary\n",
        "Contrastive search is a powerful method to generate high-quality and diverse text sequences. The implementation leverages top-k candidate recall and degeneration penalties to balance between diversity and coherence. This method can be applied to various sequence generation tasks, including text-to-text, speech-to-text, and vision-to-text models."
      ],
      "metadata": {
        "id": "UV9Tb9ayibGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/axinc-ai/contrastive-search-a-token-selection-algorithm-that-can-suppress-repetition-in-language-10712d5408da"
      ],
      "metadata": {
        "id": "3WtF5ThYktwD"
      }
    }
  ]
}