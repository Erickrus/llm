{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erickrus/llm/blob/main/tiktoken_emoji.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziirFV9PaG-T",
        "outputId": "e9a17b10-1119-471f-c52e-60d0c28eab37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init the GPT-4 Tokenizer\n",
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "print(enc.n_vocab) # number of tokens in total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57wUOMOhaL2y",
        "outputId": "3c6ba8b2-cd8f-46cc-dde7-31e7299276dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init the emojis\n",
        "import emoji\n",
        "emojis = list(emoji.EMOJI_DATA.keys())\n",
        "import random\n",
        "random.seed(15)\n",
        "random.shuffle(emojis)\n",
        "print(len(emoji.EMOJI_DATA)) # number of possible emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IODCjlnLeNjh",
        "outputId": "3f2537ac-7b03-42f2-c735-9fa8650706f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_tokens(text, max_per_row=10):\n",
        "    ids = enc.encode(text)\n",
        "    unique_tokens = set(ids)\n",
        "    # map all tokens we see to a unique emoji\n",
        "    id_to_emoji = {id: emoji for emoji, id in zip(emojis, unique_tokens)}\n",
        "    # do the translatation\n",
        "    lines = []\n",
        "    for i in range(0, len(ids), max_per_row):\n",
        "        lines.append(''.join([id_to_emoji[id] for id in ids[i:i+max_per_row]]))\n",
        "    out = '\\n'.join(lines)\n",
        "    return out"
      ],
      "metadata": {
        "id": "KvSEuUZreWms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Words vs Tokens\n",
        "A word is most likely what you think it is - the most simple form or unit of language as understood by humans. In the sentence, â€œI like catsâ€, there are three words - â€œIâ€, â€œlikeâ€, and â€œcats.â€ We can think of words as the primary building blocks of language; the fundamental pieces of language that we are taught from a very young age.\n",
        "\n",
        "A token is a bit more complex. Tokenization is the process of converting pieces of language into bits of data that are usable for a program, and a tokenizer is an algorithm or function that performs this process, i.e., takes language and converts it into these usable bits of data. Thus, a token is a unit of text that is intentionally segmented for a large language model to process efficiently. These units can be words or any other subset of language - parts of words, combinations of words, or punctuation.\n",
        "\n",
        "There are a variety of different tokenizers out there which reflect a variety of trade offs. Well-known tokenizers include NLTK (Natural Language Toolkit), Spacy, BERT tokenizer and Keras. Whether or not to select one of these or a different tokenizer depends upon your specific use case. On average, there are roughly 0.75 words per token, but there can be meaningful differences among tokenizers.\"\"\"\n",
        "\n",
        "print(text_to_tokens(text, max_per_row=15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjCMId2KaPWG",
        "outputId": "b048044a-0109-4fc6-ddd8-cd5308e3d2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§ğŸ¿ğŸƒğŸ¾â€â¡ğŸ‘©ğŸ»â€ğŸ¤â€ğŸ‘©ğŸ¼ğŸ•´ğŸ¾ğŸ‹ğŸ¿ğŸš¶ğŸ¼â€â™‚ï¸â€â¡ğŸ‹ğŸ¾â€â™€ğŸ‡¼ğŸ‡¸ğŸ¤ğŸ¾âğŸ«·ğŸ‘©ğŸ¾â€ğŸ¦¯ğŸ––ğŸ¾ğŸ‹ğŸ¾â€â™€ğŸ‘¨ğŸ»â€ğŸ¦²\n",
            "ğŸŒâ€â™‚ï¸ğŸ‡¼ğŸ‡¸ğŸ‘³â€â™‚ï¸ğŸ§‘ğŸ½â€â¤â€ğŸ§‘ğŸ¾ğŸ«ğŸ§‘ğŸ½â€ğŸ¤â€ğŸ§‘ğŸ½ğŸ—ï¸ğŸ”“ğŸ‘±â€â™€ğŸ‘©ğŸ»â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ¾ğŸª±ğŸ§µğŸ¤¾ğŸ»â€â™€ï¸ğŸ§‘ğŸ½â€âš–ğŸŒâ€â™‚ï¸\n",
            "ğŸ”£ğŸ§â€â™€ğŸ§™ğŸ½â€â™€ï¸ğŸ‘®ğŸ¾ğŸ‡ªğŸ‡·ğŸ¤™ğŸ»ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ğŸ¤ğŸğŸ‘¨â€ğŸ‘©â€ğŸ‘¦ğŸ§‘ğŸ¼â€ğŸ«ğŸ‘¨ğŸ»â€ğŸ¦²ğŸ§™ğŸ½â€â™€ï¸ğŸ‘®ğŸ¾ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾\n",
            "ğŸ§™ğŸ½â€â™€ï¸ğŸ§‘ğŸ¾â€ğŸ¦¯â€â¡ğŸ§‘ğŸ¼â€â¤ï¸â€ğŸ’‹â€ğŸ§‘ğŸ¾ğŸ‘¨ğŸ¾â€ğŸ¦²ğŸ§™ğŸ½â€â™€ï¸ğŸ±ğŸ§šğŸ»â€â™€ï¸ğŸ‘©ğŸ»â€â¤â€ğŸ’‹â€ğŸ‘©ğŸ¾ğŸ‡³ğŸ‡«ğŸ‘©ğŸ¾â€ğŸ¦¯ğŸ—ï¸ğŸ§‘ğŸ¼â€ğŸ«ğŸ‘±â€â™€ğŸŒâ€â™‚ï¸ğŸ“\n",
            "ğŸ‘©ğŸ»â€ğŸ¦²â†•ï¸ğŸ—ï¸ğŸ”“ğŸ§‘â€ğŸ¦¼â€â¡ï¸ğŸŒâ€â™‚ï¸ğŸ‘¨ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ»ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ»ğŸ—ï¸ğŸ”“ğŸ‡·ğŸ‡ªğŸ‡¨ğŸ‡¦ğŸğŸ‘ï¸â€ğŸ—¨ï¸ğŸƒğŸ½â€â™‚ï¸â€â¡\n",
            "ğŸ–²â˜ªï¸ğŸ‘±ğŸ¾â€â™‚ğŸš¶ğŸ¾â€â™€â€â¡ğŸ¤«ğŸ‹ğŸ¿ğŸ‡¬ğŸ‡¹ğŸ‹ğŸ¾â€â™€ğŸ–²ğŸŒ«ğŸ§‘ğŸ»â€ğŸ¦¯â€â¡ï¸ğŸ…ğŸ¼ğŸ¤¾ğŸ»â€â™€ï¸ğŸ„ğŸ¾â€â™‚âœŒğŸ¿\n",
            "ğŸ‹ğŸ¾â€â™€ğŸŒâ€â™‚ï¸ğŸ”ƒğŸ—ï¸ğŸ§‘ğŸ¾â€â¤â€ğŸ’‹â€ğŸ§‘ğŸ¿ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ‘©ğŸ»ğŸ—ï¸ğŸ”“ğŸ¥˜ğŸ§‘ğŸ»â€ğŸ“ğŸ—ï¸ğŸ“®ğŸ‡·ğŸ‡ªğŸğŸ•µğŸ¼â€â™€ï¸\n",
            "â˜ºğŸ–²ğŸˆ´ğŸ§â€â™€ğŸ‘¨ğŸ¾â€ğŸ¦²ğŸ–²ğŸ“•ğŸ‹ğŸ¾â€â™€ğŸŒ¶ğŸ«±ğŸ«ğŸ‡«ğŸ‡·ğŸ‡·ğŸ‡ªğŸ˜•ğŸ‘©â€ğŸ¦½â€â¡\n",
            "ğŸ”ƒğŸ§â€â™€ğŸ¦¸ğŸ¾â€â™‚ï¸ğŸ§‘ğŸ½â€âš•ğŸ¤¸ğŸ¾â€â™‚â°ğŸ”“ğŸ‘¨ğŸ¾â€ğŸ¦²ğŸ§”ğŸ¼ğŸ––ğŸ¾ğŸ¥˜ğŸŠğŸ•µğŸ¼â€â™€ï¸ğŸ§‘ğŸ»â€ğŸ“ğŸ—ï¸\n",
            "ğŸ“®ğŸ¤¾ğŸ»â€â™€ï¸ğŸ’‡ğŸ¾ğŸ§â€â™€ğŸ–²ğŸ‡¬ğŸ‡¹ğŸ‹ğŸ¾â€â™€ğŸ–²ğŸ§‘ğŸ½â€ğŸ¤â€ğŸ§‘ğŸ½ğŸ—ï¸ğŸš¶ğŸ»â€â™€â€â¡ğŸ‡·ğŸ‡ªğŸ‹ğŸ¾â€â™€ğŸ§”ğŸ½â€â™€ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ»\n",
            "â˜ºğŸ–²ğŸŒğŸ”“ğŸ§ğŸ¾â€â™€â€â¡ğŸ‡°ğŸ‡¾ğŸ”ƒã€°ï¸ğŸ¤¾ğŸ»â€â™€ï¸ğŸ‘¨ğŸ½â€â¤â€ğŸ‘¨ğŸ½ğŸ—œï¸ğŸ‡³ğŸ‡«ğŸ•“ğŸ§‘ğŸ¼â€ğŸ«ğŸ«\n",
            "ğŸ¥–ğŸ„ğŸ¼ğŸ’†ğŸ½â€â™€ï¸ğŸ—ï¸ğŸ”“ğŸ‘¨ğŸ»â€ğŸ¦²ğŸ¤²ğŸ¿ğŸ—ï¸ğŸ§‘ğŸ¼â€ğŸ«ğŸ§â€â™€â™€ï¸ğŸ—ï¸ğŸ§‘ğŸ¼â€ğŸ«ğŸ§â€â™€ğŸ«\n",
            "ğŸ§‘â€ğŸ¼ğŸ¤«ğŸ‡¦ğŸ‡«ğŸğŸ–²ğŸ§‘ğŸ¾â€ğŸ¦¼â€â¡ï¸ğŸ—ï¸ğŸŒ­ğŸ‡¬ğŸ‡¹â™‘ğŸ”¶ğŸ¤ğŸ›¸ğŸ§‘ğŸ¼â€ğŸ¦½â€â¡ï¸ğŸ–²\n",
            "ğŸ§‘ğŸ¾â€ğŸ¦¼â€â¡ï¸ğŸ—ï¸ğŸ¤¾â€â™€ğŸŒ±ğŸ¤¾ğŸ»â€â™€ï¸ğŸ‘”ğŸ§—ğŸ¼â€â™€ï¸ğŸ‡¬ğŸ‡¹â™‘ğŸ‡µğŸ‡¾ğŸ‡¹ğŸ‡¯ğŸ‘©ğŸ¿â€ğŸ“ğŸ“šğŸ™‹ğŸ»â€â™€ğŸ¾\n",
            "ğŸ™‡ğŸ¿â€â™‚ï¸ğŸ“†ğŸ§šğŸ¿â€â™‚âœ‹ğŸ¼ğŸ§â€â™€ğŸ‘¨ğŸ½â€ğŸŒ¾ğŸ¤¸ğŸ¼â€â™€ï¸ğŸ“•ğŸ‘¨ğŸ¾â€ğŸ¦²ğŸ‘¨â€ğŸ¦½â€â¡ï¸ğŸ§—ğŸ¾â€â™‚ğŸ¤¾ğŸ»â€â™€ï¸ğŸ‡³ğŸ‡¨ğŸ«ğŸ§‘â€ğŸ¦½\n",
            "ğŸ‡°ğŸ‡¾â³ğŸ¤µğŸ¼â€â™€ï¸ğŸ—ï¸ğŸŠğŸ«ğŸ–²ğŸŒ­ğŸ“•ğŸ‘¨â€ğŸ¼ğŸ—£ğŸ‘©ğŸ¿â€ğŸ¦¯ğŸª–ğŸƒğŸ¿â€â™€â€â¡ï¸ğŸ§—ğŸ»â€â™€\n",
            "ğŸ¤¾ğŸ»â€â™€ï¸ğŸ–ŠğŸ¦¹ğŸ¼â€â™‚ï¸ğŸ§â€â™€ğŸ¤ğŸğŸš¶ğŸ¿â€â™‚ï¸ğŸ¤µâ€â™€ğŸ™â€â™€ï¸ğŸ¤¾ğŸ»â€â™€ï¸ğŸ’‚ğŸ§‘ğŸ¼â€ğŸ«ğŸª¤ğŸ‡¬ğŸ‡¹ğŸ§â€â™€\n",
            "ğŸ’ŒğŸ¤ğŸ‡³ğŸ‡«ğŸ•“â¬†ï¸ğŸ‘©ğŸ¾â€ğŸ¦³ğŸ™…â€â™€ğŸ‡¬ğŸ‡¹â™‘ğŸ¤¾ğŸ»â€â™€ï¸\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"How many letters 'r' in the word 'strawberry'?\"\"\"\n",
        "print(text_to_tokens(text, max_per_row=20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75OlT3yhf9p5",
        "outputId": "c3d92aa0-686c-405d-cb65-7930e3d6a8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§‘â€ğŸ¦½ğŸ‘©ğŸ¿â€â¤ï¸â€ğŸ’‹â€ğŸ‘¨ğŸ»ğŸ¤¾ğŸ»â€â™€ï¸ğŸ™â€â™€ï¸ğŸ¤™ğŸ»ğŸ§‘ğŸ¾â€ğŸ¦¼â€â¡ï¸âœŒğŸ¿ğŸ’‚ğŸ“ğŸ™â€â™€ï¸ğŸˆ´ğŸ§â€â™€ğŸğŸ§‘â€ğŸ¦¼â€â¡ï¸\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMHXKsahgBg-",
        "outputId": "0988e290-8ba5-4565-f79a-5b01ffa5fcb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4438, 1690, 12197, 364, 81, 6, 304, 279, 3492, 364, 496, 675, 15717, 71090]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}