{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erickrus/llm/blob/main/tiktoken_emoji.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziirFV9PaG-T",
        "outputId": "e9a17b10-1119-471f-c52e-60d0c28eab37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init the GPT-4 Tokenizer\n",
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "print(enc.n_vocab) # number of tokens in total"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57wUOMOhaL2y",
        "outputId": "3c6ba8b2-cd8f-46cc-dde7-31e7299276dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init the emojis\n",
        "import emoji\n",
        "emojis = list(emoji.EMOJI_DATA.keys())\n",
        "import random\n",
        "random.seed(15)\n",
        "random.shuffle(emojis)\n",
        "print(len(emoji.EMOJI_DATA)) # number of possible emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IODCjlnLeNjh",
        "outputId": "3f2537ac-7b03-42f2-c735-9fa8650706f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_tokens(text, max_per_row=10):\n",
        "    ids = enc.encode(text)\n",
        "    unique_tokens = set(ids)\n",
        "    # map all tokens we see to a unique emoji\n",
        "    id_to_emoji = {id: emoji for emoji, id in zip(emojis, unique_tokens)}\n",
        "    # do the translatation\n",
        "    lines = []\n",
        "    for i in range(0, len(ids), max_per_row):\n",
        "        lines.append(''.join([id_to_emoji[id] for id in ids[i:i+max_per_row]]))\n",
        "    out = '\\n'.join(lines)\n",
        "    return out"
      ],
      "metadata": {
        "id": "KvSEuUZreWms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Words vs Tokens\n",
        "A word is most likely what you think it is - the most simple form or unit of language as understood by humans. In the sentence, “I like cats”, there are three words - “I”, “like”, and “cats.” We can think of words as the primary building blocks of language; the fundamental pieces of language that we are taught from a very young age.\n",
        "\n",
        "A token is a bit more complex. Tokenization is the process of converting pieces of language into bits of data that are usable for a program, and a tokenizer is an algorithm or function that performs this process, i.e., takes language and converts it into these usable bits of data. Thus, a token is a unit of text that is intentionally segmented for a large language model to process efficiently. These units can be words or any other subset of language - parts of words, combinations of words, or punctuation.\n",
        "\n",
        "There are a variety of different tokenizers out there which reflect a variety of trade offs. Well-known tokenizers include NLTK (Natural Language Toolkit), Spacy, BERT tokenizer and Keras. Whether or not to select one of these or a different tokenizer depends upon your specific use case. On average, there are roughly 0.75 words per token, but there can be meaningful differences among tokenizers.\"\"\"\n",
        "\n",
        "print(text_to_tokens(text, max_per_row=15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjCMId2KaPWG",
        "outputId": "b048044a-0109-4fc6-ddd8-cd5308e3d2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧍🏿🏃🏾‍➡👩🏻‍🤝‍👩🏼🕴🏾🏋🏿🚶🏼‍♂️‍➡🏋🏾‍♀🇼🇸🤝🏾❎🫷👩🏾‍🦯🖖🏾🏋🏾‍♀👨🏻‍🦲\n",
            "🏌‍♂️🇼🇸👳‍♂️🧑🏽‍❤‍🧑🏾🫁🧑🏽‍🤝‍🧑🏽🎗️🔓👱‍♀👩🏻‍❤️‍💋‍👨🏾🪱🧵🤾🏻‍♀️🧑🏽‍⚖🏌‍♂️\n",
            "🔣🧎‍♀🧙🏽‍♀️👮🏾🇪🇷🤙🏻🧑🏼‍❤️‍💋‍🧑🏾🤎🍏👨‍👩‍👦🧑🏼‍🏫👨🏻‍🦲🧙🏽‍♀️👮🏾🧑🏼‍❤️‍💋‍🧑🏾\n",
            "🧙🏽‍♀️🧑🏾‍🦯‍➡🧑🏼‍❤️‍💋‍🧑🏾👨🏾‍🦲🧙🏽‍♀️🍱🧚🏻‍♀️👩🏻‍❤‍💋‍👩🏾🇳🇫👩🏾‍🦯🎗️🧑🏼‍🏫👱‍♀🏌‍♂️📏\n",
            "👩🏻‍🦲↕️🎗️🔓🧑‍🦼‍➡️🏌‍♂️👨🏿‍❤️‍💋‍👨🏻👩🏿‍❤️‍👩🏻🎗️🔓🇷🇪🇨🇦🍏👁️‍🗨️🏃🏽‍♂️‍➡\n",
            "🖲☪️👱🏾‍♂🚶🏾‍♀‍➡🤫🏋🏿🇬🇹🏋🏾‍♀🖲🌫🧑🏻‍🦯‍➡️🎅🏼🤾🏻‍♀️🏄🏾‍♂✌🏿\n",
            "🏋🏾‍♀🏌‍♂️🔃🎗️🧑🏾‍❤‍💋‍🧑🏿👩🏿‍❤️‍👩🏻🎗️🔓🥘🧑🏻‍🎓🎗️📮🇷🇪🍏🕵🏼‍♀️\n",
            "☺🖲🈴🧎‍♀👨🏾‍🦲🖲📕🏋🏾‍♀🌶🫱🫁🇫🇷🇷🇪😕👩‍🦽‍➡\n",
            "🔃🧎‍♀🦸🏾‍♂️🧑🏽‍⚕🤸🏾‍♂⏰🔓👨🏾‍🦲🧔🏼🖖🏾🥘🏊🕵🏼‍♀️🧑🏻‍🎓🎗️\n",
            "📮🤾🏻‍♀️💇🏾🧎‍♀🖲🇬🇹🏋🏾‍♀🖲🧑🏽‍🤝‍🧑🏽🎗️🚶🏻‍♀‍➡🇷🇪🏋🏾‍♀🧔🏽‍♀👩🏿‍❤️‍💋‍👨🏻\n",
            "☺🖲🏌🔓🧎🏾‍♀‍➡🇰🇾🔃〰️🤾🏻‍♀️👨🏽‍❤‍👨🏽🗜️🇳🇫🕓🧑🏼‍🏫🫁\n",
            "🥖🏄🏼💆🏽‍♀️🎗️🔓👨🏻‍🦲🤲🏿🎗️🧑🏼‍🏫🧎‍♀♀️🎗️🧑🏼‍🏫🧎‍♀🫁\n",
            "🧑‍🍼🤫🇦🇫🍏🖲🧑🏾‍🦼‍➡️🎗️🌭🇬🇹♑🔶🤎🛸🧑🏼‍🦽‍➡️🖲\n",
            "🧑🏾‍🦼‍➡️🎗️🤾‍♀🌱🤾🏻‍♀️👔🧗🏼‍♀️🇬🇹♑🇵🇾🇹🇯👩🏿‍🎓📚🙋🏻‍♀🏾\n",
            "🙇🏿‍♂️📆🧚🏿‍♂✋🏼🧎‍♀👨🏽‍🌾🤸🏼‍♀️📕👨🏾‍🦲👨‍🦽‍➡️🧗🏾‍♂🤾🏻‍♀️🇳🇨🫁🧑‍🦽\n",
            "🇰🇾⏳🤵🏼‍♀️🎗️🏊🫁🖲🌭📕👨‍🍼🗣👩🏿‍🦯🪖🏃🏿‍♀‍➡️🧗🏻‍♀\n",
            "🤾🏻‍♀️🖊🦹🏼‍♂️🧎‍♀🤎🍏🚶🏿‍♂️🤵‍♀🙍‍♀️🤾🏻‍♀️💂🧑🏼‍🏫🪤🇬🇹🧎‍♀\n",
            "💌🤎🇳🇫🕓⬆️👩🏾‍🦳🙅‍♀🇬🇹♑🤾🏻‍♀️\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"How many letters 'r' in the word 'strawberry'?\"\"\"\n",
        "print(text_to_tokens(text, max_per_row=20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75OlT3yhf9p5",
        "outputId": "c3d92aa0-686c-405d-cb65-7930e3d6a8bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧑‍🦽👩🏿‍❤️‍💋‍👨🏻🤾🏻‍♀️🙍‍♀️🤙🏻🧑🏾‍🦼‍➡️✌🏿💂📏🙍‍♀️🈴🧎‍♀🍏🧑‍🦼‍➡️\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMHXKsahgBg-",
        "outputId": "0988e290-8ba5-4565-f79a-5b01ffa5fcb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4438, 1690, 12197, 364, 81, 6, 304, 279, 3492, 364, 496, 675, 15717, 71090]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}